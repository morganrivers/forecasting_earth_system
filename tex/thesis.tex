\documentclass[12pt,a4paper]{article}


% ---------- Basic packages ----------
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{tcolorbox}
\usepackage{setspace}
\usepackage{url}
\usepackage{microtype}
\usepackage{tabularx}
\usepackage{booktabs}
% \usepackage{breakable}
\usepackage{subcaption}
\usepackage{caption}
\usepackage{adjustbox}
\usepackage{array}
\usepackage{amsmath}
\usepackage{makecell}
\usepackage{placeins}
\usepackage{xstring}
\usepackage{etoolbox}

\usepackage{enumitem}



\usepackage[hidelinks]{hyperref}
% \usepackage{hyperref}
\usepackage{pdfcomment}
\usepackage{cleveref}
\usepackage{threeparttable}
%better bibtex
\usepackage[backend=biber,style=authoryear,sorting=nyt,natbib=true]{biblatex}


% \usepackage{fp}


\addbibresource{references.bib}


% Drop "note" (and similar) from every bibliography entry
\AtEveryBibitem{%
  \clearfield{note}%        kill the note field
  \clearfield{addendum}%    (often used like a note)
  \clearfield{annotation}%  (some exporters use this)
}


\newbibmacro*{citewithtitle:tooltip}{%
  \printnames{labelname}%
  \addcomma\space
  \printfield{year}%
  \addspace\textbar\addspace
  \printfield{title}%
}

% (Author, Year | Title) with link + tooltip
\DeclareCiteCommand{\citewithtitle}
  {\usebibmacro{prenote}}
  {\printtext[bibhyperref]{%
     \pdftooltip{%
       \mkbibparens{%
         \printnames{labelname},\space
         \printfield{year}\addspace\textbar\addspace
         \printfield[citetitle]{title}%
       }%
     }{%
       \usebibmacro{citewithtitle:tooltip}%
     }%
  }}
  {\multicitedelim}
  {\usebibmacro{postnote}}
% Make natbib-like commands available:
\let\citep\parencite
\let\citet\textcite

\makeatletter
\edef\dictfile{macros/keyword.tex}
%\typeout{Using dictionary: \dictfile}%
\input{\dictfile}
\makeatother

% --- Load the dictionary with a safe fallback ---
\makeatletter
\edef\dictfile{macros/dictionary-\DICT.tex}
%\typeout{Using dictionary: \dictfile}%
\input{\dictfile}
\makeatother

% --- Simple conditional helpers based on \DICT ---
\newcommand{\ifdev}[1]{%
  \IfStrEq{\DICT}{development}{#1}{}%
}

\newcommand{\ifsus}[1]{%
  \IfStrEq{\DICT}{sustainability}{#1}{}%
}

\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{4}

% \newcommand{\subsubsubsection}[1]{\paragraph{#1}\mbox{}\\}

% centered, stretchable column
\newcolumntype{Y}{>{\centering\arraybackslash}X}
\newcommand{\logoheight}{1.1cm} % tweak if needed


\setstretch{1.2}
\setlength{\parindent}{0pt}
\setlength{\parskip}{0.6em}

% For even spacing of logos
\newcolumntype{C}[1]{>{\centering\arraybackslash}m{#1}}

% Make article say "Table of Contents"
\renewcommand{\contentsname}{Table of Contents}
\renewcommand{\textfraction}{0.05}
\renewcommand{\floatpagefraction}{0.8}
 
% -----------------------------------
% Editable metadata
% -----------------------------------
% \newcommand{\ThesisTitle}{\TITLE}
\newcommand{\ThesisTitle}{Forecasting the Success of Environmental and Sustainability Activities in International Development Using Language Models}
% \newcommand{\ThesisSubtitle}{SUBTITLE}
\newcommand{\ThesisCity}{[City]}
\newcommand{\ThesisDate}{[Date]}

\newcommand{\AuthorName}{Morgan Rivers}
\newcommand{\AuthorAddressTwo}{Berlin, Germany 10559}
\newcommand{\AuthorEmail}{rivers@uni-potsdam.de}
\newcommand{\MatricNo}{829112}

\newcommand{\FirstReviewer}{Prof. Christian Kuhlicke}
\newcommand{\SecondReviewer}{Dr. Ivan Kuznetzov}
\newcommand{\MonthName}{%
  \ifcase\month
  \or January\or February\or March\or April\or May\or June%
  \or July\or August\or September\or October\or November\or December%
  \fi
}
\newcommand{\DateDDMonthYYYY}{\number\day~\MonthName~\number\year}

\begin{document}
\hypersetup{pageanchor=false}

\pagenumbering{gobble}
\thispagestyle{empty}

% ---------- Title page ----------
\begin{center}

    % Desired order: RIFS, UFZ, AWI, PIK, University crest
    \noindent
    \begin{tabularx}{\textwidth}{@{}YYYYY@{}}
    % \adjustbox{max height=\logoheight, max width=\linewidth, keepaspectratio}{\includegraphics{assets/rif.jpg}} &
    \adjustbox{max height=\logoheight, max width=\linewidth, keepaspectratio}{\includegraphics{assets/helmholtz.png
      }} &
    \adjustbox{max height=\logoheight, max width=\linewidth, keepaspectratio}{\includegraphics{assets/awi.jpg}} &
    % \adjustbox{max height=\logoheight, max width=\linewidth, keepaspectratio}{\includegraphics{assets/pik.png}} &
    \adjustbox{max height=2\logoheight, max width=2\linewidth, keepaspectratio}{\includegraphics{assets/potsdam.png}}\\
    \end{tabularx}


    \vspace{1.8cm}

    
    \begin{center}
        \begin{flushleft}
        University of Potsdam\\
        Faculty of Science\\
        Institute of Environmental Science and Geography\\
        Institute of Physics and Astronomy\\
        \textbf{Climate, Earth, Water, \& Sustainability\\[4em]}
        \end{flushleft}
    
        {\large \textbf{Master Thesis}}\\
        for the award of the academic degree\\
        \textbf{Master of Science (M.Sc.)}\\
        at the University of Potsdam\\[3em]
    
        \textbf{\Large \ThesisTitle}\\ [3em] %if no subtitle
        %\textbf{\large SUBTITLE}\\[6em]
    
        Potsdam, \DateDDMonthYYYY \\[7em]
    
        \begin{flushleft}
            \textbf{Submitted by:}\\[-0.2em]
            \AuthorName\\
            % \AuthorAddressOne\\
            % \AuthorAddressTwo\\
            \AuthorEmail\\
            Matriculation No.: \MatricNo

            % \vspace{1.0cm}
            First reviewer: \FirstReviewer\\
            Second reviewer: \SecondReviewer
        \end{flushleft}

    \end{center}
    \clearpage

\end{center}
\clearpage
% ... title/roman pages ...
\hypersetup{pageanchor=true}
% ---------- Main matter ----------
\section*{Abstract}
\subsection*{Abstract in English}
% ---------------- Results constants ----------------
% ---------------- Results constants ----------------

\newcommand{\PairwiseHuman}{65}
\newcommand{\PairwiseModel}{77}
\newcommand{\PairwiseCILow}{73}
\newcommand{\PairwiseCIHigh}{81}

\newcommand{\Rsqfrac}{0.26}
\newcommand{\Rsqpct}{26}
\newcommand{\RsqpctCILow}{14}
\newcommand{\RsqpctCIHigh}{37}

\newcommand{\BaselineRsq}{10}

\newcommand{\CostEffect}{60}
\newcommand{\CostCILow}{55}
\newcommand{\CostCIHigh}{66}

% ---------------- END CONSTANTS ----------------

International aid and cooperation creates a profound difference in the rate of development in growing economies, improves the lives of the world's poorest, and often safeguards the environment and materially promotes sustainability. However, international aid has non-significant rates of failure in achieving its objectives. There have been few attempts in the literature to create models to explicitly predict the success of aid activities, and none focused on environmental outcomes. This thesis produces a forecasting system for the overall success of international aid activities at time of evaluation from the International Aid and Transparency Initiative (IATI) database, combining classical statistical methods with modern language model techniques. I apply novel techniques to improve prediction accuracy, including quantifiable information used in previous studies, averaging the success rates of semantically similar activities, and using the reasoning abilities and information gathering ability of large language models (LLMs) to improve forecasts. I first assess overall success ratings on a scale from approximately 1 to 6. Testing against the validation set, 300 later-starting activities in a dataset of 1,300 environmental and sustainability improving activities for 4 reporting organizations, the full forecasting system improves on the proxy for human forecasting skill baseline with a probability of forecasting which activity will have a higher rating from \PairwiseHuman\,\% to \PairwiseModel\,\% [95\% CI: \PairwiseCILow\,\%, \PairwiseCIHigh\,\%]. The system also explains \Rsqpct\,\% [95\% CI: \RsqpctCILow\,\%, \RsqpctCIHigh\,\%] of the variance in true ratings ($R^2=\Rsqfrac$) in the validation set, compared to \BaselineRsq\,\% for a ``pick the most common rating from reporting org'' baseline, and 0\% for the human forecasting skill proxy. I also construct a novel aggregate outcome cost-effectiveness metric, as an independent check that ratings correlate with better success metrics. Using similar methods, my forecasting system predicts which activity will have a higher cost-effectiveness among random pairs of activities in the IATI database with a success probability of \CostEffect\,\% [95\% CI: \CostCILow\,\%, \CostCIHigh\,\%]. I find a weak, positive correlation between ratings and cost-effectiveness of outcomes. However, the correlation in model forecasts is much higher, indicating that the learnable signal is similar between rating and cost-effectiveness prediction. I also release a freely available dataset of LLM-generated activity grades, summaries, success ratings, and various other quantitative activity outcomes and extracted information for 1,800 IATI activities. This work lays the foundation to improve decision making for a wide range of initiatives and policies in developing countries and also in other data-rich institutional contexts.



% International aid and cooperation creates a profound difference in the rate of development in growing economies, improves the lives of the world's poorest, and often safeguards the environment and materially promotes sustainability. However, international aid has non-significant rates of failure in achieving its objectives. There have been few attempts in the literature to create models to explicitly predict the success of aid activities, and none focused on environmental outcomes. This thesis produces a forecasting system for the overall success of international aid activities at time of evaluation from the International Aid and Transparency Initiative (IATI) database, combining classical statistical methods with modern language model techniques. I apply novel techniques to improve prediction accuracy, including quantifiable information used in previous studies, averaging the success rates of semantically similar activities, and using the reasoning abilities and information gathering ability of large language models (LLMs) to improve forecasts. I first assess overall success ratings on a scale from approximately 1 to 6. Testing against the validation set, 300 later-starting activities in a dataset of 1,300 environmental and sustainability improving activities for 4 reporting organizations, the full forecasting system improves on the proxy for human forecasting skill baseline with a probability of forecasting which activity will have a higher rating from 65\% to 77\%  [95\% CI: 73\%, 81\%]. The system explains 26\% [95\% CI: 14\%, 37\%] of the variance in true ratings ($R^2$=0.26) in the validation set, compared to 10.2\% for a ``pick the most common rating from reporting org'' baseline, and 0\% for the human forecasting skill proxy. I also construct a novel aggregate outcome cost-effectiveness metric, as an independent check that ratings correlate with better success metrics. Using similar methods, my forecasting system predicts which activity will have a higher cost-effectiveness score over all organizations in the IATI database with a success probability of 60.4\% [95\% CI: 55\%, 66\%]. I find a weak, positive correlation between ratings and cost-effectiveness of outcomes, while noting that the shared variance that the model predicts is much stronger. I also release a freely available dataset of LLM-generated activity grades, summaries, success ratings, and various other quantitative activity outcomes and extracted information for 1,800 IATI activities. This work lays the foundation to improve decision making for a wide range of initiatives and policies in developing countries and also in other data-rich institutional contexts.
% International aid and cooperation creates a profound difference in the rate of development in growing economies, improves the lives of the world's poorest, and often safeguards the environment and materially promotes sustainability. However, international aid has non-significant rates of failure in achieving its objectives. There have been few attempts in the literature to create models to predict the success of aid activities, and none focused on environmental outcomes. This thesis produces a forecasting system for the overall success of international aid activities at time of evaluation from the International Aid and Transparency Initiative (IATI) database, combining classical statistical methods with modern language model techniques. I find that the information available at the start of the activity all contribute to prediction accuracy, including quantifiable information used in previous studies, averaging the success rates of semantically similar activities, and using the reasoning abilities and information gathering ability of large language models (LLMs) to improve forecasts. Testing against the held-out, latest-starting 400 activities in a dataset of 1,300 environmental and sustainability improving activities for 4 reporting organizations, the full forecasting system improves success forecasting at the beginning of the activity by 5.9\% (95\% CI: XX\% to XX\%), improving the prediction from 70.0\% to 73.0\% accuracy compared to a ``pick the most common rating'' baseline. For overall success ratings on a scale from approximately 1 to 6, the system can explain 17.4\% (95\% CI: XX\% to XX\%) of the variation compared to 6.8\% for the baseline. I also release a freely available dataset of LLM-generated activity grades, summaries, success ratings, and various other quantitative activity outcomes and extracted information for 1,800 IATI activities. This work lays the foundation to improve decision making for a wide range of initiatives and policies in developing countries and also in other data-rich institutional contexts.

\subsection*{Abstract auf Deutsch}
Will do, once abstract is finalized %TODO
\clearpage

% ---------- Contents & front matter ----------
\pagenumbering{roman}
\setcounter{page}{1}
\tableofcontents
\clearpage

\pagenumbering{arabic}
\setcounter{page}{1}

















\section{Background: Prior work in LLM Forecasting and Forecasting International Aid Impact}
\subsection{Introduction}
\textbf{Proposal}
I set out to analyze the applicability and composability of machine learning techniques in forecasting the success of international aid activities using what could be known about the activities at the approval stage. The standard approach in the literature in assessing aid impact and success in environmental domains has historically involved econometric techniques, linear regression models, and occasionally more modern nonlinear models such as Random Forest or XGBoost \citep{goldembergMindingGapAid2025}. However, a thorough assessment of methods to forecast activity success in this domain has yet to be conducted.

This thesis proposes the use of Large Language Models (LLMs) and statistical models to implement judgemental forecasting to predict how effective interventions will be in the context of international aid activities affecting the environment. I obtain metadata and pdf files from thousands of activities, and separate each record into information about the activity available at approval and evaluation information about how successful the activity was. I use this data to answer the following research questions in this work:
\begin{enumerate}
  \item  How do LLM forecasting methods compare to human proxies and statistical models in forecasting international aid overall success ratings and quantitative outcomes?
  \item  Do forecasting methods using state-of-the-art natural language processing methods meaningfully improve on simpler baseline forecasting heuristics?
  \item  How do differing methods of combining LLM and statistical forecasting compare in this domain?
  \item  What methods improve the accuracy of free-form (qualitative) forecasts in this domain?
  \item  What aspects of the activity available in my dataset at the beginning of the activity lead to higher or lower ratings?
  \item  How does forecasting quantitative activity outcomes compare to forecasting ratings?  
\end{enumerate}

 I use language models to mimic the reasoning and data gathering skills of trained forecasters, in an attempt to replicate the success at using judgemental forecasting from language models in geopolitical forecasting to the adjacent domain of forecasting development cooperation outcomes affecting the environment. I then compare this approach to standard statistical methods, and utilize novel techniques which combine statistical and judgemental AI forecasting, to investigate how the advantages of both can be put to best use in improved forecasting of international aid activity outcomes. Given the difficulty of field testing ideas, policymakers and funding agencies often rely on expert forecasts on how an intervention will meet its intended goals to select which interventions will be implemented \citep{hewittPredictingResultsSocial}. Replacing or augmenting that advisory role could greatly improve decision making in the context of international aid.

This method differs in two key ways from prior literature, which have analyzed international aid evaluations. The first difference is that while many works in the literature have attempted to assess correlations between quantitative features and aid evaluation ratings, they have not focused on what knowledge would be available at approval for the activity, and do not assess out-of-time generalization of these correlations. In this work, I assess out-of-time generalization of feature importance. This is critical, because in order to improve aid decision making, one must assess the ability of models to forecast the outcomes of future interventions, not simply statically analyze a corpus of past activities. The second difference is that in addition to standard statistical models, I implement judgemental AI forecasting, as a supplement to standard statistical models.


I will briefly review current progress in event outcome prediction in developmental aid and cooperation interventions affecting the environment, and then discuss progress with LLMs in adjacent domains. %To my knowledge, there has been no attempt at forecasting real-world outcomes of interventions in developmental aid and cooperation interventions affecting the environment\ while also rigorously quantifying the skill of such a system. # no longer true!

Within the domain of LLM use, there have been early attempts at using them to improve decision making for the environment. A recent tool called ``climsight'' summarizes and aggregates information about climate adaptation and mitigation \citep{koldunovLocalClimateServices2024}, but stops short of making forecasts towards adaptation. Machine learning and LLMs have been used to collect over 80,000 articles about climate adaptation and provide analysis about which areas of implementation are lacking and point out gaps in attention towards promising categories of policies.

Limited work has also been done using LLMs such as ChatGPT-4 (GPT-4) to serve as data sources for policy deliberation and multiâ€‘criteria assessment of climate and sustainability interventions, finding GPT-4 is in rough agreement with the policy rankings of human experts for the expected outcomes \citep{binaLargeLanguageModels2025}. However, very little is done to improve on GPT-4's abilities, the assessment was made on only a few dozen generic policy examples, and no attempt was made to compare outcomes between these policies and real-world outcomes. Despite these limitations, the findings are promising. For multiple criteria decision making (MCDM), GPT-4 provided a useful collaborative starting point, eased the process of considering multiple criteria effectively, and aided policy deliberation on climate change and sustainability.

One attempt which focused on specific outcomes of activities found their model using ``embeddings'' of LLMs could explain 70\% of the variance of the unexplained residual from control variables on relevant country-level sector outcomes from the World Bank World Development Indicators, and assessed the performance of nonlinear models including the random forest model used in this work. However, they include features that could not be known at the beginning of the activity (e.g. actual duration), and do not assess out-of-time generalization, instead splitting randomly within the dataset, nor do they explicitly assess prediction performance for ratings. Furthermore, in replicating and extending their method, I found their model worse than random chance at out-of-time prediction of outcomes, indicating severe issues with out-of-time generalization, which I could only explain by what I discovered to be validation and test set data leakage into their training set. % evidence: https://github.com/luke-grassroot/aid-outcomes-ml/blob/main/notebooks/contextual_correlates_N.ipynb, : X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

\section{Methods}
This thesis implements an LLM-based forecasting method for ratings, a statistical model for ratings free of any LLM features, a system combining the best LLM method and the best statistical model, with the advantages of both, and finally a modified system designed to predict categories of quantitative activity outcomes extracted from activity evaluation documents. The system is built forecasting what the evaluation results will be for thousands of IATI records containing both a pre-intervention description of the activity, as well as an ex-post evaluation of the results. To do so, thousands of pdfs were downloaded, ranked from most to least relevant for forecasting future outcomes or evaluating the end result of the activities, had their pages ranked and graded for relevance to the task, had quantitative and qualitative descriptions and results transcribed into a unified format. 



\subsection{Data Sources} \label{sec:methods_for_llm_forecasting}

After considering several data sources for prediction, including the OpenAlex publication repository of peer-reviewed evaluation documents and abstracts, the IDEAL database of ex-post evaluations, the 3ie development database, I decided to use the IATI database, due to its substantial quantity of information available in textual format and extractable from the database records, and its sheer size. While ex-post evaluations may provide sufficient information to describe the activity, it may introduce ``future leakage'' to rely on language models to completely remove information about the eventual outcome. Furthermore, although many millions of evaluations are available, it proved difficult to reliably identify and de-duplicate academic papers regarding evaluations of environmental interventions. The IDEAL database and 3ie were in the dozens or hundreds of records for environmental topics.

The IATI database has reliable start and end dates, and typically several recorded outcomes, and usually an overall evaluation rating on a six point scale  within linked evaluation pdfs. It also reliably marks the reporting organization, allowing for an intelligent unification of the rating scales, and sometimes provides a ``results'' section where outcomes of an activity can sometimes be found. It is quite common for several activity information documents to be uploaded near the beginning of the activity, and several years later, at least one ex-post evaluation of the activity is uploaded as well, or results of key quantitative outcomes are sometimes directly recorded in the IATI database. The status of the activity is extremely commonly reported, including if it is in the planning or completion/finalization stages, which is helpful information for forecasting.

The downside of the IATI database is it is sometimes inconsistent between reporting organizations as to how the data are filled in, and the format of documents was not always PDF, requiring conversion scripts. Also, many download links were not functioning or required custom web-scraping scripts to properly extract project documents in pdf format from the original websites where project documents were hosted. Dates of documents and especially planned start or end dates, or actual start or end dates, were often missing, leading to frequent exclusion of projects. Furthermore, approximately 30\% of IATI activities do not have an activity category code, leading to a further exclusion of environmental or sustainability related activities.

In addition to IATI, I also analyzed the AidData database, which has laboriously double-coded data entry, which introduces less error than the LLM and regex data extraction techniques I used. However, I find that the delay in hand-coding the ratings and results leads to a paucity of recent environmental activities, and the lack of document links in AidData was a key downside that made it difficult to use AidData. After downloading AidData, I found the overlap between AidData and my training set was less than 30\%, which meant any data enrichment would run into issues of data availability for the remaining 70\%.

\subsection{Data Filtering}

\textbf{IATI records for prediction}

Out of the approximately 800,000 international aid activities recorded in IATI, I first reduced the set of activities of interest to 7,575 records which aimed to improving the environment, sustainability, or climate adaptation in a developing country or countries, had an appraisal/intervention description document, and an outcome evaluation or progress report document, both of which could be converted to PDF format. Links to these documents were then downloaded where possible (see the next section for details).

Once documents were downloaded and converted to pdf format, activities were further filtered so that they had at least one document describing the activity, and had a metadata date before 1/4 of the activity implementation period, as well as at least one ex-ante activity at least 3/4 through the activity period. The latest activity document also had to have a metadata date at least one year before the earliest evaluation document. An exception was project appraisal documents from the World Bank, or Project Information Documents, which were found to reliably not leak future information, and this was judged to be more trustworthy than extracting the creation date embedded in the activity document. Activities not meeting these requirements were also excluded, leaving 3,225 activities.

After passing all these filters, I finally attempted to extract the activity rating from the evaluation document using two separate methods. Because rating tendencies are systematically different for different reporting orgs, I needed sufficient data for training, validation, and testing for all organizations. I also restricted activities to those that were marked as "completed" in order to ensure comparability between rating scales, as the only activities not marked as ``completed'' were relatively recent and would dominate the held-out test set. I determined there were sufficient data for four reporting organizations: The World Bank (957 activities), BMZ/KFW/GIZ (240 activities), the Asian Development Bank (ADB) (156 activities) and the UK Foreign Commonwealth and Development Office (FCDO) (127 activities).



The activity filtering for the topic was done by-hand to filter only those activities relating to improving the environment or sustainability. These were:
\begin{itemize}[noitemsep, topsep=0pt]
\item 14015: Water resources conservation (including data collection)
\item 14020: Water supply and sanitation - large systems
\item 14021: Water supply - large systems
\item 14022: Sanitation - large systems
\item 14032: Basic sanitation
\item 14050: Waste management/disposal
\item 23110: Energy policy and administrative management
\item 23111: Energy sector policy, planning and administration
\item 23112: Energy regulation
\item 23183: Energy conservation and demand-side efficiency
\item 23210: Energy generation, renewable sources - multiple technologies
\item 23220: Hydro-electric power plants
\item 23230: Solar energy for centralised grids
\item 23231: Solar energy for isolated grids and standalone systems
\item 23232: Solar energy - thermal applications
\item 23240: Wind energy
\item 23250: Marine energy
\item 23260: Geothermal energy
\item 23270: Biofuel-fired power plants
\item 23350: Fossil fuel electric power plants with carbon capture and storage (CCS)
\item 23360: Non-renewable waste-fired electric power plants
\item 23410: Hybrid energy electric power plants
\item 23510: Nuclear energy electric power plants and nuclear safety
\item 23610: Heat plants
\item 23630: Electric power transmission and distribution (centralised grids)
\item 23631: Electric power transmission and distribution (isolated mini-grids)
\item 23642: Electric mobility infrastructures
\item 31130: Agricultural land resources
\item 31210: Forestry policy and administrative management
\item 31220: Forestry development
\item 31281: Forestry education/training
\item 31282: Forestry research
\item 31291: Forestry services
\item 32174: Clean cooking appliances manufacturing
\item 41010: Environmental policy and administrative management
\item 41020: Biosphere protection
\item 41030: Biodiversity
\item 41081: Environmental education/training
\item 41082: Environmental research
\end{itemize}


% Figure: workflow 
\begin{figure}[!htbp]
  \centering
  \includegraphics[width=1\textwidth]{assets/pie_plot_env_categories.png}
  \caption{The split of topics analyzed from the dataset.}
  \label{fig:workflow}
\end{figure}



\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.9\textwidth]{assets/splits_by_start_year.png}
  \caption{The activities included for forecasting ratings, with the splits by count year. Incomplete activities, shown in red, were not used for prediction. Activity ids used for outcome prediction were given the same temporal boundaries.}
  \label{fig:splits}

\end{figure}



% Figure: predicted vs observed absolute error (scatter)
\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.9\textwidth]{assets/disbursement_by_year.png}
  \caption{Total disbursements for IATI activities used for ratings, by year. There is no clear trend over time for activity size in the database. }
  \label{fig:disbursementsbyyear}
\end{figure}

% \end{figure}
% Figure: predicted vs observed absolute error (scatter)
\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.9\textwidth]{assets/duration_by_split.png}
  \caption{The durations of activities per split. More recently starting activities tend to be shorter, as they have not yet had time to complete and be evaluated. The out-of-distribution nature of validation and test sets increase the challenge of generalizing patterns from the training data. }
  \label{fig:splitdurations}
\end{figure}



\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.9\textwidth]{assets/reporting_org_splits.png}
  \caption{The breakdown of reporting orgs in the dataset which were used for training, validation and testing. The validation and test set are in this way significantly out-of-distribution.}`'
  \label{fig:shapratings}
\end{figure}





\textbf{Document-related Filtering}

The IATI database contains a collection of thousands of links to pdfs, word documents, html documents, and other document formats. These were first automatically converted to pdf format via a custom python script, and subsequently needed to pass several criteria before being used as documents for forecasting.

I first wrote a script that directly downloaded these and converted them to pdf format. %Specifically for UNDP, I made a custom-query to convert their interactive HTML results pages into a pdf format, all others had the links for the documents downloaded directly with no additional processing. #well, didn't end up using any UNDP
Next, I look at the pdf metadata date, and determine the creation date of the pdf files. I find this is more often closer to the date of the specific activity description document or activity evaluation document (as determined by reading the document) than metadata at the url indicating upload date, or the date entered in IATI for the document.  UNDP results had the URL specifically included in the JSON payload populating their website, so the latest year indicated in that evaluation payload was used instead for the UNDP results.

All documents are tagged in IATI with one or more of the following tags per document:
``Pre- and post-project impact appraisal'', ``Objectives / Purpose of activity'', ``Intended ultimate beneficiaries'', ``Conditions'', ``Budget'', ``Summary information about contract'', ``Review of project performance and evaluation'', ``Results, outcomes and outputs'', ``Memorandum of understanding (If agreed by all parties)'', ``Tender'', or ``Contract''. 

I mark documents with ``Objectives / Purpose of activity'' or ``Summary information about contract'', tags as preliminary ``baseline'' documents - those representing information about the activity before it begins. Documents with ``Review of project performance and evaluation'' or ``Pre- and post-project impact appraisal'' tags are marked as preliminary evaluation documents. In order to provide sufficient information to forecast with and sufficient information to evaluate that forecast, I require at least one ``baseline'' document and at least one ``outcome'' document per activity, with the baseline document at least one year prior to the evaluation document (based on the uploaded document metadata date). I also require that the activity status code is not ``Pipeline/identification''. Instead, activities are allowed to be in implementation, finalization, closed, cancelled, or suspended, such that either a final or preliminary evaluation document is possible.

I filtered further to ensure that all activity document labels which were "Conditions", "Budget", "Tender", "Contract" with no other tags were excluded, as these were typically purely legal context, often containing very little evaluation or useful additional activity information.

The date for the documents were determined using (in descending preference where available) the pdf's ``created on'' or ``last modified'' in its metadata, or the date indicated in the IATI record for the document. This ordering preference was determined as the PDF metadata dates were found to more reliably match the stated date of authorship of the documents better than the ``IATI date'' recorded within the activity record. These dates were usually available in the metadata of the original PDF, ODT, DOC, or DOCX file. Experimenting with different date options revealed that out of 400 randomly selected PDFs, the closest available date to the true date of authoring the document was the ``created by'' date, then the ``last modified'' date, then the ``IATI date''. The median difference for the date when selecting this ordering was 22 days different than the date indicated in the document itself as determined by feeding the first 3 pages of each of those 400 PDF documents to \emph{gemini-2.5-flash}.

To ensure pdf metadata dates were appropriate, an analysis was undergone to ensure the procedure for selecting the date of activity documents was valid. If the date of the activity is too early, it could lead to documents authored well after the project start leaking future information. To test this, 400 random pdf documents downloaded were uploaded to gemini and the pdf metadata dates were inspected. PDF metadata dates were discovered to have a median difference of 22 days from the date indicated on the document as extracted by gemini. It was discovered that while approximately 10\% of the dates were more than a year after the actual date indicated on the document (such that an activity was actually authored earlier than the start date of the activity), a concerning 0.5\% of documents had a creation pdf metadata date later than the date extracted directly from the document. 

In order to ensure the forecasts were all based on project information available only roughly at the beginning of the activity, a search was undergone through the information available to the model when forecasting to ensure the forecasting was based only on what could have been known at the beginning of the activity. Approximately 10 activities with pdf metadata dates more than a year earlier than the true authoring of activity documents would be expected, based on the 0.5\% rate of ``>1 year too early'' errors from the date analysis.

To prevent any information leakage, which could be due to incorrect dates as well as incorrect marking of the start date of the activity in IATI, or significant progress being made within the first quarter of the activity where documents are allowed, approximately 40 random chatgpt-generated activity summaries (see the next section) were inspected, with none indicating advanced progress, indicating less than 2.5\% of activities should be of concern. A selective search for phrases revealed some activities had made clear progress on targeted outcomes. Consequently, a python script with 6,800 separate search terms was used to further search for inappropriate documents. Exact string search terms were made, with variants of phrases including, ``on track'', ``ongoing project has been performing'', ``ongoing project is performing'', ``already made considerable progress'', ``key milestones already achieved'', ``significant progress had been made'', ``the programme has already made considerable progress'', etc. This led to the review of approximately 150 additional activities, and the discovery of 21 activities with clear progress on key project milestones. Progress such as the formation of planning committees or initial disbursements of funds to the implementing organization were not considered grounds for exclusion, given that these milestones are unlikely to be substantially informative. However, extension activities or Phase II / Phase III activities were not excluded, unless significant progress had already been made on the extension or phase being evaluated.


In order to properly extract accurate overall success ratings for each activity and useful textual information about the project for forecasting, I processed each pdf document using the following data processing pipeline:

\subsection{Preliminary Data Processing}

All document pages had their rotation detected, and were rotated to vertical before processing via the Gemini API. Documents with ``.odt'', ``.doc'', or ``.docx'' extensions were converted to pdfs with a custom script. The pages when converted to pdfs were counted and zero-page documents were excluded.

\textbf{1. Ranking documents}
Documents were ranked from most to least useful for forecasting the outcome, or evaluating the results, respectively. \emph{gemini-2.5-flash} structured output with direct pdf input was used to make the rankings. Only documents with c- or better grades on a grading scale from a+ to f were considered for the next stage. Also, the documents were ranked from most to least informative for forecasting among the baseline documents, and most to least valuable for ex-post evaluation among the outcome documents. Baseline documents that were closest to the activity start, and the latest outcome documents were preferenced. Documents with sufficient detail but not excessive lengths, such as executive summaries, were prioritized. Documents that were duplicates in a non-English language were excluded if the equivalent was available in English. For outcomes, if there were multiple progress reports, all the earlier ones were excluded and only the latest were kept in the rankings. After ranking, 2,312 documents had sufficiently informative activity information and activity evaluation documents.

\textbf{2. Categorizing pages within documents}
The highest ranking documents were then split into 3-page chunks. Each 3-page chunk was sent in pdf form to \emph{gemini-2.5-flash}. The pages were categorized differently based on whether the document was a baseline or outcome. Categories for outcomes allowed retrieval based on whether final evaluation in quantitative or qualitative form are present on the page, deviations from plans or other types of outcomes were detailed, or if the pages were simply overviews of the activity. Specifically, the allowed categorizations were ``condensed summary'', ``sub activities outlined'', ``detailed implementation plans'', ``broad objectives'', ``possible outcomes'', ``quantitative targets'', ``qualitative targets'', ``risks as word or numeric'', ``risks or dangers generally'', ``plans to address key risks'', ``positive indicators'', ``progress reports'', ``similar cases outcomes'', ``implementation context country'', ``contextual challenges'', ``financing details'', ``budget and legal'', ``who implements'', ``whether part of larger program'', ``partner identity or skill'', ``whether skin in the game'', ``other stakeholder engagement'', or ``activity monitoring details'' for baseline document pages, and  ``expected outcomes'', ``deviation from plans'', ``preliminary results'', ``final outcomes'', ``delays or early completion'', ``over or under spending'', ``overview as was planned'', or ``unrelated to evaluation'' for outcome document pages. Only one category choice among these was possible per page.

In order to exclude irrelevant pages, the pages were also given a second category, for outcome document pages as ``glossary'', ``blank  page'', ``table of contents'', ``outcome evaluation'', ``activity description'', ``references'', or ``other'', and for baseline document pages the same categories were options, in addition to ``core activities'', ``theory of change'', ``targets'', ``broader context'', and ``preliminary results''. Only one category choice among these was possible per page.


\textbf{3. Extracting Ratings}
Two separate methods were used to extract rankings. The first method sent each individual outcome page ranked above 7/10 for relevance to evaluation, or with a ``quantitative targets'' categorization, to \emph{gemini-2.5-flash} to extract any overall ratings, and a second script summarized the overall ratings into a single value for the document. However, this was often insufficient to capture the overall ratings. Another ``fallback'' script involved a custom generated word search with approximately 500 different rephrasings of ``overall rating'', ``final result'', ``synthesized score'', etc, in English, and searched the pdfs directly for an exact match on those terms, prioritizing pages with one or more exact text matches of such terms. Otherwise, if such words could not be found, the earliest pages in the document which were not categorized as ``blank page'', ``appendix'', ``glossary'', ``table of contents'', ``references'', or ``activity description'' were included and \emph{gemini-2.5-flash} was queried to extract the overall rating from the documents.

%A slightly different process was done for UNDP activities. The fallback script was used again, but documents were judged by \emph{gemini-2.5-flash} for whether the outcomes represented an ``overall successful'' or ``overall unsuccessful'' activity, and this was entered as the result, as UNDP rarely delivers directly an overall rating for the activity. # we don't use these anymore

For BMZ/GIZ/KFW documents, activity baseline documents were extremely rare. For this reason, the evaluation document was treated as a baseline document for the purposes of forecasting activity success. Categorization for these evaluations also was via the ``baseline'' document method described above. When grading or summarizing the features of the evaluation document, \emph{gemini-2.5-flash} was instructed to only describe what could have been known at the beginning of the activity, and to under no circumstances reveal the final outcome of the activity.

\textbf{4. Interpreting Ratings}
Ratings were reported both with the rating itself, as well as a maximum and minimum possible rating. The World Bank rating scale from 1 (``Highly Unsatisfactory'') to 6 (``Highly Satisfactory'') was used as the template rating, and other ratings were attempted to match against this scale. Notably, BMZ/GIZ/KFW ratings were inverted to reach this scale. A ``Satisfactory'' score was considered equivalent to scores such as ``successful'', ``On Track'', or ``met expectations''. Scores listed as percentages or fractions were re-scaled to the 1 to 6 scale as well. In order to ensure ratings were fairly compared, only the top four most common organizations with ratings were included for training and validation of the forecasting system. %Because UNDP results were always binary from the rating of \emph{gemini-2.5-flash}, and the Asian Development Bank was nearly always rated either "successful" or "needs improvement" % todo: check this
%, both of these were collapsed to the binary ``satisfactory''/``unsatisfactory'' ratings.


\FloatBarrier
\subsubsection{Outcome Extraction}
In addition to extracting ratings, a similar approach was used with \emph{gemini-2.5-flash} to extract quantitative outcomes. I extracted quantitative outcomes from all pages which were categorized as outcomes, and marked as containing quantitative information. Unlike with ratings, I did not limit to the top 4 reporting organizations, as reporting ratings is more susceptible to between-reporting-organization variation and gaming than the reportable quantitative outcomes of projects.

Once these PDF pages were extracted, I employed a combination of manual examination of the extracted outcomes and a report of common bigrams to identify outcome variables that could be compared between projects. For each common outcome category, I came up with a list of words and phrases that would commonly match reports of these outcome variables in the description, as well as appropriate units. 


\textbf{Keyword- and unit-based outcome parsers}

For each outcome category, I implemented a dedicated parser that scans the extracted quantitative description, baseline, target, outcome, and units. Each parser follows the same general pipeline:
(i) \emph{filtering} by description keywords and unit constraints,
(ii) \emph{sanitization} of numeric values (e.g., dropping negative sentinels or implausible magnitudes),
(iii) \emph{normalization} to a canonical unit (e.g., hectares, tonnes, people),
and (iv) \emph{aggregation} to a single activity-level outcome.


\textbf{Comparable outcome categories}

Using manual inspection and frequency statistics over common bigrams, I defined a set of outcome categories that recur across evaluations and are interpretable across projects. The final set included:
\begin{itemize}
  \item \textbf{Cost--benefit ratios (B/C):} benefit-cost ratio outcomes.
  \item \textbf{Rates of return:} economic rate of return (ERR/EIRR) and financial rate of return (FRR/FIRR), in percent.
  \item \textbf{Emissions reductions:} CO\textsubscript{2} or CO\textsubscript{2}e reductions (total or per-year) in tonnes.
  \item \textbf{Water and sanitation connections:} counts of service connections, either new or repaired.
  \item \textbf{Pollution load removed:} wastewater pollutant load reductions (e.g., BOD/COD/nutrients), in tonnes and categorized by time basis (total, per-year, per-day).
  \item \textbf{Forest indicators:} trees/seedlings planted (counts) and area-based forest outcomes (reforested, under management, protected) in hectares.
  \item \textbf{Irrigation outcomes:} increases in irrigated area (hectares), computed as a positive increase relative to baseline where available.
  \item \textbf{Energy outcomes:} installed generation capacity, in MW (or occasionally GWh where the source reported capacity in energy units).
  \item \textbf{Air quality (PM2.5):} PM2.5 reductions reported as concentration, emissions, or percent, kept as separate distributions.
  \item \textbf{Clean cooking stoves:} counts of stoves distributed/installed.
  \item \textbf{Agricultural yields:} yield increases expressed either as level changes (normalized to tonnes per hectare when possible) or as percent increases.
\end{itemize}

Each category required a custom python script, primarily because evaluation reports often contain multiple related indicators (e.g., component-level versus project-level rates of return) and frequently use heterogeneous units or phrasing.

Across all outcome categories, I used custom Python scripts to identify activity-level outcomes. Indicators were detected via keyword and unit constraints, implausible or non-numeric values were dropped, and quantities were standardized to canonical units (e.g., people, tonnes, hectares, MW, t/ha). Domain-specific filters reduced false positives (e.g., wastewater context for pollution loads, water context for connections, agriculture context for yields), textual multipliers and unit abbreviations were parsed to normalize magnitudes, and time bases (total vs. annual) were inferred.

While I originally simply took the mean of extracted outcomes, I found the extracted values were often error prone, so instead I took all matching values and compiled them, sending them to \emph{gemini-2.5-flash} for aggregation into a final baseline, target, and outcome value where available for all outcome categories and activities.

Finally, I used the total disbursement for the activity reported by IATI and determined a rough estimate of the USD per unit outcome (see ``Splitting Disbursements'' below), except for Benefit-Cost Ratios, Rates of Return, and agricultural yield outcomes. I found on investigation that most outcomes were approximately log-normally distributed. I took the log10 of all categories except Benefit-Cost ratios, rates of return, and agricultural yields.


\textbf{Splitting disbursements}

Unfortunately, I did not have access to outcome-level funding splits from the IATI database. In order to roughly represent the fact that dollar-per-unit spending can be allocated across several outcomes, I wrote a custom algorithm to evenly allocate total activity expenditures to what are usually distinctly funded outcomes. My procedure assigns each activity's total expenditure across the outcome components it reports, so later cost-per-unit calculations do not implicitly treat multi-outcome activities as having multiple full budgets. Benefit/cost ratios and economic and financial rate of return are excluded from monetary allocation, and other outcomes are eligible for splitting. To avoid double-counting when two indicators are simply alternative measurements of the same underlying result, closely related indicators are first grouped into shared conceptual buckets, such as pairing protected area with area under management, pairing different yield-increase measures, and pairing tree planting with reforested area.

Once the components are bucketed, the algorithm gives each bucket an equal share of the activityâ€™s allocatable funding. Every component inside a bucket inherits that same share, meaning components that are â€œalternative measures of the same thingâ€ share one portion of the allocations rather than each taking a slice.

Carbon dioxide reductions are handled as a special case because they can act as a summary metric that overlaps with other mitigation outputs. If CO$_2$ reductions are reported without any closely linked mitigation outputs (such as improved stoves, added generation capacity, or trees planted), then CO$_2$ reductions receive an equal share like any other bucket. If CO$_2$ reductions are reported alongside any of those linked outputs, it inherits the combined allocation already assigned to the linked outputs present for that activity. This prevents CO$_2$ from inflating allocated spending when it is a co-reported consequence of other outcomes.

\subsection{Baseline Methods}

Three relatively simple baseline methods were attempted, to ensure the relatively complex and expensive LLM-based methods are better than simpler approaches. I choose three simple baseline methods, in order to ensure the forecasts
 were significantly better than the baseline methods for activity success forecasting.

\textbf{Prediction baseline: always predict the most common rating for the reporting organization}

This baseline technique provides a sanity check that more sophisticated methods are worthwhile. Because the prediction task is inherently difficult with much of the variation in outcomes unable to be forecasted at the outset of the activity, this is a relatively strong baseline.
% \subsubsection{Prediction baseline: Random Forest Regression}
% The random forest regression method is a widely applied method to predict unseen data in machine learning. It has the benefit of requiring relatively little compute, performing quite well for a wide range of dataset sizes, and it doesn't tend to overfit as much as neural-network based machine learning models \cite{randomforest}. As random forest cannot use natural language of the prompt, I intend to instead supply the algorithm with fields in the OECD Open Data with the proposed policy and demographic and trend data from the World Bank dataset. The random forest will then predict a value in tonnes CO$_2$.

% However, random forest cannot use the semantics in the natural language prompt, which leads us to our next baseline.

\textbf{Prediction baseline: Ridge Regression with Reporting Organization and Risk Score}

As a rough proxy for the forecasting ability of human evaluators, a ridge regression model was trained given both the llm grade for the degree of risk assessed at the beginning of the project, as well as the dummy variable representing which organization was doing the rating. While by construction the full forecasting system also includes these features and thus with sufficient regularization must perform better than this baseline, it does provide some insight into how skilled aid evaluators are at assessing the overall risk of a low rating. This score was only calculated for activities where the llm was able to successfully extract the grade for how risky the project is from the baseline documents. This method suffers from potential inconsistency in llms extracting the overall risk from baseline documents, and it may be the case that aid evaluators in general, when stating the overall risks of the project, 

\textbf{Ridge Regression Trained with non-LLM categories}

In order to justify the addition of non-LLM categories, we use the baseline statistical categories apparent in prior literature and train a General Linear Model (GLM) on the outputs. Features include
\begin{itemize}[noitemsep,topsep=0pt]
 \item planned activity duration
 \item planned total disbursement
 \item whether the activity is primarily loan or grant-based
 \item the one-hot encoded reporting organization % of the top 4 most common organizations in the database (The World Bank (957 activities), BMZ/KFW/GIZ (240 activities), UNDP (257 activities), and the Asian Development Bank (156 activities))
 \item the Country Policy and Institutional Assessment (CPIA) score from the World Bank for that country
 \item the scope of the activity on a scale from 1-7, ranging from local to global
 \item the $log(\text{GDP} / \text{capita})$ of the countries where the activity takes place weighted by the percentage of the activity performed in each country. %todo: complete the list of features and countries
 \item \emph{gemini-2.5-flash}-generated evaluation on a score from 0 to 100 of:
 \begin{itemize}[noitemsep,topsep=0pt]
 \item  how well financed the activity is
 \item  the activity integratedness within the broader activity ecosystem
 \item  the expected implementer performance
 \item  the ease of targeted outcomes
 \item  the degree of contextual challenge
 \item  the overall risk level
 \item  the activity's overall technical complexity.
\end{itemize}
\end{itemize}
The activity start date was not used, as there was no clear linear pattern with regards to overall activity success over time in the training data.

%\textbf{Prediction baseline: Zero-shot LLM}
%In order to ensure the series of improvements on the LLM system in fact genuinely improve accuracy above simply querying the model with some basic activity information and requesting a prediction, we insert the summarized information about the category along all of the evaluation axes where the model could find relevant information. This show that the methods used to improve accuracy are indeed increasing accuracy above simply a single generated prediction by ChatGPT-3.5.

\subsection{Experimental methods}

Various methods were used to obtain experimental results. I describe them below.

\textbf{Non-Parametric Bootstrap}

The non-parametric bootstrap is a method used to diversify the training data, increasing the diversity in models that are trained many times. It can be used both for ensuring methods robustly improve performance on a diversity of different training setups, and in the case of training the random forest, increases independence between trees. This works by randomly sampling the same number of samples as exist in the training set, with replacement (the same training point may repeat more than once, at random).

\textbf{GLM using IATI Features and Grades}

A GLM is trained with ridge regression to reduce overfitting on noise and improve generalization.


\textbf{Nearest Neighbor (Vector Similarity)} \label{ssub:nearest_neighbor_vector_similarity_}

I first constructed a similarity test using features including countries of the activity, GDP per capita as described previously, the scope of the activity, and the implementing and funding organization ID.
I found however that this similarity test significantly underperformed compared to the semantic similarity of the \emph{gemini-2.5-flash}-generated summary of the activity documents. 
I first weight the similarity proportional to its embedding semantic similarity score, and tested a cutoff for averaging 1, 3, 7, 10, 15, and 20 nearest neighbors using the Gemini embeddings model \emph{gemini-embedding-001}. I found 15 nearest neighbors was the highest-performing using this method, and thus use the weighted average of the nearest neighbor ratings to predict the overall activity score. Although the nearest neighbor method was used to collect examples for the LLM prompt, it was found that simply taking the weighted mean of ratings underperformed the ``most common rating'' method.

\textbf{Random Forest}

The Random Forest method is a statistical algorithm which constructs an ensemble of decision trees which would produce the correct output on the training data, and averages those decision trees. The averaging nature of the random forest algorithm reduces overfitting on the training data. The algorithm is inherently "regularized", penalizing an overly complex decision tree. The decision trees split based on value ranges of the features. By reducing the depth of the trees (the number of decision points where the decision tree splits), we can reduce the memorization of the training data from the trees, and improve generalization of the model. Each decision also only considers a random fraction of the features, encouraging each tree to be more independent of each other and improving generalization further. The bootstrap method is also used to train trees, encouraging tree independences.

\textbf{XGBoost}

The XGBoost (Extreme Gradient Boosting) method is a statistical algorithm which constructs an ensemble of decision trees sequentially, where each subsequent tree attempts to correct the errors made by the previous trees. Unlike the Random Forest which trains trees independently and averages their predictions, XGBoost builds trees iteratively, with each new tree focusing on the residual errors of the ensemble. 

The algorithm incorporates both L1 and L2 regularization terms to penalize model complexity and prevent overfitting on the training data. The decision trees split based on value ranges of the features, using a splitting criterion that accounts for the gradient and hessian of the loss function. By limiting the depth of the trees (the number of decision points where the decision tree splits) and controlling the learning rate, we can reduce the memorization of the training data and improve generalization of the model. Each decision also considers only a random fraction of the features (column subsampling), and each tree is trained on a random fraction of the training samples (row subsampling), which helps prevent overfitting and improves model robustness.


\textbf{Language Model Embeddings}

Statistical models are unable to explicitly capture textual information. Accordingly, I insert this information via usage of embeddings, using the same Gemini embeddings model \emph{gemini-embedding-001} of LLM-generated ``targets'' field as a semantic representation of what each activity is trying to accomplish. This follows \citep{goldembergMindingGapAid2025} in extracting the World Bank Project Development Objectives (PDO), as I find they are very similar to the LLM-generated outputs. I also attempted PDO extraction using regex methods, but found the results were noisy on matching, especially on non-world-bank projects, and did not improve prediction performance as much as embeddings on the llm-generated targets. I first normalize the LLM-extracted targets text into a stable canonical form (removing formatting artifacts, unescaping, splitting on separators, dropping â€œNO RESPONSEâ€ tails, and deduplicating near-identical chunks). I then embed the cleaned targets text for each activity with the \emph{gemini-embedding-001} model, yielding a single high-dimensional vector per activity that summarizes activity objectives (targets) in a continuous latent space.

I then replicate \citep{goldembergMindingGapAid2025} and compress target embeddings using a two-stage dimensionality reduction pipeline. First, I apply Principle Component Analysis (PCA) to reduce the embedding vectors to 50 dimensions and fit UMAP on the PCA outputs to produce 2D and 3D coordinate maps. I find the 3D embeddings (umap\_x, umap\_y, umap\_z) perform better on prediction than 2D. This preserves enough local topology that activities with similar targets remain near each other in the compressed space. I also find qualitatively, that sectors with similar 2D vectors cluster around the activity environmental category, as in \citep{goldembergMindingGapAid2025}. While  \citep{goldembergMindingGapAid2025} find significant signal in deviations from average embeddings for countries or sectors, I do not find these theorized degree of ``contextualization'' features aid forecasting skill when I add them to my model.

By counting the word occurrences between features higher or lower on the UMAP x,y, and z axes, it is possible to investigate what aspects are reported by the embeddings, and the common sectors which they were categorized in. Broadly, low x values correspond to forestry, agriculture, water, and managemant related terms while high x correspond to energy sector and financing related terms. Low y correspond to similar terms as high x, both mostly in the energy sector. High y terms related to biodiversity, wastewater, and conservation. The z axis seems to relate more to urban vs rural: Low z corresponds to wildlife and undp related terms as well as "rural", while high z corresponds to sanitation and wastewater, as well as "urban" and "city". The z axis may also correlate slightly with contextualization, where the more "cookie cutter" objectives with regards to the sector in question are typically correlated with low z values. Lastly, the z axis also has a weak negative correlations with the number of countries: low z values tend to occur in fewer countries.

\textbf{Embeddings for Sector Clustering}

A key component of the efficacy of projects is how they allocate their funding. To do so, I query \emph{gemini-flash-2.5} to identify the breakdown of outcome-related funding, where available in the baseline documents. The allocations of funding are required to approximately sum to the total expenditure of the project, with approximately 55\% of projects having their budgets identified. If no high-scoring finance or budget pages were identified in the categorization step, I fallback to the first 10 pages of the highest rank baseline document. 

In order to make the resulting dataset of budgets usable by the statistical model, I use the embeddings model to cluster the descriptions of the subsectors into 15 sector clusters (having tried 10, 15, and 20 clusters, I find 15 provides the optimal performance on the validation set). I sum the allocations within each cluster and report as a fraction of the total as a feature for the statistical model. 

\subsubsection{LLM Forecasting Method}

Multiple studies have measured zero-shot LLM forecasting capability against the base model performance, and found better general ability base models tend to perform better on forecasting tasks \citep{halawiApproachingHumanLevelForecasting2024} \citep{kargerForecastBenchDynamicBenchmark2024}: In one study with dozens of base models and a dynamically updating benchmark on prediction market forecasting questions, an inverse linear relationship was found between the human preference of a model's answer (in terms of an ELO score) and the Brier score, and similarly a log-linear inverse relationship between the compute used to train the model and the Brier score \citep{kargerForecastBenchDynamicBenchmark2024}.

In order to guard against leakage of information from the training, I selected deepseek %Llama 70B \citep{touvronLlama2Open2023}%
as my forecasting model, due to its strong performance comparable to other models which have similar training cutoff dates (2023 for Deepseek V3.2).

The LLM forecasting method was decided upon by iteratively inspecting both the quality of the response, and the overall accuracy of the forecasts made by the LLM. To generate the LLM forecasting methods, \emph{gemini-2.5-flash} was prompted with a series of ``mock forecasts'', generated by \emph{gemini-2.5-pro}. The ``mock forecast'' used relevant pages retrieved by ranking the categorized topics by forecast informativeness %todo: specify exactly how
and retrieving 10 pages of the most relevant activity data and 10 pages of the most relevant evaluation data, prioritizing pages marked as ``deviations from plans'', ``delays or early completion'', or ``over or under spending'' with a minimum forecasting relevance score of 3/10, and otherwise returning the pages with the highest forecasting relevance score.

To generate each mock forecast, we constructed a retrieval-augmented input consisting of up to 10 baseline pages and up to 10 outcome/evaluation pages per activity. Baseline pages were selected from high-scoring passages in predefined ``forecast-informative'' categories (e.g., objectives, implementation plans, risks, financing details, contextual challenges, and stakeholder/implementer information), using a high relevance threshold (minimum categorization score of 9) and including nearby pages when insufficient high-scoring pages were available. Outcome pages were selected from outcome documents emphasizing deviations from plans (including deviations, delays/early completion, and over/under-spending), using a lower relevance threshold (minimum score of 3) and likewise including surrounding pages to reach the target count when needed. We then merged these retrieved excerpts with activity metadata (title, scope, planned start/end dates, planned financing totals when present) and brief model-generated baseline summaries (activity description and risk summary) before prompting Gemini to write a forecast from the ex-ante perspective. Importantly, the prompt required the model to end by outputting the \emph{known} final evaluation rating for that activity (derived from the merged ratings file and converted into scale-specific text via \texttt{get\_ratings\_text}), while also instructing it to ground the narrative in the retrieved evaluation pages and to return ``NO RESPONSE'' if the evaluation excerpts did not contain sufficient justification for the assigned rating.


The most semantically relevant activities which ended approximately at or before the start of the activity being forecasted was then retrieved (see Section \ref{ssub:nearest_neighbor_vector_similarity_}. In addition, the activity ``risks'' were inserted before each mock forecast, to provide context for the example. Each mock forecast was structured in a way similar to the highest performing scratchpad method from \citep{halawiApproachingHumanLevelForecasting2024}.

A series of features including the activity title, start date, and activity location were injected into the prompt to provide context for the activity, as well as a \emph{gemini-2.5-flash}-generated summary.

% For Method \#2, the most relevant pages of the activity documents were directly converted to text and inserted into the prompt.

Finally, the distribution of rating outcomes was inserted into the prompt, in order to prevent collapse towards only a few ratings.

\subsubsection{LLM Prompting Strategies}

The full prompt template for the LLM Forecast is shown in Figure~\ref{fig:forecast-prompt-multistage}.

\textbf{Few-Shot Block}
In both methods, I use a $k$-nearest-neighbors (KNN) few-shot block of semantically similar activities in the training data (see Section \ref{ssub:nearest_neighbor_vector_similarity_} for how semantic similarity was determined). I selected a range of nearest neighbors. I asked the language model to extrapolate lessons about rating scales for the most similar ``Highly Unsatisfactory'', ``Unsatisfactory'' or ``Moderately Unsatisfactory'', the most similar ``Moderately Satisfactory'', and the most similar ``Satisfactory'' or ``Highly Satisfactory'' rated examples in the training data. A selection of $k=3$ summarized mock forecasts was found to perform better $k=1$, $5$, or $7$.

Each example activity in the few-shot block included (i) key metadata (title and, where available, location and a brief summary), (ii) a short ``risks'' summary, (iii) the retrospective mock forecast analysis, and (iv) the final evaluation outcome label.

\textbf{Additional Prompts}
Two additional prompts were given, and inserted into the final forecast: (1) reasons the activity may have been evaluated as ``Moderately Satisfactory'' or worse, (2) reasons the activity may have been evaluated as ``Moderately Satisfactory'' or better.

The forecasting prompt required a structured response format that explicitly considered both lower- and higher-outcome arguments on the rating scale and ended with a single-line prediction. Concretely, the model was instructed to: (1) provide reasons the overall success might be rated \texttt{\{midpoint\_low\_text\}} or lower, (2) provide reasons it might be rated \texttt{\{midpoint\_high\_text\}} or higher, (3) aggregate considerations and select exactly one of the \texttt{\{num\_options\}} outcomes, and (4) output the final forecast on the last line beginning with \texttt{FORECAST: } followed by only the chosen option.

Finally, I appended a short description of the empirical distribution of rating outcomes in the training data. This was found to reduce mode-collapse toward a narrow subset of ratings.


% \begin{figure}[htbp]
%   \centering
%   \begin{subfigure}{0.48\textwidth}
%     \begin{tcolorbox}[left=4pt, right=4pt, top=4pt, bottom=4pt]
% \ttfamily\footnotesize
% SYSTEM:\newline
% You are an experienced international aid decision maker with a quantitative mindset.
% Forecast the overall evaluation rating from the options:
% \{options\_text\}.\newline
% USER:\newline
% Forecast what the outcome will be for this activity.\newline
% \#\#\# EXAMPLE ACTIVITIES \#\#\#\newline
% [For each neighbor: title; risks; example forecast;
% rating scale; final evaluation outcome]\newline
% \#\#\# NEW ACTIVITY TO FORECAST \#\#\#\newline
% ACTIVITY ID: \{activity\_id\}\newline
% ACTIVITY TITLE: \{activity\_title\}\newline
% ORIGINAL PLANNED START DATE: \{planned\_start\}\newline
% ORIGINAL PLANNED END DATE: \{planned\_end\}\newline
% ACTIVITY LOCATION(S): \{location\}\newline
% ACTIVITY DESCRIPTION (SUMMARY): \{gemini\_generated\_summary\}\newline
% ACTIVITY RISKS: \{risks\_summary\}\newline
% Provide the following format for your response:\newline
% 1. Provide reasons why the overall success might be rated \{midpoint\_low\_text\} or lower.\newline
% 2. Provide reasons why the overall success might be rated \{midpoint\_high\_text\} or higher.\newline
% 3. Aggregate your considerations, and decide on the final outcome among the \{num\_options\} options.\newline
% 4. Provide the final forecast on the last line beginning with 'FORECAST: '
% followed by only the forecast with no extra words.\newline
% [Append: training-set rating distribution text]\newline
% Respond only in English.
%     \end{tcolorbox}
%     \caption{Method \#1 (summary injection).}
%   \end{subfigure}\hfill
%   \begin{subfigure}{0.48\textwidth}
%     \begin{tcolorbox}[left=4pt, right=4pt, top=4pt, bottom=4pt]
% \ttfamily\footnotesize
% SYSTEM:\newline
% You are an experienced international aid decision maker with a quantitative mindset.
% Forecast the overall evaluation rating from the options:
% \{options\_text\}.\newline
% USER:\newline
% Forecast what the outcome will be for this activity.\newline
% \#\#\# EXAMPLE ACTIVITIES \#\#\#\newline
% [Same few-shot block as Method \#1]\newline
% \#\#\# NEW ACTIVITY TO FORECAST \#\#\#\newline
% ACTIVITY ID: \{activity\_id\}\newline
% ACTIVITY TITLE: \{activity\_title\}\newline
% ORIGINAL PLANNED START DATE: \{planned\_start\}\newline
% ORIGINAL PLANNED END DATE: \{planned\_end\}\newline
% ACTIVITY LOCATION(S): \{location\}\newline
% EXCERPTS FROM BASELINE ACTIVITY DOCUMENTS:\newline
% \{pdf\_to\_text\_excerpts\}\newline
% ACTIVITY RISKS: \{risks\_summary\}\newline
% Provide the following format for your response:\newline
% 1. Provide reasons why the overall success might be rated \{midpoint\_low\_text\}.\newline
% 2. Provide reasons why the overall success might be rated \{midpoint\_high\_text\}.\newline
% 3. Aggregate your considerations, and decide on the final outcome among the \{num\_options\} options.\newline
% 4. Provide the final forecast on the last line beginning with 'FORECAST: '
% followed by only the forecast with no extra words.\newline
% [Append: training-set rating distribution text]\newline
% Respond only in English.
%     \end{tcolorbox}
%     \caption{Method \#2 (raw text injection).}
%   \end{subfigure}
%   \caption{Prompt templates for LLM forecasting Methods \#1 and \#2. The methods share the same scaffold (few-shot examples, metadata, risks, and response-format constraints) and differ only in how activity context is injected (summary vs.\ raw document text).}
%   \label{fig:forecast-prompts}
% \end{figure}



% =========================
% FIGURE 1: Multi-stage final forecast prompt (single method)
% =========================
\begin{figure}[htbp]
  \centering
  \begin{tcolorbox}[left=4pt, right=4pt, top=4pt, bottom=4pt, width=0.96\textwidth]
\ttfamily\footnotesize
SYSTEM:\newline
You are an experienced international aid decision maker with a quantitative mindset.
Respond with a comprehensive, thorough forecast of what the overall evaluation rating of the activity will be,
from the options of \{options\_text\}.\newline

USER:\newline
Forecast what the outcome will be for this activity.\newline

\#\#\# Lessons from similar activities \#\#\#\newline
\{knn\_summary\_text\}\newline
% (If no summary is available, this block can instead be the raw few-shot examples:) \newline
% \{few\_shot\_example\_activities\}\newline
\#\#\# End lessons \#\#\#\newline

\#\#\# Additional specific information about the activity that you summarized \#\#\#\newline
\{rag\_synthesis\_additional\_info\}\newline
\#\#\# End of additional information you summarized \#\#\#\newline

ACTIVITY ID: \{activity\_id\}\newline
ACTIVITY TITLE: \{activity\_title\}\newline
ORIGINAL PLANNED START DATE: \{planned\_start\}\newline
ORIGINAL PLANNED END DATE: \{planned\_end\}\newline
ACTIVITY SCOPE: \{activity\_scope\}\newline
PLANNED TOTAL DISBURSEMENT (USD): \{planned\_total\_disbursement\_usd\}\newline
ACTIVITY LOCATION(S): \{locations\}\newline
LOCATION GDP PER CAPITA, USD: \{gdp\_percap\}\newline
PARTICIPATING ORGANIZATIONS: \{reporting\_orgs\}\newline
IMPLEMENTING ORGANIZATION CATEGORY: \{either "Government" or "NGO", otherwise line not inserted\}\newline

ACTIVITY DESCRIPTION: \{chatgpt\_description\}\newline
ACTIVITY TARGETS: \{targets\_summary\}\newline
ACTIVITY CONTEXT: \{activity\_context\}\newline
ACTIVITY COMPLEXITY: \{complexity\_details\}\newline
ACTIVITY INTEGRATEDNESS: \{how\_integrated\_description\}\newline
FINANCING DETAILS: \{finance\_summary\}\newline
IMPLEMENTER PERFORMANCE CONTEXT: \{implementer\_performance\_text\}\newline
ACTIVITY RISKS:\newline
\{risks\_summary\}\newline
ACTIVITY POSSIBILITIES: \{possibilities\_summary\}\newline


% -------------------------
% Final-stage (s3) instruction scaffold (this is the prompt whose structure you show)
% -------------------------
\{training-set rating distribution text\}\newline
Here are a few reasons that you said the answer might be ``Moderately Satisfactory'' or worse:\newline
\{insert\_stage\_s1\_answer\_here\}\newline
Here are a few reasons that you said the answer might be ``Satisfactory'' or better:\newline
\{insert\_stage\_s2\_answer\_here\}\newline

YOUR TASK:\newline
Aggregate your considerations above. Think like a superforecaster (e.g.\ Nate Silver).
On the very last line of your response, write `FORECAST: ' followed by exactly one option from this rating scale with no extra words:\newline
\{options\_text\}\newline

Respond only in English.
  \end{tcolorbox}
  \caption{Single-method multi-stage forecasting prompt. Stages s1 and s2 are run as separate calls, and their outputs are inserted into the final (s3) prompt via \{\texttt{insert\_stage\_s1\_answer\_here}\} and \{\texttt{insert\_stage\_s2\_answer\_here}\}.}
  \label{fig:forecast-prompt-multistage}
\end{figure}


\textbf{Ensembling}
Ensembling is simply averaging many individual forecasts of the same model, prompted in slightly different ways. I found ensembling was a relatively weak method while validating, so I reserve the ensembling method for the final held-out set, which will come once this thesis has reached completion. 
%All methods which demonstrated above-chance skill in forecasting were averaged using the mean value of the forecast. This was found to robustly outperform any individual forecasting method, although with only a modest overall improvement due to high autocorrelation between individual forecasts. Ensembles of LLM-generated forecasts using differing random seeds and differently selected K-Nearest-Neighbors. %(with a different weight on most recent vs most semantically similar). A summary of the 10 pages was also included (method \#3).
% More details will come, once I am sure exactly how I plan to implement the methods. %TODO

% \textbf{Fine-tuning}
% The Llama 70B model was fine-tuned using past paper results and pairings collected before the model cutoff date. In past work, even though data were in the training data, fine-tuning significantly improved prediction performance \citep{wenPredictingEmpiricalAI2025}. \\


% NOTE: MORE DETAILS ON FINE-TUNING TO COME, ONCE ESTABLISH THAT I REALLY HAVE TIME TO DO THIS %TODO

% \textbf{}
\textbf{Fine Tuning}
In past work in a similar domain, fine-tuning significantly improved forecasting performance \citep{wenPredictingEmpiricalAI2025}. I attempted to fine-tune \emph{gemini-2.5-flash} using Vertex AI. Do do so, I used Direct Preference Optimization (DPO), which requires one example of a good prompt, and one example of a bad prompt. Out of a random sample of 100 activity IDs in the training data, I forecasted the final forecast stage 5 separate times using \emph{deepseek-V3.2}. I used 50 pairs where the model forecasted one rating increment closer than another rating, as the pairs of forecasts.

It was often the case that there were multiple options for the best or worse forecast due to the limited 6-point scale. In order to choose among forecasts that were equally good or equally bad, I also took the embeddings of the forecasts and found the consine similarity to the embeddings of the mock forecast, and to embeddings of the outcome document and averaged these similarity scores. The most similar among equally good ratings were chosen as the good example for the DPO training pair, and the least similar among equally bad ratings as the bad example.  

Once the 50 pairs were identified, I ran the fine-tuning using default settings over 20 epochs from Vertex AI (Learning rate multiplier of 1, Adapter size of 4, Beta of 0.1).

\subsection{Scoring Metrics} \label{sub:scoringmetrics}

% We combine Brier score, calibration, and accuracy into a composite forecasting skill metric to attempt to mitigate the various issues in the individual metrics. In general, we only proceed with adding an accuracy boosting feature if it does not worsen any individual metric. We also utilize a held-out test set to ensure the validation metrics remain similarly performant in the final dataset. In event forecasting, scoring Metrics are typically used to quantify forecaster skills. A scoring rule is ``proper'' if the forecaster maximizes the expected score for an observation drawn from the distribution F if they issue the probabilistic forecast F, rather than G $\neq$ F \citep{gneitingStrictlyProperScoring2007}. Brier score and log-loss are both strictly proper, but accuracy is not, limiting its comparability to other domains. However, accuracy has the advantage of intuitive simplicity.

\textbf{Accuracy}
The percent of the time the correct rating is forecasted. Non-integers are rounded to integers.

\textbf{Side Accuracy}
The percent of correctly predicted ``Satisfactory'' or above vs ``Moderately Satisfactory'' or below (above or below 3.5). Approximately 50\% of the training dataset sits above and approximately 50\% sits below this boundary.

\textbf{RMSE (Root Mean Square Error)}
Take the square of the difference between every prediction and the true value, take the mean of all such squared values, then take the square root. Measure of â€œaverageâ€ distance. Lower is better. On a scale from 0 to 5, therefore worst possible value is 5, best possible value is zero. This method heavily penalizes predictions that are significantly incorrect.

\textbf{MAE (Mean Absolute Error)}
Measure of â€œaverageâ€ distance, by taking the mean of the absolute value of the residuals. Lower is better. On a scale from 0 to 5, therefore worst possible value is 5, best possible value is zero. This method does not heavily penalize predictions that are significantly incorrect.

\textbf{Coefficient of Determination ($R^2$)}
$R^2$: Coefficient of determination. Theoretically equals zero, if we always choose the mean (however using the training set mean results in a lower score on the test set in the baseline measure below). If more than 1 regressors are included, $R^2$ is the square of the coefficient of multiple correlation and can be negative. Measures proportion of the variation in the dependent variable that is predictable from the independent variable. Higher is better. This method generally does not penalize outliers significantly.

\textbf{Adjusted $R^2$}
Adjusted $R^2$ is a version of $R^2$ that accounts for the number of regressors in the model. Unlike plain $R^2$, it penalizes adding predictors that do not meaningfully improve fit, making it more appropriate when comparing models with different numbers of features. It can decrease when irrelevant regressors are included, and it can be negative. Higher is better. While it penalizes extra parameters that may lead to overfitting, adjusted $R^2$ within a training set does not reflect model skill as accurately as out-of-time $R^2$.

\textbf{Pairwise Probability}
Pairwise probability is evaluated as the proportion of pairs of individual predictions in the validation or test set that were correctly ordered from lower to higher rating. This method is insensitive to global shifts in ratings, which may occur due to events like the COVID 19 pandemic. It also is insensitive to calibration of the spread of possible outcomes. Given the significant noise related to globally relevant challenges, it can represent a more achievable and informative metric than $R^2$ or MAE for model forecasting skill

% \begin{table}[htbp]
% \centering
% \caption{Caution when ranking forecasting systems using single metrics: each evaluation metric has its own issues \citep{palekaPitfallsEvaluatingLanguage2025}.}
% \label{tab:forecasting-metrics-pitfalls}
% \renewcommand{\arraystretch}{1.2}
% \setlength{\tabcolsep}{5pt}
% \begin{tabular}{|p{0.18\textwidth}|p{0.28\textwidth}|p{0.48\textwidth}|}
% \hline
% \textbf{Metric} & \textbf{Method} & \textbf{Pitfalls when using for ranking outcomes} \\
% \hline
% Calibration &
% Consider all questions where the forecaster predicts a probability close to $p$. 
% The forecaster has good calibration if the proportion of questions where the outcome 
% is ``Yes'' is close to $p$. 
% It is usually measured over ``bins'' of questions based on the predicted probability. 
% $\Pr(Y=1 \mid \hat{p}=p) \approx p$. &
% Calibration can penalize useful forecasting. \emph{Card example:} Guessing a card suit and rank in a 52 card deck. A base-rate forecaster that predicts $1/52$ for every card is perfectly calibrated. A more discerning forecaster assigns $10\%$ to five ``front-runners'' and $0.5\%$ to the remaining 47. If their ranking is genuinely informative so that the true card lies among the top five in $60\%$ of rounds, the observed success rates are about $12\%$ in the $10\%$ bin and $0.8\%$ in the $0.5\%$ bin so calibration looks worse. Yet this forecaster is far more useful.\\
% \hline
% Accuracy &
% Share of correct binary classifications after thresholding probabilities 
% (e.g., predict ``Yes'' if $p \ge 0.5$, otherwise ``No''). 
% $\displaystyle \text{Acc} = \tfrac{1}{N}\sum_{i=1}^N [\hat{y}_i = y_i]$. &
% Accuracy rises when there are fewer options, and when the outcome itself occurs more often. Therefore, it should not be used to compare between outcomes with variable statistical distributions. \\
% \hline
% Brier score &
% Mean squared error between predicted probability and outcome, averaged across questions 
% (lower is better). 
% $\displaystyle \text{Br} = \tfrac{1}{N}\sum_{i=1}^N (p_i - y_i)^2$. &
% Averages of Brier score overweight mid-probability discrimination relative to rare-event skill.

% \emph{Example:} Out of 120 ``Good'' or ``Bad'' outcomes, 60 are mid-probability with a 40\% base rate (half 60\% ``Good'' events and half are 20\% ``Good'' events). The remaining 60 are rare, events with a 2\% ``Good'' rate.  
% \begin{minipage}[t]{\linewidth}\begin{itemize}
% \item Forecaster A is perfect on rare events but predicts 0.4 on all mid-prob questions, yielding an average Brier of $0.12$. 
% \item Forecaster B is baseline on rare events (predicts 0.02) but discriminates mid-prob questions (predicts 0.6 for 60\% cases and 0.2 for 20\% cases), yielding an average Brier of $\approx 0.1$. 
% Despite being useless on rare events, B is ranked better overall.
% \end{itemize}\end{minipage}
% \\ \hline
% \end{tabular}
% \end{table}

% % NOTE: CHAT_GPT BASED LOG-LOSS - NEED TO DOUBLE-CHECK %TODO
% \begin{table}[htbp]
% \centering
% \caption{Caution when ranking forecasting systems using single metrics: each evaluation metric has its own issues \citep{palekaPitfallsEvaluatingLanguage2025}.}
% \label{tab:forecasting-metrics-pitfalls}
% \renewcommand{\arraystretch}{1.2}
% \setlength{\tabcolsep}{5pt}
% \begin{tabular}{|p{0.16\textwidth}|p{0.28\textwidth}|p{0.20\textwidth}|p{0.30\textwidth}|}
% \hline
% \textbf{Metric} & \textbf{Method} & \textbf{Equation} & \textbf{Pitfalls when using for comparing forecasting skill} \\
% \hline
% Calibration &
% Consider all questions where the forecaster predicts a probability close to $p$; compare predicted and observed frequencies across bins. &
% $\Pr(Y=1 \mid \hat{p}=p) \approx p.$ &
% Can penalize useful forecasting; depends on binning; see text below. \\
% \hline
% Accuracy &
% Percent of correct classifications after thresholding probabilities (e.g., predict ``Yes'' if $p \ge 0.5$). &
% $\displaystyle \mathrm{Acc}=\frac{1}{N}\sum_{i=1}^N [\hat{y}_i = y_i].$ &
% Rises with class imbalance and fewer options; not comparable across outcomes with different base rates. \\
% \hline
% Brier score &
% Mean squared error between predicted probability and outcome (lower is better). A brier score is strictly proper. More outcome categories will raise the brier score (as the correct outcome is more difficult to predict)&
% $\displaystyle \mathrm{Br}=\frac{1}{N}\sum_{i=1}^N (p_i - y_i)^2.$ &
% Overweights mid-probability discrimination relative to rare-event skill; see text below. \\
% \hline

% Logarithmic score &
% Strictly proper scoring rule for multi-category outcomes (often reported as negative log-loss). &
% $ \ell(\mathbf{p}, y)=\log{p_{y}}$ &
% Extremely sensitive to overconfident errors; undefined at $p_{y}=0$ without clipping; scale depends on log base. \\
% \hline


% % Logarithmic score &
% % Proper log scoring rule (often reported as negative log-loss). &
% % $\displaystyle \ell(p,y)=y\log p + (1-y)\log(1-p).$ &
% % Extremely sensitive to overconfident errors; undefined at $p\in\{0,1\}$ without clipping; scale depends on log base. \\
% \hline


% \end{tabular}
% \end{table}

% Each form of evaluating the quality of forecasts has its own limitations. 

% \paragraph{Calibration issues}
% Calibration can penalize useful forecasting. For example, guessing a card suit and rank in a 52-card deck. A base-rate forecaster that predicts $1/52$ for every card is perfectly calibrated. A more discerning forecaster assigns $10\%$ to five ``front-runners'' and $0.5\%$ to the remaining 47. If their ranking is genuinely informative so that the true card lies among the top five in $60\%$ of rounds, the observed success rates are about $12\%$ in the $10\%$ bin and $0.8\%$ in the $0.5\%$ bin so calibration looks worse, yet this forecaster is far more useful.

% \paragraph{Brier score issues}
% Out of 120 ``Good'' or ``Bad'' outcomes, 60 are mid-probability with a 40\% base rate (half 60\% ``Good'' events and half are 20\% ``Good'' events). The remaining 60 are rare, events with a 2\% ``Good'' rate.  
% \begin{itemize}
% \item Forecaster A is perfect on rare events but predicts 0.4 on all mid-prob questions, yielding an average Brier of $0.12$. 
% \item Forecaster B is baseline on rare events (predicts 0.02) but discriminates mid-prob questions (predicts 0.6 for 60\% cases and 0.2 for 20\% cases), yielding an average Brier of $\approx 0.1$. 

% Despite being useless on rare events, B is ranked better overall.

% \end{itemize}
% Consider 120 ``Good''/``Bad'' outcomes. Sixty are mid-probability with a 40\% base rate (half are 60\% ``Good'' and half are 20\% ``Good''). The remaining 60 are rare events with a 2\% ``Good'' rate.
% \begin{itemize}
% \item Forecaster A is perfect on rare events but predicts $0.4$ on all mid-probability questions $\Rightarrow$ average Brier $=0.12$.
% \item Forecaster B is baseline on rare events (predicts $0.02$) but discriminates mid-probability questions (predicts $0.6$ for the 60\% cases and $0.2$ for the 20\% cases) $\Rightarrow$ average Brier $\approx 0.10$.
% \end{itemize}
% Despite being useless on rare events, B is ranked better overall.

% \paragraph{Logarithmic score issues}
% The logarithmic score \citep{gneitingStrictlyProperScoring2007} is proper but has practical issues:
% \begin{itemize}
% \item \textbf{Extreme penalties:} overconfident mistakes with $p\approx 0$ when $y=1$ (or $p\approx 1$ when $y=0$) dominate the average.
% \item \textbf{Undefined at the boundaries:} $\log 0$ is undefined, so implementations clip $p\in[\epsilon,1-\epsilon]$, and results depend on $\epsilon$.
% \item \textbf{Unit dependence:} the scale changes with the log base, complicating comparisons across papers.
% \item \textbf{Dataset mix sensitivity:} comparisons can be distorted when outcome prevalence differs across evaluation sets.
% \end{itemize}

% \paragraph{Logarithmic score issues}
% \begin{itemize}
% \item \textbf{Extreme penalties:} overconfident mistakes with $p_y\!\approx\!0$ dominate the average.
% \item \textbf{Undefined at the boundaries:} $\log 0$ is undefined, so implementations clip $p_y\in[\epsilon,1-\epsilon]$ and results depend on $\epsilon$.
% \item \textbf{Unit dependence:} the scale changes with the log base, complicating comparisons across papers.
% \item \textbf{Dataset mix sensitivity:} comparisons can be distorted when outcome prevalence differs across evaluation sets.
% \end{itemize}


% \subsection{Outcome Grading}
% Next, each outcome was evaluated in multiple ways.

% First, a grading scheme was identified as a useful taxonomy, which could allow easy comparison between the predictions and the test set:
% \begin{itemize}
% \item [1.] \textbf{Very significant}: Substantial improvement with robust evidence
% \item [2.] \textbf{Significant}: Noticeable improvement with moderate evidence
% \item [3.] \textbf{Neutral/mixed results}: Some improvement but limited or unclear
% \item [4.] \textbf{No effect}: No discernible impact
% \item [5.] \textbf{Outcome was worsened}: Negative impact
% \end{itemize}

% Grading for free-form prediction was also allowed, whereby GPT-o4-mini was used to directly compare a free-form prediction of the outcome, to the outcome described in the abstract. NOTE: FURTHER DETAILS WILL COME UPON IMPLEMENTING THIS METHODOLOGY %TODOf


% For each forecast, a qualitative free-form forecast is generated. Subsequently, a grade on the 5-point scale is also generated.

% \subsection{Grading Forecast Accuracy}

% \ifdev{ \subsection{NOTE: YES THEY REALLY WANT THIS: Forecasting the Confidence of a Forecast}
% In addition to forecasting the outcome of an intervention, it is also useful to classify just how confident the forecast is. To do so, we prompt the model to produce a confidence score. OPTIONAL, NOT SURE HOW MANY OF THESE IDEAS TO USE Research on LLM confidence scoring shows that ``consistency'' based approaches are more accurate than eliciting confidence directly from the model \citep{lyuCalibratingLargeLanguage2025}. Agreement-based consistency works best for
% open-source models. Because I already produce $K$ independent forecasts using differing reasoning prompts, the empirical variance across sample predictions provides an agreement-based forecast confidence for Llama 70B. For Llama 70B I also use the probability of the classified output token as a further reported metric where applicable and determine the correlation between the consistency of a prediction and the logit-based token probabilities.
% We use the FSD approach for closed-source models as this approach has been shown to . 

% Finally, I separately fine-tune a small language model to predict forecastability using the training dataset based on the intervention description and the log-loss of the outcome score. I report how correlated all of these metrics are and use a weighted combination to provide a final composite ``forecastability'' prediction.
% }

\subsubsection{Grading Free Form Forecasts}
In addition to extracting the LLM forecasted rating, I also use \emph{gemini-2.5-flash-lite} to grade free-form textual format forecasts. In order to do so, I had \emph{gemini-2.5-flash} first summarize pages categorized as highly relevant to outcome evaluations, obtaining 10 pages with a score of at least 3/10 marked as "deviation\_from\_plans", "delays\_or\_early\_completion", or "over\_or\_under\_spending", or failing that, that were graded as highly relevant to activity evaluation. If no such pages were categorized, the first 10 pages of the highest ranked activity outcome evaluation were used. Next, for all ensembles of LLM forecasts, a grade was given for how similar the forecast was to the activity outcome using \emph{gemini-2.5-flash-lite}. Grading was performed based on two key criteria: accurately identification of likely drivers of activity success or failure, and identification of likely outcomes being forecasted. 

In accordance with the typical US grading scheme definitions, I define an "F" grade as 55 and an A+ grade as approximately 97, with other grades defined in even intervals. I provide \emph{gemini-2.5-flash-lite} the following rubric:


\textbf{Grading scale}
\begin{itemize}
\item A+/A/A-: Excellent forecast, highly accurate, attention on key drivers, multiple major events forecasted accurately.
\item B+/B/B-: Good forecast, mostly accurate. A mix of correct and incorrect, but at least one major outcome was forecasted. At least one key driver identified.
\item C+/C/C-: Adequate forecast, partially accurate or reasonable. Focus was adequate. Major outcomes incorrect, but some smaller aspects were correct.
\item D+/D/D-: Poor forecast, although perhaps one or two small correct things. Mostly inaccurate. Wrong focus on drivers.
\item F: Failed forecast, completely wrong or unsupported.
\end{itemize}



\clearpage
\FloatBarrier
\section{Results}
I provide a summary of the data made available from this work, provide an analysis of the success of the various methods at forecasting evaluation ratings, and provide preliminary results on the ability for the model to forecast specific outcomes.
In order to provide a robust, generalizable forecast of activity success, two primary strategies were employed. The first was an LLM-implemented judgmental forecasting method. The second method was to use statistical methods. Both strategies were found to have separate strengths and weaknesses:
\begin{figure}[!htbp]
\centering
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{p{0.45\linewidth} p{0.45\linewidth}}

\textbf{Strengths} &
\textbf{Weaknesses} \\

\textbf{LLM Forecasting}
\begin{itemize}
  \item Can explicitly reason and identify missing information
  \item General reasoning skills may transfer across domains
  \item Can produce text-based forecasts of specific events
\end{itemize}

\textbf{Statistical Models}
\begin{itemize}
  \item Cheap and fast to iterate
  \item Can use large numbers of features without context limits
  \item Can incorporate LLM forecasts and grades as features
  \item Efficiently generalize over large datasets
  \item Easier to prevent future leakage
  \item Mature methodology
\end{itemize}

&
\textbf{LLM Forecasting}
\begin{itemize}
  \item Expensive and slow to iterative
  \item Difficult to calibrate
  \item Fine-tuning is expensive, and unavailable for best models
  \item Limited interpretability of model reasoning
  \item Best models are closed-source
  \item Risk of training-data leakage
  \item Limited context window constrains attention and calibration
\end{itemize}

\textbf{Statistical Models}
\begin{itemize}
  \item Cannot perform explicit reasoning
  \item Cannot directly use rich textual or world knowledge
  \item More prone to overfitting dataset-specific quirks
\end{itemize}

\\
\end{tabular}
% \caption{Strengths and Weaknesses of Large Language Models and Statistical Models for Forecasting}
\label{fig:forecast_strengths_weaknesses}
\end{figure}


\FloatBarrier
After consideration of several metrics, I decided the Pairwise probability and $R^2$ were the most appropriate for assessing model skill in the domain of ratings and activity quantitative outcomes. $R^2$ is sensitive to bias and outliers, which are important for assessing absolute predictive accuracy. However, a common use-case in aid funding decision making is comparing a pair of activities, or even a suite of many activities. In such a use-case, the Pairwise probability is more representative of what is needed. Furthermore, a weakness in $R^2$ is that global shifts in the ratings and outcomes due to external factors can be unpredictable, and a model may by-chance capture these shifts without any real improvement in forecasting skill. Both are clearly interpretable: always forecasting the mean value is 0, equivalent to 0\% of variance explained for $R^2$, while 50\% is equivalent to the pairwise forecast of random chance.

\subsection{Forecasting Ratings with LLMs}

One research question of this thesis is how do differing methods of llm forecasting apply in in the context of forecasting international aid success ratings. I find that more generally capable models are consistently better forecasters. I find otherwise little evidence that any other forecasting techniques improve model performance. The strategy of incorporating additional information relevant to the forecasting question as well as references to similar activities does not clearly improve forecasting skill. I find no evidence of data leakage contaminating the ratings. I also find little evidence that fine-tuning can improve forecasting accuracy in this domain, although a thorough assessment of the potential for fine-tuning \emph{gemini-2.5-flash} was cost prohibitive. I also find language models on their own consistently significantly underperformed statistical models in forecasting skill.

Using statistical models as the drivers of LLM forecasts, I find that the assessed similarity between forecasts and summaries of outcome evaluations strongly correlate with improved ability to predict ratings, and that statistical models significantly improve the quality of LLM free-form forecasts.

In addition to extracting the LLM forecasted rating, I also assess LLM grades. I find that the assessed similarity between forecasts and summaries of outcome evaluations strongly correlate with improved ability to predict ratings (see Figure \ref{fig:grades}). Furthermore, I find that when the predicted rating using the best statistical model is injected into the prompt, LLM forecasts of activities become much more similar to the summaries of their outcome evaluations. This provides evidence that while language models can accurately forecast the reasons activities would succeed or fail if given the evaluation ratings, they generally lack the ability to correctly weigh the various factors to produce a calibrated forecast.

While ratings are not clearly correlated with better similarity grades, RAG, KNN retrieval, and extra reasoning of the models (stages 1 and 2) consistently improve free-form textual similarity to the outcome, albeit by a small margin. Simply presenting the grading model with the \emph{gemini-2.5-flash} summary of risks of the project from the project documents lead to a forecasting score of 80.2 (between a ``C+'' and a ``B-'', corresponding to an ``Adequate'' forecast, with only ``Adequate'', rather than a good, focus on the drivers). However, the risks did not typically include any affirmative forecasts of outcomes, which may in part explain the low ratings. The best forecasting method in terms of similarity to outcomes was the forced ratings with a large amount of context (RAG+KNN) and included stage 1 and 2, achieving a grade of 88.5, which corresponds to a "B+" grade given by \emph{gemini-2.5-flash-lite} (which sits midway between ``Good'' and ``Excellent'', meaning the forecasts were on average between ``mostly accurate'' and ``highly accurate'', at  at least one major outcome was forecasted, and at least one key driver of the outcome was identified on average). Without any additional methods, \emph{deepseek-v-3.2} can typically achieve a "B" grade on average, while use of the statistical model or of \emph{gemini-3-pro} can lift scores to the "B+" range. The improvement in ability to forecast overall ranking improves significantly as well.

\textbf{Fine Tuning}
I found the training loss for \emph{gemini-2.5-flash} steadily reduced over the course of the fine-tuning, reducing by 95\%. I tested this on an early subset of the validation data which excluded BMZ activities. The performance results were mixed. While the $R^2$ performance improved modestly, it remained worse than simply guessing the mean rating in the validation set, and remained far below the performance of the more powerful \emph{gemini-3-pro}. Furthermore, the DPO objective is specifically intended to improve pairwise performance between two pairs of forecasts. However, the Pairwise probability score was slightly worsened by fine-tuning. The cost for the fine-tuning of 20 epochs for 50 pairs was close to 140 euros. Therefore, scaling up larger training points for fine-tuning does not seem to be a promising strategy, either to reach the performance of \emph{gemini-3-pro}, or to reach the performance of the more powerful combined statistical models.

\textbf{RAG and KNN}
I find small differences in in forecasting skill for the addition of RAG and KNN on forecasting ratings. However, the free-form similarity scores consistently improve on the validation set when RAG and KNN context is included. This indicates that while the LLM is able to identify more important information when reasoning about the final forecast, it is insufficiently calibrated to use the additional information it has gathered to improve the forecast itself.

\textbf{LLM Model Selection}
I find no consistent difference between \emph{gemini-2.5-flash} and \emph{deepseek-v3.2} in forecasting skill. However, \emph{gemini-3-pro} appears to be significantly better at forecasting than both, in both sets of activities achieving higher similarity grades, higher $R^2$, and higher Pairwise probability than other models. The only exception to this is the models which have been instructed to report an overall forecast which matches the forecast from statistical models.


\begin{figure}[!htbp]
  \centering
   % \includegraphics[width=0.9\textwidth]{assets/4pane_mean_grade_vs_r2_and_pairwise.png}
   \hspace*{-0.08\textwidth}
   \includegraphics[width=1.1\textwidth]{assets/4pane.png}

  \caption{A comparison of various LLM forecasting methods. Notably, \emph{gemini-3-pro} outperforms other models when considering similarity grades. While information injection appears to help the forecasts come to their conclusions for the right reasons, it also appears to harm forecasting skill. When only the Activity ID, title, and a \emph{gemini-2.5-flash} summary of the most important pages for activity forecasting were injected, the LLM performed better on absolute ratings. When forced, somewhat better free-form grades are achievable than without forcing, but only if additional information such as RAG and KNN is given to the model.}
  \label{fig:scatterllmcorrected}
\end{figure}
\FloatBarrier

\subsection{Forecasting Ratings with Statistical Models}
Overall, the forecasting system I produce is capable of forecasting evaluation ratings significantly above chance on out-of-time activities. Compared to prior work \citep{ashtonPuzzleMissingPieces2023}, I report a  value consistent with an adjusted $R^2$ with training set (I report an $R^2$ of 0.34 and an adjusted $R^2$ of 0.29 for within-training set correlations on primarily world bank ratings, while others report at maximum an adjusted $R^2$ of ~0.3 \citep{goldembergMindingGapAid2025}. I was not able to identify comparable out-of-time, time-ordered split analysis in the literature. As expected for forecasting under data distribution shift, my results on the out-of-time validation set were somewhat weaker, with an $R^2$ of 0.23 for the random forest model, and an $R^2$ of 0.26 when incorporating the language model forecasting results correction + recency model.

\subsubsection{Overfitting Corrections}
$R^2$ was chosen as the ``Adjusted $R^2$'' has been used in similar work to evaluate model performance in the development aid literature and penalize overfitting by reducing the reported $R^2$ as a function of the number of input parameters. Mirroring similar reported methods in the literature, I calculated adjusted $R^2$ on the training points. While this is sensitive to overfitting, it is a common practice in the development aid literature. However, I find adjusted $R^2$ within the training set is highly sensitive to the specific parameters of the RF model and the subsequent degree of overfitting, such that adjusted $R^2$ increases to above 0.6 with default random forest parameters, while performance on the validation set drops (See Table \ref{tab:method_comparison_validation}. I conclude that adjusted $R^2$ should not be used as a measure of forecast skill.

Relative to the default \texttt{RandomForestRegressor} configuration (e.g., \texttt{n\_estimators}=100, \texttt{max\_depth}=\texttt{None}, \texttt{min\_samples\_split}=2, \texttt{min\_samples\_leaf}=1, \texttt{max\_features}=1.0, \texttt{bootstrap}=\texttt{True}, \texttt{ccp\_alpha}=0.0), the specification used here deliberately constrains model capacity in ways that typically reduce overfitting. Trees are explicitly depth-limited (\texttt{max\_depth}=14 rather than unbounded) and splits are only permitted when nodes contain substantially more data (\texttt{min\_samples\_split}=20 and \texttt{min\_samples\_leaf}=20), which smooths forecasts by limiting fine-grained partitioning of the feature space. In addition, using a smaller feature subset at each split (\texttt{max\_features}=0.488) increases tree diversity and reduces variance relative to the default that considers all features. The model also uses row subsampling (\texttt{max\_samples}=0.86), further reducing variance by injecting additional randomness into each tree's training set. Overall, compared to defaults, these choices trade some bias for a meaningful reduction in variance, making the fitted ensemble less susceptible to overfitting.


\subsubsection{Embedding Targets}
The language model derived features modestly aided forecast accuracy, in aggregate providing an improvement of about 7\% additional explanation of the variance of outcomes out of the 26\% discoverable by the RF model (See Table \ref{tab:method_comparison_validation}). The  finance, integratedness, implementer\_performance, targets, context, risks, and complexity features were directly inserted as grades from the model.


% =======================
% PREAMBLE (add once)
% =======================
\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{4}

% % LaTeX has no \subsubsubsection by default
% \newcommand{\subsubsubsection}[1]{\paragraph{#1}\mbox{}\\}


% =======================
% BODY (paste in thesis.tex)
% =======================


\subsubsection{Recency and LLM Adjustment Ridge Regression}
\label{subsubsub:recency_llm_adjustment}

I wanted to both correct the random forest model for temporal distribution shift (e.g., changing reporting practices, evaluation standards, portfolio composition, and macro conditions), and incorporate any usable information from the direct LLM forecasts. Even when the input features are stable, the conditional relationship $p(y \mid x)$ can drift, so a model trained on older activities can become mis-calibrated on newer ones. Furthermore, I found the LLM forecasts were significantly correlated with prediction error in the validation set.

\textbf{Residual-correction formulation}

Let $\hat{y}^{\text{RF}}_i$ be the random forest prediction for activity $i$, and let $\hat{y}^{\text{LF}}_i$ denote the LLM Forecast. I define the random-forest residual on the i'th activity as:
\[
r_i := y_i - \hat{y}^{\text{RF}}_i .
\]
I then fit a ridge regression model to predict residuals from a small feature vector consisting of the RF prediction and (optionally) the LLM Forecast:
\[
\hat{r}_i := \beta_0 + \beta_1 \hat{y}^{\text{RF}}_i + \beta_2 \hat{y}^{\text{LF}}_i ,
\]
with an $\ell_2$ penalty on $(\beta_1,\beta_2)$ controlled by \texttt{alpha} (ridge strength). The corrected prediction is:
\[
\hat{y}^{\text{corr}}_i := \operatorname{clip}_{[0,5]}\!\left(\hat{y}^{\text{RF}}_i + \lambda \hat{r}_i\right),
\]
where $\lambda$ is a scaling factor (set to 1.0 in my experiments) and clipping enforces the valid rating range between 0 and 5.

This is a simple stacked model: the RF provides the base signal, and ridge regression learns an adjustment to remove systematic residual error that appears in the recent/LLM-covered slice.

I tested two separate methods:
\begin{enumerate}
\item Recency correction (RF re-calibration on recent activities).
In this variant I remove the LLM forecast entirely fixing $\beta_2=0$, but still calculate an offset $\beta_0$ and scaling $\beta_1$ on the 150 latest training examples.

\item LLM-informed correction (recency + LLM Forecast).
In this variant, the ridge model uses both the RF prediction and the LLM Forecast as covariates on the activities where $\hat{y}^{\text{LF}}_i$ is available. This allows the correction to learn a mapping from $(\hat{y}^{\text{RF}}_i, \hat{y}^{\text{LF}}_i)$ to the residual $r_i$, effectively learning when the LLM Forecast contains signal about systematic RF error on the recent slice. The correction is applied only to activities where $\hat{y}^{\text{LF}}_i$ exists; otherwise, forecasts fall back to the uncorrected RF output.
\end{enumerate}
 

% requires: \usepackage{booktabs}
\begin{table}[t]
\centering
\small
\setlength{\tabcolsep}{6pt}
\renewcommand{\arraystretch}{1.15}

\caption{Validation performance in forecasting ratings across forecasting methods. Rows are sorted by ascending $R^2$ (higher is better). RMSE and MAE are lower-is-better, others are better if higher. Bold indicates the best value in each metric column. Variation between side-accuracy methods (``Moderately Satisfactory'' or lower vs ``Satisfactory'' or lower) were not statistically significant. The ``recency correction'' variants combine models using the 150 latest-starting activities in the training set for calibration/combination. The ``no LLM features'' Random Forest excludes the following features: finance, integratedness, implementer\_performance, targets, context, risks, complexity, umap3\_x, umap3\_y, umap3\_z.}
\label{tab:method_comparison_validation}
\begin{tabular}{p{3cm}rrrrrr}
\toprule
Method & $R^2$ $\uparrow$ & RMSE $\downarrow$ & MAE $\downarrow$ & Side Acc. $\uparrow$ & Acc. $\uparrow$ & Pairwise $\uparrow$ \\
\midrule
RF + LLM Forecast + recency & \textbf{0.258} & \textbf{0.815} & 0.590 & \textbf{0.760} & \textbf{0.547} & \textbf{0.767} \\[3pt]
RF + recency & 0.254 & 0.817 & 0.593 & 0.750 & \textbf{0.547} & 0.767 \\[3pt]
XGBoost only & 0.233 & 0.829 & 0.618 & 0.713 & 0.487 & 0.759 \\[3pt]
Random Forest (default params) & 0.214 & 0.839 & 0.622 & 0.703 & 0.507 & 0.757 \\[3pt]
RF + LLM Forecast + recency (rounded) & 0.203 & 0.845 & \textbf{0.535} & \textbf{0.760} & \textbf{0.547} & 0.537 \\[3pt]
Random Forest only & 0.203 & 0.845 & 0.641 & 0.710 & 0.520 & 0.767 \\[3pt]
Ridge GLM + Random Forest (mean) & 0.198 & 0.848 & 0.644 & 0.697 & 0.523 & 0.756 \\[3pt]
XGBoost, no LLM features & 0.189 & 0.852 & 0.661 & 0.700 & 0.493 & 0.729 \\[3pt]
Random Forest, no LLM features & 0.186 & 0.854 & 0.658 & 0.710 & 0.517 & 0.749 \\[3pt]
Ridge GLM & 0.156 & 0.869 & 0.668 & 0.683 & 0.507 & 0.722 \\[3pt]
Mode of reporting-org score baseline & 0.102 & 0.897 & 0.579 & 0.700 & 0.530 & 0.426 \\[3pt]
Ridge Baseline (risks + org only) & 0.017 & 0.938 & 0.709 & 0.570 & 0.433 & 0.651 \\[3pt]
\bottomrule
\end{tabular}

\end{table}



\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.9\textwidth]{assets/final_plot_r2_26_val.png}
  \caption{The scatter of observed vs predicted points on the validation set for the LLM-corrected RF prediction. $R^2=\Rsqfrac$, Pairwise probabilty of \PairwiseModel\,\%.   }
  \label{fig:scatterllmcorrected}
\end{figure}

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.9\textwidth]{assets/shap_rf_ratings.png}
  \caption{A SHAP analysis of ratings on the validation set from the RF. Red indicates an increase in the value of the feature, while blue indicates a below-average value. Points to the right of zero shift ratings up, points to the left of zero shift ratings down. Overall, planned expenditure, duration, and the organization for rating were the most important features for the random forest model. }
  \label{fig:shapratings}
\end{figure}



% Mode of reporting-org score baseline & \textbf{0.102} & \textbf{0.897} & 0.579 & \textbf{0.700} & \textbf{0.530} & \textbf{0.426} \\
% Ridge Baseline (risks + org only) & 0.017 & 0.938 & 0.709 & 0.570 & \textbf{0.433} & 0.651 \\
% Random Forest only & 0.203 & 0.845 & 0.641 & 0.710 & 0.520 & 0.767 \\
% Random Forest, no LLM features & 0.186 & 0.854 & 0.658 & 0.710 & 0.517 & 0.749 \\
% XGBoost only & 0.233 & 0.829 & \textbf{0.618} & \textbf{0.713} & \textbf{0.487} & 0.759 \\
% XGBoost, no LLM features & 0.189 & 0.852 & 0.661 & 0.700 & 0.493 & 0.729 \\
% Ridge GLM & 0.156 & 0.869 & 0.668 & 0.683 & 0.507 & 0.722 \\
% Random Forest (default params) & 0.214 & 0.839 & 0.622 & 0.703 & 0.507 & 0.757 \\
% Ridge GLM + Random Forest (mean) & 0.198 & 0.848 & 0.644 & 0.697 & 0.523 & 0.756 \\
% RF + recency & 0.254 & 0.817 & 0.593 & 0.750 & 0.547 & 0.767 \\
% RF + LLM Forecast + recency & 0.258 & 0.815 & 0.590 & 0.760 & 0.547 & 0.767 \\
% RF + LLM Forecast + recency (rounded) & 0.203 & 0.845 & 0.535 & 0.760 & 0.547 & 0.537 \\
% \bottomrule




% \begin{tabular}{lrrrrr}
% \toprule
% Method & $R^2$ $\uparrow$ & RMSE $\downarrow$ & MAE $\downarrow$ & Side Acc. $\uparrow$ & Acc. $\uparrow$ \\
% \midrule
% Mode of reporting-org score baseline & 0.102 & 0.897 & \textbf{0.579} & 0.700 & 0.530 \\
% Random Forest only & 0.173 & 0.861 & 0.651 & 0.703 & 0.513 \\
% Random Forest, no LLM features & 0.143 & 0.876 & 0.679 & 0.677 & 0.490 \\
% Ridge GLM & 0.148 & 0.873 & 0.654 & 0.683 & 0.513 \\
% Random Forest (default params) & 0.157 & 0.869 & 0.653 & 0.693 & 0.497 \\
% Ridge GLM + Random Forest (mean) & 0.173 & 0.861 & 0.644 & 0.710 & \textbf{0.537} \\
% RF + recency correction & 0.183 & 0.855 & 0.629 & \textbf{0.733} & 0.523 \\
% RF + LLM Forecast + recency correction & \textbf{0.193} & \textbf{0.850} & 0.623 & 0.730 & 0.527 \\

% Method & $R^2$ $\uparrow$ & RMSE $\downarrow$ & Brier $\downarrow$ & Side Accuracy 
% $\uparrow$ \\
% \midrule
% Mode of reporting-org score baseline & 0.102 & 0.897 & 0.30 & 0.70 \\
% Random forest, no LLM features & 0.142 & 0.878 & 0.33 & 0.67 \\
% Ridge GLM & 0.148 & 0.873 & 0.30 & 0.70 \\
% Random Forest (default params) & 0.158 & 0.869 & 0.32 & 0.68 \\
% Ridge GLM + Random Forest (mean) & 0.170 & 0.862 & 0.30 & 0.70 \\
% Random Forest only & 0.171 & 0.862 & 0.30 & 0.70 \\
% RF + recency correction & 0.179 & 0.858 & 0.30 & 0.70 \\
% RF + LLM Forecast + recency correction & \textbf{0.190} & \textbf{0.853} & 0.30 & 0.70 \\
% \bottomrule
% \end{tabular}

% \end{table}

% \begin{table}[t]
% \centering
% \footnotesize
% \setlength{\tabcolsep}{4pt}
% \renewcommand{\arraystretch}{1.15}

% \begin{threeparttable}
% \caption{Random-forest drop-one feature importance, sorted by impact. Each row reports the decrease in validation $R^2$ when the feature is removed ($\Delta R^2$\tnote{*}), along with the share of training rows that were median-imputed for that feature (\%\ missing\tnote{**}) and the training-set mean/SD (in the model feature space). The largest impacts come from planned budget and planned duration; \texttt{rep\_org\_*} are one-hot reporting-organization indicators.}
% \label{tab:rf_dropone_feature_importance}

% \begin{tabularx}{\linewidth}{@{}l r r r r@{}}
% \toprule
% Feature & $\Delta R^2$\tnote{*} & \% missing\tnote{**} & Mean (train) & SD (train) \\
% \midrule
% planned\_expenditure                  & 0.0940 & 38.18 & 18.026 & 1.273 \\
% planned\_duration                     & 0.0840 & 0.15  & 8.806  & 3.371 \\
% finance\_is\_loan                     & 0.0320 & 4.26  & 0.635  & 0.482 \\
% rep\_org\_2                           & 0.0310 & 0.00  & 0.648  & 0.478 \\
% wgi\_regulatory\_quality\_est         & 0.0290 & 6.90  & -0.401 & 0.488 \\
% finance                              & 0.0290 & 2.50  & 81.634 & 9.268 \\
% targets                              & 0.0280 & 0.15  & 66.625 & 13.442 \\
% rep\_org\_0                           & 0.0270 & 0.00  & 0.051  & 0.221 \\
% umap3\_y                              & 0.0260 & 9.84  & -1.615 & 1.023 \\
% wgi\_government\_effectiveness\_est   & 0.0230 & 6.90  & -0.386 & 0.473 \\
% wgi\_rule\_of\_law\_est               & 0.0230 & 6.90  & -0.521 & 0.492 \\
% rep\_org\_1                           & 0.0230 & 0.00  & 0.059  & 0.235 \\
% gdp\_percap                           & 0.0230 & 7.34  & 8.265  & 0.996 \\
% umap3\_x                              & 0.0220 & 9.84  & 0.460  & 2.776 \\
% wgi\_control\_of\_corruption\_est     & 0.0220 & 6.90  & -0.550 & 0.467 \\
% wgi\_political\_stability\_est        & 0.0210 & 6.90  & -0.570 & 0.789 \\
% activity\_scope                       & 0.0200 & 0.00  & 3.185  & 1.397 \\
% risks                                & 0.0190 & 0.59  & 64.439 & 19.752 \\
% context                              & 0.0190 & 0.15  & 74.029 & 16.487 \\
% cpia\_score                           & 0.0170 & 48.75 & 3.499  & 0.382 \\
% complexity                           & 0.0160 & 0.15  & 61.912 & 17.406 \\
% umap3\_z                              & 0.0160 & 9.84  & 1.599  & 1.869 \\
% implementer\_performance              & 0.0150 & 0.29  & 82.044 & 11.749 \\
% integratedness                        & 0.0090 & 1.32  & 83.266 & 5.568 \\
% \bottomrule
% \end{tabularx}

% \begin{tablenotes}\footnotesize
% \item[*] $\Delta R^2$ is computed as $(R^2_{\text{full}} - R^2_{\text{dropped}})$ on the same validation split; larger values indicate greater marginal contribution under this ablation test.
% \item[**] Percent of training rows with missing values for the feature, filled via median imputation (as used in the model pipeline).
% \end{tablenotes}
% \end{threeparttable}
% \end{table}





% % Figure: predicted vs observed absolute error (scatter)
% \begin{figure}[!htbp]
%   \centering
%   \includegraphics[width=0.9\textwidth]{assets/splits_by_start_year.png}
%   \caption{The activities included for forecasting ratings, with the splits by count year. Incomplete activities, shown in red, were not used for prediction.}
%   \label{fig:splits}


% Figure: predicted vs observed absolute error (scatter)



\FloatBarrier
\subsection{Forecasting Cost-Effectiveness}
In general, cost-effectiveness forecasts were weaker than ratings. A similar work found an adjusted $R^2 >0.7$ for outcome ratings including in forecasting beneficiaries reached \citep{goldembergMindingGapAid2025}, but this was including actual rather than planned durations, actual rather than planned financial disbursements, and several features including breakdowns of per-sector funding for activities and manager performance ratings from AidData that I did not include in my dataset. My attempt to replicate their result revealed that they likely had training data leakage, but I have not had a response to my inquiries about whether the final code used to produce the tables in their paper was indeed the code containing the bug. Even if they correctly implemented their method, there are some reasons to think my outcomes would be harder to predict. Their paper did not predict on cost-effectiveness, making outcome prediction a much easier task when given overall program spending. Also, the outcomes with high detected correlation measured a lagged 5-year country-level indicator, which is less susceptible to reporting or extraction error, while my data were extracted directly from the outcomes using \emph{gemini-2.5-flash}.

Several factors contributed to the difficulty of forecasting specific outcomes from extracted IATI data:
\begin{itemize}
  \item Rarity of quantitative outcomes
  \item Unclear apportioning of funding towards each outcome, which is challenging to extract.
  \item Inconsistent measurement styles and definitions of terms like Benefit-Cost Ratio, which effects would be included in Economic Rate of Returns.
  \item Incorrect aggregation of multiple ratings within the documents. I find inaccurate aggregation was initially (artificially) increasing my prediction accuracy, as the errors were more predictable than the outcomes themselves.
\end{itemize}

\textbf{Outcome model training and evaluation}
For each outcome distribution in Table~\ref{tab:outcome_model_performance}, I trained a random-forest regression model containing similar features and hyperparameters to the model used for forecasting ratings, but with the rating target replaced by the relevant activity-level outcome. A one-hot encoded dummy variable for which outcomes being averaged was also included. The ratings were included as a feature to aid learning about activity success. Models were trained using activity IDs in the training split with non-missing outcomes and evaluated on the validation activities. The counts reported in Table~\ref{tab:outcome_model_performance} correspond to the number of activities available in the validation split for each outcome. Outcome distributions with fewer than 10 activities in either the training or validation split were excluded.

In addition, a single aggregate Z-score was calculated, which subtracts the mean value of each outcome (including ratings) and divides by the standard deviation in the training set. For each activity, the mean value of the z-scores was taken for all dependent variables, including the rating. Due to its high predictability, I theorize that the z-score is a stronger indicator of activity success than activity rating alone, due to the prevalence of ``gaming'' activity ratings \citep{goldembergMindingGapAid2025}.

I find outcome prediction to be highly challenging, and my model does not beat simple "predict the mean validation zagg" for outcome prediction (see Figure \ref{tab:outcome_model_performance}). However, it appears that the model is able to distinguish cost-effectiveness between pairs of progress, at a rate of 60\% accuracy, compared to random chance of 50\%. The top 3 features of the model are UMAP x axis (negative), the planned duration (positively correlated), and the expenditure.

\begin{table}
\caption{Predictive performance of cost-effectiveness outcome RF model on the validation set. R$^2$, Pairwise ordering probability, and Spearman correlation are shown with 95\% bootstrap confidence intervals. In general, only the aggregate had sufficient data to produce a meaningfully predictive model, although the model is only able to distinguish more cost-effective activities 60\% of the time. Outcomes are sorted by R$^2$ (descending). The Benefit-Cost Ratios had identical forecasts for all examples, hence a pairwise and Spearman could not be properly calculated. All other outcomes had fewer than 10 items in training or validation.}
\label{tab:outcome_model_performance}

\footnotesize
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.15}

\begin{tabularx}{\linewidth}{@{}>{\raggedright\arraybackslash}X r r r r >{\raggedleft\arraybackslash}p{2.5cm}@{}}

\toprule
Outcome & $R^2$ & Pairwise (\%) & Spearman & MAE & $N_{\mathrm{train}}/N_{\mathrm{val}}/N_{\mathrm{test}}$ \\
\midrule
All selected outcomes (z-aggregate activity mean) & 0.05 [-0.03, 0.13] & 60 [54, 67] & 0.31 [0.13, 0.46] & 0.56 & 682 / 299 / 299 \\
Water Connections  Connections (log10(connections)) & -0.01 [-0.39, 0.13] & 66 [54, 77] & 0.43 [0.07, 0.69] & 0.70 & 71 / 25 / 25 \\
Economic Rate Of Return  Percent (percent) & -0.01 [-0.19, 0.03] & 53 [45, 60] & 0.09 [-0.14, 0.28] & 15.39 & 293 / 103 / 103 \\
Co2 Emission Reductions  Tonnes Co2E (log10(tonnes co2e)) & -0.03 [-0.32, 0.00] & 41 [27, 58] & -0.28 [-0.62, 0.18] & 1.46 & 38 / 23 / 23 \\
Benefit Cost Ratios  Ratio (ratio) & -0.05 [-0.42, -0.00] & N/A & N/A & 0.33 & 44 / 18 / 18 \\
Financial Rate Of Return  Percent (percent) & -0.15 [-1.19, 0.03] & 51 [39, 64] & 0.01 [-0.32, 0.36] & 15.20 & 147 / 43 / 43 \\
Generation Capacity    (log10(Mw)) & -0.32 [-1.13, 0.02] & 62 [47, 75] & 0.39 [-0.06, 0.68] & 0.73 & 44 / 20 / 20 \\
\bottomrule
\end{tabularx}
\end{table}

In keeping with the results of \citep{goldembergMindingGapAid2025}, I do not find a significant correlation between activity ratings and z-scored outcomes. I found an overall pearson correlation of only 0.07 between cost-effectiveness and ratings over the entire dataset for zagg (N=566).

In conclusion, I find that outcomes cannot be reliably predicted using these methods, revealing the need for more work to extract sufficient data for reliable outcome prediction. Furthermore I find that obtaining  cost-effectiveness metrics is more challenging than obtaining ratings, in contrast to the claims from prior literature \citep{goldembergMindingGapAid2025}. 

% \begin{table}[t]
% \centering
% \caption{Predictive performance of random-forest outcome models on the validation set. For cost-effectiveness outcomes, the target is \emph{USD per unit outcome} (total disbursement divided by the activity-level outcome) and is modeled in log space; for the remaining outcomes (benefit--cost ratios, rates of return, and agricultural yield outcomes) the targets are modeled on their original (unlogged) scale. Reported errors (RMSE/MAE) are in the same scale as the modeled target; $N_{\mathrm{val}}$ is the number of activities in the validation split used for each outcome.}
% \label{tab:outcome_model_performance}
% \begin{tabular}{lllrccc}
% \toprule
% Outcome (readable) & Units & Target transform & $R^2$ & RMSE & MAE & $N_{\mathrm{val}}$ \\
% \midrule
% Beneficiaries (cost per beneficiary) & people & $\log(\mathrm{USD/person})$ & 0.057 & 0.962 & 0.670 & 102 \\
% CO$_2$/CO$_2$e emissions reductions (cost-effectiveness) & tonnes CO$_2$e & $\log(\mathrm{USD/tCO_2e})$ & -0.227 & 1.566 & 1.171 & 27 \\
% Generation capacity (cost-effectiveness) & MW & $\log(\mathrm{USD/MW})$ & -0.083 & 1.016 & 0.866 & 30 \\
% Water and sanitation connections (cost-effectiveness) & connections & $\log(\mathrm{USD/connection})$ & 0.099 & 0.688 & 0.480 & 29 \\
% Benefit--cost ratio & ratio & none & -0.044 & 0.385 & 0.270 & 18 \\
% Economic rate of return & percent & none & -0.225 & 24.189 & 17.549 & 128 \\
% Financial rate of return & percent & none & -0.752 & 19.973 & 16.068 & 49 \\
% Agricultural yield increase (percent) & percent & none & -0.319 & 37.686 & 29.880 & 10 \\
% \bottomrule
% \end{tabular}
% \end{table}
% preamble (once)


% % table
% \begin{table}
% \caption{Updated predictive performance of random-forest outcome models on the validation set. For cost-effectiveness outcomes, the target is USD per unit outcome and is modeled in log space; for the remaining outcomes the targets are modeled on their original scale. Outcomes are sorted by $R^2$ (descending).}
% \label{tab:outcome_model_performance}

% \footnotesize
% \setlength{\tabcolsep}{4pt}
% \renewcommand{\arraystretch}{1.15}

% \begin{tabularx}{\linewidth}{@{}>{\raggedright\arraybackslash}X >{\raggedright\arraybackslash}p{1.7cm} >{\raggedright\arraybackslash}p{3.0cm} r r r >{\raggedleft\arraybackslash}p{2.9cm}@{}}

% \toprule
% Outcome & Units & Target transform & $R^2$ & RMSE & MAE & $N_{\mathrm{val}}/N_{\mathrm{train}}/N_{\mathrm{heldout}}$ \\
% \midrule
% All selected outcomes (z-aggregate activity mean) & z & none (z-score) & 0.22 & 0.72 & 0.53 & 326 / 737 / 248 \\
% Rating & rating & none & 0.15 & 0.88 & 0.67 & 324 / 732 / 244 \\
% Water and sanitation connections (cost-effectiveness) & connections & $\log(\mathrm{USD/connection})$ & 0.11 & 0.69 & 0.49 & 30 / 84 / 10 \\
% Beneficiaries (cost per beneficiary) & people & $\log(\mathrm{USD/person})$ & 0.05 & 0.97 & 0.67 & 102 / 220 / 59 \\
% Economic rate of return & percent & none & 0.0084 & 22.0 & 15.0 & 128 / 379 / 65 \\
% Agricultural yield increase (percent) & percent & none & -0.0053 & 33.0 & 26.0 & 10 / 33 / 9 \\
% Benefit--cost ratio & ratio & none & -0.045 & 0.38 & 0.27 & 18 / 49 / 16 \\
% Generation capacity (cost-effectiveness) & MW & $\log(\mathrm{USD/MW})$ & -0.074 & 1.0 & 0.83 & 30 / 76 / 16 \\
% CO$_2$/CO$_2$e emissions reductions (cost-effectiveness) & tonnes CO$_2$e & $\log(\mathrm{USD/tCO_2e})$ & -0.14 & 1.5 & 1.2 & 27 / 47 / 13 \\
% Financial rate of return & percent & none & -0.28 & 17.0 & 14.0 & 49 / 170 / 21 \\
% \bottomrule
% \end{tabularx}
% \end{table}





% \begin{table}[t]
% \centering
% \small
% \setlength{\tabcolsep}{4pt}
% \caption{Predictive performance of random-forest outcome models on the validation set. For quantity outcomes with wide dynamic range (beneficiaries, CO$_2$/CO$_2$e reductions, generation capacity, trees planted, and water/sanitation connections), targets are modeled in log space; for benefit--cost ratios and rates of return, targets are modeled on their original (unlogged) scale. Reported errors (RMSE/MAE) are in the same scale as the modeled target; $N_{\mathrm{val}}$ is the number of activities in the validation split used for each outcome.}
% \label{tab:outcome_model_performance}

% \begin{tabularx}{\linewidth}{@{}>{\raggedright\arraybackslash}X l l r r r r@{}}
% \toprule
% Outcome (readable) & Units & Target transform & $R^2$ & RMSE & MAE & $N_{\mathrm{val}}$ \\
% \midrule
% Benefit--cost ratio & ratio & none & -0.044 & 0.385 & 0.270 & 18 \\
% Economic rate of return & percent & none & -0.225 & 24.189 & 17.549 & 128 \\
% Financial rate of return & percent & none & -0.752 & 19.973 & 16.068 & 49 \\
% \bottomrule
% \end{tabularx}
% \end{table}


I do find that failing to divide by the total disbursement as marked in iati increases predictability. This is in part because there is some noise inherent in the total expenditure, and also because when not dividing by expenditure, the model learns to predict linearly higher outcomes correlating with the expenditure.  When dividing by each activity's disbursement for those that are marked as dollar-per-unit in Table \ref{tab:outcome_model_performance}, and looking at all outcomes except ratings, the correlation on z-scores drops to 0.05, (95\% CI: -0.03, 0.13) and pairwise probability of 60\% (95\% CI: 54\%, 67\%) ($n_val=299$), compared to an $R^2$ of 0.10 (95\% CI -0.0982, 0.2625) and a pairwise probability of 68\% (95\% CI: 0.62, 0.74) ($n_val$=84) when forecasting the z-score for CO$_2$ emission reductions, generation capacity, and water connections. While pairwise ordering ability for outcomes is statistically significant, the overall ability is weak and appears to be largely a function of expenditure and sector clusters.


% % Figure: predicted vs observed absolute error (scatter)
% \begin{figure}[!htbp]
%   \centering
%   \includegraphics[width=0.9\textwidth]{assets/zagg prediction_scatter.png}
%   \caption{Validation: predicted versus observed absolute error. Each point corresponds to the mean of the random forest z-score prediction for ratings and all other outcomes listed in table \ref{tab:outcome_model_performance}, for activities in the validation set where at least one rating or outcome were predicted. The dashed line indicates perfect calibration ($y=x$).}
%   \label{fig:val-abs-err-scatter}
% \end{figure}


% % Figure: predicted vs observed absolute error (scatter)
% \begin{figure}[!htbp]
%   \centering
%   \includegraphics[width=0.9\textwidth]{assets/rating_outcome_corr_all.png}
%   \caption{All activity's z-scored ratings plotted vs z-scored outcomes. There is a very low positive correlation (Pearson correlation of 0.044). The outliers near "8" averages a 433 percent Economic rate of return and a benefit-cost ratio of 1.6 for the "Jakarta Urgent Flood Mitigation Project", an unusually economically effective project, that should regardless not be removed simply for being an outlier. The project's efficiency was substantial thanks to significant loan savings. However, due to ``the lack of clarity in the reported data, the limited application of FMIS findings, and the unfulfilled activities added at restructuring'', the Overall Outcome rating for this outlier project to ``Moderately Satisfactory''.  See \url{https://datastore.iatistandard.org/activity/44000-P111034}.}
% \end{figure}
% \label{fig:val-abs-err-scatter}


Overall, little can be concluded from individual outcome correlations. For more detailed work, expert coding is likely required for robust extraction of outcomes, and funding breakdowns for projects should be used to more accurately evaluate cost-effectiveness, rather than the course assumption that all funding for a project goes to all outcomes. New water or sanitation piping connections is a more clearly comparable outcome, although there may be systematic differences in the costs of sanitation connections and water supply that are not disambiguated by the model.

Despite these limitations, it appears that even a coarse coding of directly comparable activity outcomes is likely to provide a more robust ordering of overall activity cost-effectiveness than evaluator ratings alone, although outcome preditions are not statistically significantly better than forecasting the mean value for the outcome in a given sector in directly forecasting a single activity's cost-effectiveness.

\FloatBarrier


\FloatBarrier
\section{Conclusion}
I conclude by summarizing my findings regard each research question posed in the introduction.

\textbf{How do LLM forecasting methods compare to statistical models in forecasting international aid overall success ratings and quantitative outcomes?}
LLM forecasts themselves consistently underperform statistical models in this domain, while having the disadvantage of being costlier and more difficult to iterate with. However, they provide meaningfully accurate free-form forecasts when combined with statistical models.

\textbf{Do forecasting methods using state-of-the-art natural language processing methods meaningfully improve on simpler baseline forecasting heuristics?}

Across the board, LLM forecasting methods I investigate do not perform well at forecasting activity ratings when compared to statistical models.  

\textbf{Do forecasting methods using state-of-the-art natural language processing methods meaningfully improve on simpler baseline forecasting heuristics?}

The results in this work support the evidence that several methods can improve on baseline heuristics. More sophisticated models such as random forests beat out simpler ridge regression models on every metric. Baseline metrics, such as a linear regression using the organization ID and risks, and a "predict the median rating for this organization" were above random chance, but fell short of more sophisticated methods.

\textbf{How do differing methods of combining LLM and statistical forecasting compare in this domain?}
Embeddings of language models inserted into statistical models significantly improve forecasting ability. Furthermore, using embeddings as a means of selecting nearest neighbors and as a component of RAG retrieval gathered information that reliably increased the similarity grades between LLM forecasts and the eventual outcome over all tested LLM forecast configurations. Embeddings were also effective as a method for clustering activity disbursements.

Language models themselves were helpful in extracting grades for various aspects of the forecast, which as a group improved the statistical model forecast. While direct averaging did not demonstrate improved performance, I found that training a simple model to use the residual between the LLM and statistical model modestly improved forecasting skill across most metrics. There is some evidence that training a secondary model to incorporate the final LLM prediction with a random forest prediction can also improve the overall forecast skill.

\textbf{What methods improve the accuracy of free-form (qualitative) forecasts in this domain?}
It appears that the only reliable way of improving ratings over a range of different configurations was 1. to switch the model from the somewhat smaller \emph{deepseek-v3.2} or \emph{gemini-2.5-flash} models to the more expensive but more generally capable \emph{gemini-3-pro} model,  2. to incorporate additional retrieval of information via RAG and inserting outcomes from similar past activities using the KNN technique and 3. to directly prompt the model to come to the same conclusion as the random forest model.

Additional prompts to elicit reasons the outcome may go well or badly (stages 1 and 2 in my methods) sometimes deteriorated scores in certain model configurations, although the best performing models used these stages in its context window when forecasting. Similarly, fine-tuning sometimes helped, and sometimes deteriorated scores.

\textbf{What aspects of the activity available in my dataset at the beginning of the activity lead to higher or lower ratings?}
As others in the literature have reported,  increased duration and planned expenditures tend to correlate with higher activity ratings \citep{vivaltHowMuchCan2020} \citep{ashtonPuzzleMissingPieces2023} \citep{eilersVolumeRiskComplexitya}. However, there is moderate evidence from the cost-effectiveness random forest model that cost-effectiveness of quantitative outcomes does not clearly correlate with increased spending. Rating organizations tend to differ systematically, and by-sector differences in ratings appear to be more significant in general than regional differences. However, this was not the core focus of this work and needs further research.

\textbf{How does forecasting quantitative cost-effectiveness activity outcomes compare to forecasting ratings?}
The current dataset appears insufficient to forecast activity outcomes, both in numbers of quantitative outcomes and noisiness inherent in their extraction. There is some evidence in the validation set that an aggregate measure of cost-effectiveness can be used to rank promising activities, with a 60\% chance of a correct ordering compared to 50\% which would be arrived at by random chance.

\clearpage
\section*{Works Cited}
\printbibliography

% \addcontentsline{toc}{section}{Works Cited}
% Add your references or switch to biblatex later.

\clearpage
\section*{ErklÃ¤rung zur akademischen IntegritÃ¤t / Declaration of Academic Integrity}
\addcontentsline{toc}{section}{Declaration of Academic Integrity}
Hiermit versichere ich an Eides statt, dass ich die vorliegende Arbeit selbststÃ¤ndig und nur mit den angegebenen Quellen und Hilfsmitteln (z. B. Nachschlagewerke oder Internet) angefertigt habe. Alle Stellen der Arbeit, die ich aus diesen Quellen und Hilfsmitteln dem Wortlaut oder dem Sinne nach entnommen habe, sind kenntlich gemacht und im Literaturverzeichnis aufgefÃ¼hrt. Weiterhin versichere ich, dass weder ich noch andere diese Arbeit weder in der vorliegenden noch in einer mehr oder weniger abgewandelten Form als Leistungsnachweise in einer anderen Veranstaltung bereits verwendet haben oder noch verwenden werden. Die Arbeit wurde noch nicht verÃ¶ffentlicht oder einer anderen PrÃ¼fungsbehÃ¶rde vorgelegt. / \emph{I hereby certify under penalty of law that I have prepared this thesis independently and only using the cited sources and resources (e.g., reference works or the internet). All passages of the thesis that I have taken from these sources and resources, either verbatim or in spirit, are cited and listed in the bibliography. Furthermore, I certify that neither I nor anyone else has used or will use this thesis, either in its present form or in a more or less modified form, as evidence in another course. This thesis has not yet been published or submitted to another examining authority.} \\
\\
Potsdam, \DateDDMonthYYYY

\end{document} 