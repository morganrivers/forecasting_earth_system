\documentclass[12pt,a4paper]{article}


% ---------- Basic packages ----------
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{tcolorbox}
\usepackage{setspace}
\usepackage{url}
\usepackage{microtype}
\usepackage{tabularx}
\usepackage{booktabs}
% \usepackage{breakable}
\usepackage{subcaption}
\usepackage{caption}
\usepackage{adjustbox}
\usepackage{array}
\usepackage{amsmath}
\usepackage{makecell}
\usepackage{placeins}
\usepackage{xstring}
\usepackage{etoolbox}

\usepackage{enumitem}



\usepackage[hidelinks]{hyperref}
% \usepackage{hyperref}
\usepackage{pdfcomment}
\usepackage{cleveref}
\usepackage{threeparttable}
%better bibtex
\usepackage[backend=biber,style=authoryear,sorting=nyt,natbib=true]{biblatex}
\addbibresource{references.bib}


% Drop "note" (and similar) from every bibliography entry
\AtEveryBibitem{%
  \clearfield{note}%        kill the note field
  \clearfield{addendum}%    (often used like a note)
  \clearfield{annotation}%  (some exporters use this)
}


\newbibmacro*{citewithtitle:tooltip}{%
  \printnames{labelname}%
  \addcomma\space
  \printfield{year}%
  \addspace\textbar\addspace
  \printfield{title}%
}

% (Author, Year | Title) with link + tooltip
\DeclareCiteCommand{\citewithtitle}
  {\usebibmacro{prenote}}
  {\printtext[bibhyperref]{%
     \pdftooltip{%
       \mkbibparens{%
         \printnames{labelname},\space
         \printfield{year}\addspace\textbar\addspace
         \printfield[citetitle]{title}%
       }%
     }{%
       \usebibmacro{citewithtitle:tooltip}%
     }%
  }}
  {\multicitedelim}
  {\usebibmacro{postnote}}
% Make natbib-like commands available:
\let\citep\parencite
\let\citet\textcite

\makeatletter
\edef\dictfile{macros/keyword.tex}
%\typeout{Using dictionary: \dictfile}%
\input{\dictfile}
\makeatother

% --- Load the dictionary with a safe fallback ---
\makeatletter
\edef\dictfile{macros/dictionary-\DICT.tex}
%\typeout{Using dictionary: \dictfile}%
\input{\dictfile}
\makeatother

% --- Simple conditional helpers based on \DICT ---
\newcommand{\ifdev}[1]{%
  \IfStrEq{\DICT}{development}{#1}{}%
}

\newcommand{\ifsus}[1]{%
  \IfStrEq{\DICT}{sustainability}{#1}{}%
}

\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{4}

% \newcommand{\subsubsubsection}[1]{\paragraph{#1}\mbox{}\\}

% centered, stretchable column
\newcolumntype{Y}{>{\centering\arraybackslash}X}
\newcommand{\logoheight}{1.1cm} % tweak if needed


\setstretch{1.2}
\setlength{\parindent}{0pt}
\setlength{\parskip}{0.6em}

% For even spacing of logos
\newcolumntype{C}[1]{>{\centering\arraybackslash}m{#1}}

% Make article say "Table of Contents"
\renewcommand{\contentsname}{Table of Contents}

% -----------------------------------
% Editable metadata
% -----------------------------------
% \newcommand{\ThesisTitle}{\TITLE}
\newcommand{\ThesisTitle}{Forecasting the Success of Environmental and Sustainability Activities in International Development Using Language Models}
% \newcommand{\ThesisSubtitle}{SUBTITLE}
\newcommand{\ThesisCity}{[City]}
\newcommand{\ThesisDate}{[Date]}

\newcommand{\AuthorName}{Morgan Rivers}
\newcommand{\AuthorAddressTwo}{Berlin, Germany 10559}
\newcommand{\AuthorEmail}{rivers@uni-potsdam.de}
\newcommand{\MatricNo}{829112}

\newcommand{\FirstReviewer}{Prof. Christian Kuhlicke}
\newcommand{\SecondReviewer}{Dr. Ivan Kuznetzov}
\newcommand{\MonthName}{%
  \ifcase\month
  \or January\or February\or March\or April\or May\or June%
  \or July\or August\or September\or October\or November\or December%
  \fi
}
\newcommand{\DateDDMonthYYYY}{\number\day~\MonthName~\number\year}

\begin{document}
\hypersetup{pageanchor=false}

\pagenumbering{gobble}
\thispagestyle{empty}

% ---------- Title page ----------
\begin{center}

    % Desired order: RIFS, UFZ, AWI, PIK, University crest
    \noindent
    \begin{tabularx}{\textwidth}{@{}YYYYY@{}}
    % \adjustbox{max height=\logoheight, max width=\linewidth, keepaspectratio}{\includegraphics{assets/rif.jpg}} &
    \adjustbox{max height=\logoheight, max width=\linewidth, keepaspectratio}{\includegraphics{assets/helmholtz.png
      }} &
    \adjustbox{max height=\logoheight, max width=\linewidth, keepaspectratio}{\includegraphics{assets/awi.jpg}} &
    % \adjustbox{max height=\logoheight, max width=\linewidth, keepaspectratio}{\includegraphics{assets/pik.png}} &
    \adjustbox{max height=2\logoheight, max width=2\linewidth, keepaspectratio}{\includegraphics{assets/potsdam.png}}\\
    \end{tabularx}


    \vspace{1.8cm}

    
    \begin{center}
        \begin{flushleft}
        University of Potsdam\\
        Faculty of Science\\
        Institute of Environmental Science and Geography\\
        Institute of Physics and Astronomy\\
        \textbf{Climate, Earth, Water, \& Sustainability\\[4em]}
        \end{flushleft}
    
        {\large \textbf{Master Thesis}}\\
        for the award of the academic degree\\
        \textbf{Master of Science (M.Sc.)}\\
        at the University of Potsdam\\[3em]
    
        \textbf{\Large \ThesisTitle}\\ [3em] %if no subtitle
        %\textbf{\large SUBTITLE}\\[6em]
    
        Potsdam, \DateDDMonthYYYY \\[7em]
    
        \begin{flushleft}
            \textbf{Submitted by:}\\[-0.2em]
            \AuthorName\\
            % \AuthorAddressOne\\
            % \AuthorAddressTwo\\
            \AuthorEmail\\
            Matriculation No.: \MatricNo

            % \vspace{1.0cm}
            First reviewer: \FirstReviewer\\
            Second reviewer: \SecondReviewer
        \end{flushleft}

    \end{center}
    \clearpage

\end{center}
\clearpage
% ... title/roman pages ...
\hypersetup{pageanchor=true}
% ---------- Main matter ----------
\section*{Abstract}
\subsection*{Abstract in English}

International aid and cooperation creates a profound difference in the rate of development in growing economies, improves the lives of the world's poorest, and often safeguards the environment and materially promotes sustainability. However, international aid has non-significant rates of failure in achieving its objectives. There have been few attempts in the literature to create models to predict the success of aid activities, and none focused on environmental outcomes. This thesis produces a forecasting system for the overall success of international aid activities at time of evaluation from the International Aid and Transparency Initiative (IATI) database, combining classical statistical methods with modern language model techniques. I find that the information available at the start of the activity all contribute to prediction accuracy, including quantifiable information used in previous studies, averaging the success rates of semantically similar activities, and using the reasoning abilities and information gathering ability of large language models (LLMs) to improve forecasts. Testing against the validation set, 300 later-starting activities in a dataset of 1,300 environmental and sustainability improving activities for 4 reporting organizations, the full forecasting system improves the prediction from 70.0\% to 73.0\% accuracy compared to a ``pick the most common rating'' baseline. For overall success ratings on a scale from approximately 1 to 6, the system can explain 19.4\% of the variance in ratings compared to 10.2\% for the baseline. I also produce an aggregate success metric averaging the z-score for ratings and commonly quantified activity outcomes relating to activity cost-effectiveness, and find the same model explains 22\% of the variance in this metric. I also release a freely available dataset of LLM-generated activity grades, summaries, success ratings, and various other quantitative activity outcomes and extracted information for 1,800 IATI activities. This work lays the foundation to improve decision making for a wide range of initiatives and policies in developing countries and also in other data-rich institutional contexts.
% International aid and cooperation creates a profound difference in the rate of development in growing economies, improves the lives of the world's poorest, and often safeguards the environment and materially promotes sustainability. However, international aid has non-significant rates of failure in achieving its objectives. There have been few attempts in the literature to create models to predict the success of aid activities, and none focused on environmental outcomes. This thesis produces a forecasting system for the overall success of international aid activities at time of evaluation from the International Aid and Transparency Initiative (IATI) database, combining classical statistical methods with modern language model techniques. I find that the information available at the start of the activity all contribute to prediction accuracy, including quantifiable information used in previous studies, averaging the success rates of semantically similar activities, and using the reasoning abilities and information gathering ability of large language models (LLMs) to improve forecasts. Testing against the held-out, latest-starting 400 activities in a dataset of 1,300 environmental and sustainability improving activities for 4 reporting organizations, the full forecasting system improves success forecasting at the beginning of the activity by 5.9\% (95\% CI: XX\% to XX\%), improving the prediction from 70.0\% to 73.0\% accuracy compared to a ``pick the most common rating'' baseline. For overall success ratings on a scale from approximately 1 to 6, the system can explain 17.4\% (95\% CI: XX\% to XX\%) of the variation compared to 6.8\% for the baseline. I also release a freely available dataset of LLM-generated activity grades, summaries, success ratings, and various other quantitative activity outcomes and extracted information for 1,800 IATI activities. This work lays the foundation to improve decision making for a wide range of initiatives and policies in developing countries and also in other data-rich institutional contexts.

\subsection*{Abstract auf Deutsch}
Will do, once abstract is finalized %TODO
\clearpage

% ---------- Contents & front matter ----------
\pagenumbering{roman}
\setcounter{page}{1}
\tableofcontents
\clearpage

\pagenumbering{arabic}
\setcounter{page}{1}






















\section{Background: LLMs and the Science of Forecasting}
\subsection{Introduction}

\textbf{Background} 
The Earth system sciences concern the complex interaction between biological, chemical, physical, and anthropogenic processes. A broad goal of the Earth system sciences is to model and accurately predict the outcomes of interventions with regard to the environment and its impact on humans. Much of the progress in Earth system science has been on linking these complex phenomena into large models, such as integrated assessment models (IAMs), computable general equilibrium models (CGEs), or agent-based models (ABMs). While many attempts have been made to model specific subsystems within the Earth system, such as the carbon cycle, environmental and economic linkages, or understanding human impacts in the climate-water-food nexus, there have been few attempts to create a comprehensive model which can predict quantitative or qualitative outcomes of a wide range of cross-domain interventions in the Earth system which could be described in natural language.

In particular, the Earth system is a ``complex system'' - characterized by difficult-to-predict, emergent phenomena, and both positive and negative feedback loops. Thus far, models in the Earth system sciences have largely relied on mechanistic, theoretically-based models of the underlying complex systems they analyzed. However, this is not the only way to predict outcomes - Machine Learning (ML) outcomes, while lacking the rigorous mechanistic underlying processes characterizing integrated assessment models (IAMs), CGEs, and ABMS, have recently been shown to perform better than the best prior computational approaches in several complex-system domains such as language modelling \citep{brownLanguageModelsAre2020}, protein folding \citep{jumperHighlyAccurateProtein2021}, biodiversity protection \citep{silvestroImprovingBiodiversityProtection2022}, and weather forecasting \citep{lamLearningSkillfulMediumrange2023}.

In the specific context of developmental cooperation, the system of interactions between people, their wellbeing, medical, educational, and career outcomes, the economy, the government, and the natural environment surrounding development cooperation interventions also displays difficult to understand emergent phenomena such as regime changes, disease spread, and economic collapse.

Together, these characteristics allow us to characterize the system being improved by development cooperation interventions affecting the environment as a ``complex system'', where, by definition, decision making about outcomes is challenging.


The collective failure of the scientific community to model complex outcomes in the Earth system has severe implications. For example, work from \citep{stechemesserClimatePoliciesThat2024} has demonstrated that out of 1500 policies between 1998 and 2022, only 68 had statistically significant causal effect to reduce country emissions with a 99\% or higher confidence. Furthermore, they find that more than four times the effort witnessed so far in emissions reductions from implementing more successful policies in line with past reductions would have to be exerted to close the emissions gap to remain below 2 degrees C in global temperature rise. Broadly, their findings support the claim that even when climate policy is implemented, it is largely ineffective, and in the future it will need to be much more effective to avoid dangerous levels of CO$_2$ concentrations. In terms of biodiversity,  achieving sustainability cannot be met by current trajectories, and goals for 2030 and beyond may only be achieved through transformative changes across economic, social, political and technological factors \citep{watsonGlobalAssessmentReport2019}. As of 2022 pollution remains responsible for approximately 9 million deaths per year, corresponding to one in six deaths worldwide \citep{fullerPollutionHealthProgress2022}.

While much scientific effort has been expended on understanding underlying systems, much less effort has been directly focused on predicting which specific interventions would realistically improve outcomes for activities in the Earth system sciences. 
%Meanwhile, examples exist in the literature where regulation can greatly reduce or even eliminate environmental problems - the Montreal protocol has met with great success in closing the hole in the ozone layer [CITE IF KEEP THIS SENTENCE!]. 

Despite many examples of other computer models which have some success (see section XXYY), in many relevant sub-domains, such as climate policy, ex ante analysis of mitigation action and of mitigation plans is limited \citep{intergovernmentalpanelonclimatechangeipccMitigationDevelopmentPathways2023}. Given the overwhelming complexity of the Earth system, and the corresponding failures to properly model many of the system components in the Earth system and especially how they interact with human interventions, complementing mechanistic understanding and prediction with ML approaches is urgently needed.

This thesis was written in conjunction with the German Federal Ministry for Economic Cooperation and Development (BMZ) in order to improve their environment-related international aid decision making. In the context of official development aid (ODA), German development finance commitments on behalf of the German Federal Government were the second largest ODA source in 2023 at approximately 40 billion USD  \citep{NetODAOECD}. This was the case before recent major reductions in the US ODA in 2025, which indicate that Germany may soon be the largest source. However, despite significant care and effort put forth in documenting development cooperation outcomes at the the BMZ, ex-post evaluations are rarely read at the BMZ or the affiliated KfW Development Bank, even though around 19\% of evaluated projects are unsuccessful \citep{sustainabilityidosLearningKfWsExpost}. Given the large volume of directed aid and the likely gaps in knowledge due to low utilization of ex-post evaluations, an opportunity arises to close these gaps using recent advances in ML, especially large language models (LLMs), which can quickly search and synthesize findings over a much larger quantity of information than aid funding decision makers (from here on we will refer to them as ``evaluators''). At BMZ, these are the BMZ officers. 

\textbf{Proposal}
We set out to predict near-term, future states in a wide array of different contexts. One method applicable to context-rich domains is ``judgemental forecasting'', which allows expert forecasters to use tools including Fermi estimates, intuition, and information gathering to make a calibrated prediction on the likelihood of a given outcome \citep{halawiApproachingHumanLevelForecasting2024}. This can be contrasted with ``statistical forecasting'' which typically uses time-series prediction methods or purely quantitative approaches.

This thesis proposes the use of Large Language Models (LLMs) to implement judgemental forecasting to predict how effective interventions will be in the context of developmental interventions affecting the environment. By splitting records of the effectiveness of thousands of interventions from various international aid organizations into an intervention and an outcome, I use language models to mimic the reasoning and data gathering skills of trained forecasters, in an attempt to replicate the success at using judgemental forecasting from language models in geopolitical forecasting to the adjacent domain of forecasting development cooperation outcomes affecting the environment. Ultimately, the goal is to learn whether it is possible to complement a scientifically founded prediction for the effectiveness of a given intervention with a system with LLMs that are specifically trained for the task at their core. Given the difficulty of field testing ideas, policymakers and funding agencies often rely on expert forecasts on how an intervention will meet its intended goals to select which interventions will be implemented. Replacing or augmenting that advisory role could greatly improve decision making in this context \citep{hewittPredictingResultsSocial}.

This method differs in two key ways from prior literature. The first difference is that while many works in the literature have attempted to assess correlations between quantitative features and aid evaluation ratings, they have not focused on what knowledge would be available near the start of the activity, and do not assess out-of-time generalization of these correlations. In this work, I assess out-of-time generalization of feature importance. This is critical, because in order to improve aid decision making, one must assess the ability of models to forecast the outcomes of future interventions. The second difference is that in addition to standard statistical models, I implement judgemental AI forecasting, as a supplement and even a competitor to standard statistical models. 


I will briefly review current progress in event outcome prediction in developmental aid and cooperation interventions affecting the environment, and then discuss progress with LLMs in adjacent domains. %To my knowledge, there has been no attempt at predicting real-world outcomes of interventions in developmental aid and cooperation interventions affecting the environment\ while also rigorously quantifying the skill of such a system. # no longer true!

In the process of training the LLM, it was necessary to collect and label a large volume of interventions and associated outcomes along a wide range of metrics in developmental aid and cooperation interventions affecting the environment. Accordingly, in tandem with the open source LLM forecasting system, I also release a large structured database of interventions and associated outcomes in developmental aid and cooperation interventions affecting the environment. The database contains intervention descriptions, quantitative and qualitative outcomes identified with each intervention, and further statistical information about intervention categories and other statistical trends described in Section \ref{sub:database_of_evaluations}.

Within the domain of LLM use, there has been some progress. A recent tool called ``climsight'' summarizes and aggregates information about climate adaptation and mitigation \citep{koldunovLocalClimateServices2024}, but stops short of making predictions towards adaptation. Machine learning and LLMs have been used to collect over 80,000 articles about climate adaptation and provide analysis about which areas of implementation are lacking and point out gaps in attention towards promising categories of policies.

Limited work has also been done using LLMs such as ChatGPT-4 (GPT-4) to serve as data sources for policy deliberation and multi‑criteria assessment of climate and sustainability interventions, finding GPT-4 is in rough agreement with the policy rankings of human experts for the expected outcomes \citep{binaLargeLanguageModels2025}. However, very little is done to improve on GPT-4's abilities, the assessment was made on only a few dozen generic policy examples, and no attempt was made to compare outcomes between these policies and real-world outcomes. Despite these limitations, the findings are promising. For multiple criteria decision making (MCDM), GPT-4 provided a useful collaborative starting point, eased the process of considering multiple criteria effectively, and aided policy deliberation on climate change and sustainability.

One attempt which focused on specific outcomes of activities found their model using ``embeddings'' of LLMs could explain 70\% of the variance of these outcomes, and assessed the performance of nonlinear models including the random forest model used in this work. However, they include features that could not be known at the beginning of the activity (e.g. actual duration), and do not assess out-of-time generalization, instead splitting randomly within the dataset, nor do they explicitly assess prediction performance for ratings% evidence: https://github.com/luke-grassroot/aid-outcomes-ml/blob/main/notebooks/contextual_correlates_N.ipynb, : X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

\subsection{LLMs: The Transformer Architecture}
LLMs used in this work all use variants of the same fundamental architecture: the ``transformer'' \citep{vaswaniAttentionAllYou2017}. Transformers are ML models which are trained on vast quantities of textual data. During training, transformers convert input documents and text-based sources into "tokens" which are typically parts of words or smaller chunks of pdf or image-based data which commonly appear during training. In this work we use a simpler "decoder-only" variant of the transformer, as is commonly used for reggressive token prediction in chatbot applications, like Chat-GPT.

After input documents and textual sources are converted to tokens, transformers use a learned linear transformation called "embedding" matrix to convert the token-space into a lower-dimensional semantic space, typically with a few hundred dimensions. Each input token to the transformer is converted into a semantic vector. These vectors are important because they encode similar input tokens into nearby locations in the high-dimensional semantic space. At this stage, the transformer runs each token through a series of layers which in parallel convert all of the input tokens to the next predicted output token. Typically a transformer contains dozens or hundreds of such layers. These layers are composed of both Multi-Layer Perceptrons (MLP) which are simply feed-forward neural networks commonly used in many other ML architectures, and ``attention heads'', which are unique to the transformer architecture. A compressed representation is compressed into the ``residual stream'' after each layer, and the process is repeated until the reverse of the embedding matrix is applied to the final residual stream back into the token space, allowing the transformer to finally predict the next token.

The goal of the transformer is always to predict the next token. Accordingly, while MLP layers are typically able to store information for the memorization and fact-based learning in transformer training, ``attention heads'' have the ability to learn to locate locations in the past tokens where particularly relevant sections for predicting the next token would be. By copying in the relevant information into the residual stream, transformers are able to access important information even thousands of tokens in the past, a capability which is challenging to replicate in other architectures (such as Long-Short Term Memory (LSTM) architectures).

Transformers are the most appropriate choice as a model due to their remarkable ability to apply reasoning and generalization past their training data, and their ability to utilize both quantitative and semantic information for accurate next-token prediction. 

Finally, it is possible to fine-tune transformers - iterate update the weights within the embedding, MLP, and attention head components to reduce the loss on the fine-tuning training data. Fine-tuning can be viewed simplistically as the final stage of training where models are reconfigured and optimized towards skill at a narrow task, such as forecasting outcomes of development cooperation interventions relevant to ESS.


% Figure: workflow 
\begin{figure}[h]
  \centering
  \includegraphics[width=1\textwidth]{assets/simpletransformer.png}
  \caption{A simplified decoder-only transformer architecture.}
  \label{fig:workflow}
\end{figure}

% \subsection{Prediction Markets and Superforecasting}
% In recent years, significant progress has been made on accurate near-term forecasting outside of specific domains. The most promising approaches appear to be a mix of prediction markets, and specialized, trained experts known as ``superforecasters'' \citep{tetlockSuperforecastingArtScience2015}. Prediction markets have gained recent prominence in the domain of geopolitical forecasting, with significant volumes of transactions on predicting future geopolitical outcomes with a broad purview, including election results, the outcomes of treaties, or whether a regime will topple. Prediction accuracy is typically above-chance hundreds of days before resolution and steadily improves as deadlines approach. Predictions are typically above-chance within approximately one year time horizon, with the accuracy notably improving as the event reaches question resolution: one study finds a Brier score of approximately 0.2-0.3 for geopolitical and economic questions within about 3 months before resolution using a large constructed prediction market, dropping close to a Brier score of about 0.75 within a day or two of the question resolution \citep{tetlockSuperforecastingArtScience2015}.  In a broad range of complex, human-involved outcomes, prediction markets are superior to expert analysis. In the words of the economist Robin Hanson, ``racetrack market odds improve on the prediction of racetrack  experts; orange juice commodity futures improve on government weather forecasts;  stocks fingered the guilty firm in the Challenger crash long before the official NASA  panel; Oscar markets beat columnist forecasts; gas demand markets beat gas  demand experts; betting markets beat Hewlett Packard official printer sale  forecasts; and betting markets beat Eli Lily official drug trial forecasts.''  \citep{hansonShallWeVote2013}. 

% However, prediction markets have demonstrated that a smaller subset of forecasters in the market, known as ``superforecasters'', are statistically much better forecasters than the prediction market, and ensembling these forecasters and letting them exchange information among themselves leads to higher accuracy predictions than prediction markets alone \citep{mellersIdentifyingCultivatingSuperforecasters2015}. One source shows superforecasters saw the correct outcome with a 60\% probability approximately 300 days out (significantly earlier than prediction markets), and 75\% probability 250 days ahead of the outcome \citep{tetlockSuperforecastingArtScience2015}.

% Due to the relatively high expense and human effort required to organize superforecasting tournaments (the gold standard for event prediction), they have been largely focused on specific geopolitical and economic questions, some of which may fall under the domain of intervention impact in the Earth system sciences, although most point to broad trends where information may be gathered from the news and informal internet searches, and deep expertise in any single domain would not be required for an accurate forecast. In fact, in terms of ``calibration'', superforecasters usually beat domain experts in their own fields by maintaining a broad sense of good judgement and cultivating a trained skill at accurately estimating prediction probabilities, rather than overly relying on a single strategy (such as econometric analysis, or specific statistical methods) \citep{tetlockSuperforecastingArtScience2015}.

% \subsection{LLM Forecasting of Outcomes for Development Cooperation}
% \subsubsection{Ex-post Evaluations and Other Data Sources}
% Before turning to the possibility of LLM forecasting, we first consider what data we could use as context for their forecasts, and how useful it is for human evaluators. evaluators already use evaluations and other sources to inform their work at the BMZ. While evaluations are rigorous, surveys of evaluators find that evaluations come too late after the ends of projects, often several years after, to be strongly relevant to current projects and that ex-post evaluations are of limited relevance to evaluators, either individually or in aggregate form. \citep{sustainabilityidosLearningKfWsExpost}. This suggests both that the specific relevance of evaluations is unlikely to produce high accuracy results alone, and that information gathered in the few years immediately prior to the project is of particular importance to predicting project outcomes.

% Thus far, several repositories of high-quality intervention evaluations have been published. These databases document thousands of examples of ex-post predictions which allow us to infer how future development cooperation outcomes will proceed. In particular, the Interactive Database for Evaluation and Learning (IDEaL) contains over 1,200 rigorous studies with 6 separate evaluation criteria grading how well the intervention proceeded \citep{kfwdevelopmentbankIDEaL}. Collectively, over 10,000 intervention examples over the past few decades have been collected, although the quality and format of these additional interventions is relatively unknown \citep{DatenlaborbmzAwesomedevelopmentcooperationdata2025}. 

% Social Science Prediction Platform (SSPP) has collected thousands of examples of studies where social science results have been collected and compared to obtain informative priors in Bayesian analysis \citep{HowCanYou}. SSPP provides hundreds of evaluations, including a leaderboard and mean absolute error of the top 10 performing forecasters with at least 10 forecasts each. As the results become open source in the coming years, this can be a valuable resource for development impact forecasting.

% Finally, the largest extant database of recorded interventions available is that of the International Aid and Transparency Initiative (IATI) \citep{IATIDashboardIATI}. The full database contains over 800,000 records of interventions, the majority of which are marked as to whether they were closed or cancelled. Within this dataset, tens of thousands are relevant to interventions in ESS, and a large fraction of those contain date-stamped project appraisal and evaluation documents. However, due to the inclusion of over 1,000 distinct contributing international aid organizations, IATI is known for its difficulty to interpret and inconsistency in structure of each activity record.

% At BMZ, even when ex-post evaluations are read by evaluators, they usually only read the cover sheet. Evaluators typically have to read a large number of documents, including activity reports. Ex post evaluations are a small part of the material used to make funding decisions in development cooperation interventions \citep{sustainabilityidosLearningKfWsExpost}. This further indicates that information outside the purely evaluative nature of past projects is quite important for funding decisions.

% One large existing collection can be found in \citep{vivaltHowMuchCan2020}, with 15,024 estimates from 635 papers on 20 types of interventions in international development. Notably, this work has catalogued 1,932 quantitative results from 307 separate papers over approximately 70 categories, when restricting attention to only those results that can be compared with results from another paper on the same intervention-outcome. A majority of papers were found to be assessing the same outcome, so only the latest results were chosen for the quantitative analysis. Only a small percentage of quantitative outcomes were among the 70 categories.

% Another larger collection of data is the AidData database. I did not investigate the AidData database due to my discovering it late in the thesis-writing process. Although AidData does not link to project documents itself, it does provide a more rigorously coded and disambiguated activity-codes, provides a more robust, consistently coded set of data about key features like planned disbursements, durations, and evaluation activity ratings. Furthermore, it contains organizations which are not reporting to IATI. The AidData is therefore a promising avenue for continued work, especially for data augmentation and improved data quality.


% \subsubsection{Determinants of Success in Development Cooperation Interventions}
% A longstanding, well-researched question in foreign aid has been ``which projects are effective?''. Knowing which types of features and aspects of project determine their effectiveness allows this work to ensure the proper information is supplied to the LLMs. It also allows construction of naive baselines, using purely statistical correlations between projects aspects and outcomes. In the last decade, focus has shifted towards experimental impact evaluations such as randomized controlled trials (RCTs), which experimentally answer specific questions about intervention effectiveness, producing a large body of high-quality evidence for effectiveness of a wide range of interventions \citep{olkenBanerjeeDufloKremer2020}.

% There are many broad ways of answering which interventions will be generally effective. One method is to distinguish between country-level factors, such as the economic and political conditions, and project-level factors, such as the amount of funding or sector of the project. While both country-level and project-level aspects are necessary when forecasting project performance, project-level aspects appear to be a much stronger determinant of project success, typically with an $R^2$ of about 0.2-0.3 of predicting success outcomes given specific project variables \citep{GoodCountriesGood2013} \citep{bulmanGoodCountriesGood2017} \citep{eilersVolumeRiskComplexitya}. In terms of project variables, one study finds that for KfW funded projects, technical complexity and longer implementation duration, as well as increased assessed risk at the beginning of the project correlate significantly with unsuccessful projects, and increased funds correlate significantly with successful ones; choice of project structure was not found to have a significant correlation with project success \citep{eilersVolumeRiskComplexitya}. 

% A comprehensive literature review finds that project size is indicative of project performance is mixed, duration is negatively correlated, while preparation time is positively correlated \citep{ashtonPuzzleMissingPieces2021}. There is low-strength evidence that non-governmental actors have better evaluation scores. The quality of the economic analysis and a strong analytical underpinning at the start is also positively correlated with success. Staff and management has been found to be key to success across a wide range of studies. A better track record of the task team lead alone is associated with an increase in the chance of project success by 6\%.

% A separate analysis found when regressing on different explanatory variables, larger projects do worse, Academic/NGO-implemented projects do better than government-implemented projects \citep{vivaltHowMuchCan2020}. See especially Table 7, with around 500 observations for these findings: 0.013 standard deviations decrease in outcome effect size as a function of size of study, -0.081 standard deviations worse outcome if from government compared to -0.018 from academia (so government is much worse, on average, even controlling for program size). RCTs do better (0.021 standard deviations). The region was found to matter, with  North Africa often performing better than South Africa, although there is insignificant regional data there to firmly draw that conclusion. In terms of specific intervention categories, there were clear significant differences, but it is not easy to make quick judgments about which types of interventions generalize. Essentially, reducing to the few variables investigated discards the key contextual background which ultimately drives most outcomes in development. One appropriate analogy may be attempting to model the next word in human language with a linear statistical model. The performance will be terrible because contextual cues form the majority of the required information to predict outcomes.

% Direct consultation with an economist in the evaluation department of BMZ revealed the most important predictor is the degree of ownership of a project. Secondary effects were the length of the project (negatively correlated), how capable the partner government is (positively correlated), previously identified high risks for projects (negatively correlated), and whether the project was directly integrated into a larger program, such as a country's large portfolio of energy sector projects (positively correlated).

% % country-level “macro” factors and project-level “micro” factors in driving project-level outcomes

% % Detailed statistical work investigating various hypotheses about interventions at particular from BMZ funded projects find evidence that  


% \subsubsection{Other Computer Modelling Methods}

% \textbf{[TODO: Add a diagram demonstrating how these models work and put in some more details demonstrating that I understand how they work in the text] } 

% ML is not the only tool used in modelling intervention outcomes in complex systems. IAMs have shown promise in modelling outcomes of specific policies, with the disadvantage that they are harder to use and set up, require a high computational power and expertise to use effectively, and are not rigorously benchmarked on large databases of existing interventions and associated outcomes. For any user-defined policy package (for example, introducing efficient clean-burning cookstoves in India), Greenhouse Gas and Air Pollution Interactions and Synergies (GAINS) can calculate the reduction in emissions (PM-2.5, NO$_x$, CO$_2$, etc), the improvement in ambient air quality, and the health impacts such as lives saved from lower PM-2.5 exposure \citep{CosteffectiveControlAir2011}. Other IAMs include the MIT Emissions Prediction and Policy Analysis (EPPA) model, which requires manually entering assumptions of the effects of policies into models of the world economy, calculates the implications on health and runs a CGE to estimate the economic effects \citep{MITEmissionsPrediction}. 

% In the domain of biodiversity, an ML-based framework called CAPTAIN uses a reinforcement learning (RL) agent coupled with a spatially explicit ecosystem simulation to statistically learn which areas to protect over time in order to maximize species survival under budget constraints, to maximize cost-effectiveness in protecting biodiversity \citep{silvestroImprovingBiodiversityProtection2022}. 
% Other techniques used to predict outcomes of interventions include linear optimisation combined with econometric theory, such as the Open Source Energy Modelling System (OSeMOSYS). OSeMOSYS simulates energy production and consumption under policy constraints including a model of the energy grid. By incorporating physical and known constraints, such models have the potential to predict outcomes of policy interventions over longer time horizons \citep{OSeMOSYSOpenSource2011}. An even more fine-grained, bottom up approach of modelling intervention outcomes is possible. For example, combining bio-economic farm optimization models with ABMs, researchers have modelled evolution of pesticide-related risks for the country of Switzerland \citep{dueriModelingImplicationsPolicy2024}.

% The underlying workings of these models are highly variable. What unifies them is their domain specificity: predicting outcomes of interventions has typically required careful coding of environmental variables and human involvement in manually calibrating results. It has not been possible with these models to take into account the broader factors that influence the success of specific interventions. Factors such as reputability of the partner country, the rate of success of similar interventions in the past, and judgement about the general fit between an intervention and its specific context, have thus far all been relegated to human judgement of aid decision makers.

%TODO: add ARINA? Examples from here? https://onlinelibrary.wiley.com/doi/full/10.1002/aaai.12085

\subsection{Methods and Capabilities}
This thesis implements an LLM-based forecasting method, predicting what the evaluation results will be for thousands of IATI records containing both a pre-intervention description of the activity, as well as a post- or mid-intervention evaluation of the results. To do so, thousands of pdfs were downloaded, ranked from most to least relevant for forecasting future outcomes or evaluating the end result of the activities, had their pages ranked and graded for relevance to the task, had quantitative and qualitative descriptions and results transcribed into a unified format, and next several versions of the LLM forecasting system were trialed on the validation set. Finally, the most promising version was used to predict evaluation outcomes on hundreds of evaluations.


Implementing the gold standard prediction method - superforecaster tournaments - to predict the efficacy of interventions such as new environmental laws in low and middle income countries (LMIC), specific interventions such as introduction of cleaner burning ovens, or the construction of a solar power plant would be worthwhile, but also costly and logistically challenging given the very large number of annual interventions over wide geographic regions. Even if such a tournament were to be ran, ML methods to estimate the outcomes could be complementary and increase the accuracy for such a tournament. This work focuses on the mimicking of techniques known to be effective for tournaments of superforecasters with LLMs, both to aid expert forecasters and grantmakers, and to provide direct, useful predictions for those without access to expert knowledge. Recently, the number of grant evaluators at BMZ has been reduced. This work may also assist the remaining grant evaluation to make the most of the time they do have available. While there has been no attempt at predicting real-world outcomes of interventions in developmental aid and cooperation interventions affecting the environment\ while also rigorously quantifying the skill of such a system, much encouraging progress has been made in closely adjacent domains which I will survey below.

If using LLMs to directly output probabilities or yes/no answers to forecasting questions, the base models appear to underperform compared to crowds of humans \citep{abolghasemiHumansVsLarge2025} \citep{schoeneggerLargeLanguageModel2023}. In such a context, more recent work on the question has shown that increasing model reasoning ability increases the forecasting accuracy \citep{yanq.serajr.hej.mengl.andsylvaint.AutoCastEnhancingWorld2024}, and that with proper techniques and careful prompting, LLMs will approach or sometimes exceed accuracy of assemblages of superforecasters on questions with a high degree of context and with proper ensembling and fine-tuning of the LLM system \citep{halawiApproachingHumanLevelForecasting2024}. \citep{WisdomSiliconCrowd} \citep{abolghasemiHumansVsLarge2025} \citep{yanq.serajr.hej.mengl.andsylvaint.AutoCastEnhancingWorld2024}. 

In a recent study, a RAG+fine-tuned LLM system was sufficiently more skilled than the human crowd to reliably earn a profit on Polymarket event predictions \citep{turtelLLMsCanTeach2025}, providing a real-world example of the prediction skill of such systems against humans.

Despite these findings, it has been argued that utilization of the direct probabilities in complex domains may be more accurate, if the prediction is a function of ``many noisy intertwined signals across subfields'', in which case methods such as CoT may reduce the power of ``intuition'' available to the model \citep{xLargeLanguageModels2025}.

In general however, the best results are achieved by capitalizing on the broad world-knowledge of LLMs and the augmentation of their knowledge in high-news or near-term contexts \citep{halawiApproachingHumanLevelForecasting2024}. Along these lines, several improvements to the base-level prediction ability can be applied to approach superforecasting level calibration and accuracy. These include: 
\begin{enumerate}
\item Fine-tuning the LLMs to replicate the format of good forecasts, using hundreds or thousands of correct forecasts as the fine-tuning dataset (or in some cases, directly fine-tuning on existing content in the target area \citep{wenPredictingEmpiricalAI2025})
\item Have the LLM integrate relevant and timely information into the context to improve the forecast
\item Have the LLM split questions into sub-questions before being used to query RAG system
\item Prompting techniques (Have the LLM think step-by-step, rephrase the question to improve comprehension, and reason over chains of crafted prompts to ensure sufficient reasoning effort has gone into the answer)
\item Reduce error rates by ensembling the final predictions (``Wisdom of the crowd'')
\item Testing a variety of prompts [NOTE: CITE that ``Or How I learned to be careful about prompt variants''] and reducing the complexity of the prompt to prevent the model from forgetting its training data \citep{kaiserLeveragingLLMsPredictive2025}
\end{enumerate}


In one similar work, the technique of Chain of Thought (CoT) has been used to improve the reasoning abilities of GPT-4 in predicting the outcome of 1261 conclusions from 276 papers which analyze the real-world outcomes of field experiments in the social sciences. While not specifically investigating outcomes with relevance in the Earth system sciences, they do investigate the prediction ability for the impact of educational incentives, household finance behavior, healthcare enrollment, and financial planning. Remarkably, over the 1261 outcomes, 78\% were predicted accurately by the system \citep{chenPredictingFieldExperiments2025}.

In terms of social intervention outcome prediction, another study separately analyzed 346 treatment effects estimated from the responses of over one million participants, with hundreds of ex-ante predictions made from experts before the outcomes were known \citep{hewittPredictingResultsSocial}. The study adopted a bottom-up technique of simulating how individual respondents would respond to surveys and field experiments using GPT-4 according to their demographic profiles, specifically mimicking demographic profiles in the USA. The interventions included surveys that simulated the effect of informational content which promoted pro-democratic attitudes, encouraged respondents to increase beneficial choices with respect to climate change, and increase their vaccination rates. Notably GPT-4 matched or exceeded expert prediction accuracy in this domain. Interestingly, GPT-4 predictions were more accurate for survey experiments than field experiments (79\% vs 64\% accurate respectively). 

Another recent study found that LLMs can correctly predict outcomes in scientific domains such as predicting results of papers in neuroscience \citep{xLargeLanguageModels2025}. This result used the raw probabilities generated by the language model rather than explicit reasoning, and for this reason was able to use very small language models compared to GPT-4 as was used in most other studies. Because language models work by assigning a probability of each token (typically some commonly occurring part of a word),  multiplying the probabilities of all the words multiplied in the entire abstract allows researchers to compare the multiplied probability of the real abstract to the multiplied probability of the fabricated abstract directly, without having the language model generate any text involving reasoning or CoT. 

This capability could be related to the surprising ability of language models to perform direct time-series even in zero-shot settings. The findings relate to a wide range of domains (energy, traffic, weather, retail, health), and show that RLHF reduces performance in such domains \citep{ghasemlooInformedForecastingLeveraging2025}.

Another study found a similar result with regards to publications in the domain of AI algorithms, finding their system beating human experts in predicting the ability of an AI algorithm to improve on the state of the art performance in AI models \citep{wenPredictingEmpiricalAI2025}. In this domain, the researchers use a sophisticated framework with RAG and fine-tuning.

Insofar as identifying whether results from social science papers will replicate is a similar task as forecasting the impact of an intervention in developmental aid and cooperation interventions affecting the environment, we can be encouraged that statistical and categorical aspects of the interventions should be sufficient to identify the likely success of real-world outcomes, and remain skeptical that LLMs are strictly necessary to rival humans at predicting categorical outcomes, where ML may be sufficient. However, insofar as reasoning is required for forecasting in complex domains, non-reasoning ML models have a lower upper bound in potential accuracy than a full reasoning model, and regardless computational resources are not so restricted that LLMs could not be used in developmental aid and cooperation interventions affecting the environment. Furthermore, ML models using simple semantic vectors cannot produce free-form predictions of outcomes like LLMs, limiting the flexibility of their application in real-world use-cases.

Another study uses LLMs to predict the likely direction and effect size of empirical studies evaluating dietary interventions \citep{kaiserLeveragingLLMsPredictive2025}. This demonstrates that a fine‑tuned LLM can predict direction of empirical intervention outcomes better than classical meta‑regression baselines in dietary policy interventions, at an accuracy of approximately 80\% to predict the directional outcome of the policy. The authors utilize a fine-tuned version of GPT3.5 and carefully select prompt variants which tend to score higher. 

A very different result was found in the context of ex-post impact evaluations of interventions in developing countries. One study determined that from a large collection of existing ex-post evaluations of outcomes of similar interventions, there is a very high variability in the effect sizes even from the same intervention \citep{vivaltHowMuchCan2020}. They find limited benefit from a slightly more complex mixed effects model with explanatory variables, rather than a random effects model. 
We may infer from \citep{vivaltHowMuchCan2020} that there is both the possibility that by taking into account contextual heterogeneity between interventions, prediction could be greatly improved compared to a statistical baseline, and the risk for LLMs that quantitative predictability is simply very low in general (because no other studies I found collected as many quantitative results and compared them). One possibility explaining this results is that parameter heterogeneity is in fact to be driven by economy- or institution-wide contextual factors, rather than specific characteristics of the intervention itself \citep{pritchettContextMattersSize2013}.

% The stuff below is interesting, but probably not relevant anymore because I'm not looking at published studies.

%  rewrite in own words after reading study: ``An inference about another study will have the correct sign
% about 61\% of the time. If trying to predict the treatment effect of a similar study using
% only the mean treatment effect in an intervention-outcome
% combination, the median ratio of the MSE to that mean is 2.49 across intervention-outcome combinations. Only about 6\% of total variance can be attributed to sampling variance. Modelling the variation with a mixed model can help a little,
% but not a lot....only about 6\% of the observed variation in study results can be attributed to sampling
% variance. I find about 20\% of the remaining variance could be explained using a single
% best-fitting explanatory variable. However, this statistic obscures a lot of heterogeneity,
% with the median decrease being about 10\% among the intervention-outcomes for which
% this comparison was made.

% In a separate study by the same author, they a clear overall publication bias using the ``caliper'' test, but it remains somewhat smaller than other social sciences. The ``caliper'' test counts the number of papers nudging their results to be just over the 5\% significance boundary for reporting a significant result. There is a clear statistical "nudging" going on, but in development, it appears to be primarily for very small shifts in results, and not particularly common when considering wider bands around the 5\% threshold. RCT studies fared significantly better than non-RCT, with statistically lower bias \citep{vivaltSpecificationSearchingSignificance2019}. The biases in development programs were found to be much smaller than those previously observed in other social sciences (such as in \citep{gerberPublicationBiasEmpirical2008}). Another analysis finds:
% ''
% ``For ease of exposition we begin by comparing results that are insignificant at the
% 5 percent level to results that are significant at the 5 percent level. In the IV lit-
% erature, a result that is statistically insignificant is only 21.4 percent as likely to
% be published as a significant one. Said differently, a significant IV result is almost
% 5 times more likely to be published than an insignificant IV result. In the DID liter-
% ature, a statistically significant result is 4.2 times more likely to be published. For
% RCTs, a significant result is only 1.9 times more likely to be published. For RDDs,
% a significant result is 2.8 times more likely to be published. All of these estimates
% are statistically significant at the 1 percent level.''

% ``  We find that the ratio of tests just above and below 1.96 is only 1.10 in economics in
%   comparison to over 2 for political science and sociology. This result provides strong
%   evidence that the extent of p-hacking is much smaller in economics (at least when
%   using these inference methods) than in other disciplines''
% \citep{brodeurMethodsMatterPHacking2020}

If LLMs are able to approach or surpass human ability in predicting unpublished results in complex domains of predicting which techniques in improving state of the art AI system performance, predicting the outcomes of neuroscience papers, predicting social science replicability, the impact of informational field campaigns, or predicting geopolitical events such as election results, then it stands to reason that they may be able to predict the outcomes of interventions in developmental aid and cooperation interventions affecting the environment. While geopolitical forecasting may not be amenable to scientific techniques, neuroscience and AI algorithm improvements certainly are - yet LLMs still beat human experts in these domains. Furthermore, LLM systems are far simpler to use, and far less costly to run and maintain than IAMs, CGEs, or ABMs, while having the benefit of producing human-interpretable reasoning and the ability to be extremely flexible as to their domain of application. Finally, given their low cost to use, LLMs can often be used as starting points or augmentation to expert judgement in ex-ante outcome prediction, rather than being the sole source of judgement about expected intervention outcomes, and the collaboration has been found to produce a higher forecast accuracy than expert forecasts or LLM forecasts alone \citep{schoeneggerAIAugmentedPredictionsLLM2025}\citep{schoeneggerPromptEngineeringLarge2025}. However, caution is also warranted - in some cases, humans over-adjust their estimates towards the weaker LLM forecast, and the results can be worse than humans alone. %[TODO: FIND THIS CITATION SOMEWHERE! I KNOW I SENT IT TO MYSELF AT SOME POINT].

\subsection{Limitations}
\label{sub:limitations}
As might be expected given the absence of real-world experience and limited reasoning abilities of LLMs, simply replacing a crowd of humans with a crowd of untrained LLMs does not generally outperform the crowd average, especially where unpredictability and volatility of the question require strong reasoning abilities and good judgement to integrate relevant information into forecasts \citep{abolghasemiHumansVsLarge2025} \citep{schoeneggerLargeLanguageModel2023}. Therefore, moderate-to-high complexity in the forecasting framework surrounding the LLM is required for a well-performing system. As a limitation, that this reduces this thesis's reproducibility and increases software maintenance requirements, and making it more difficult to produce useful forecasting systems.

It remains an open question whether forecasting systems can reproduce the success in other domains, with at least one study indicating forecasting in developmental aid and cooperation interventions affecting the environment\ may be especially challenging. The study previously mentioned, with the bottom-up technique of simulating how individual respondents would respond to surveys and field experiments using GPT-4 according to their demographic profiles, found that the ``social policy'' papers had a relatively low correlation with prediction accuracy at an accuracy of 0.64 compared to an average of about 0.9 compared to other studies \citep{hewittPredictingResultsSocial}. Although the methodology may lead to differing outcomes (simulating individual profiles in their work, as compared to versus the approach of this thesis, which prompts the LLM to directly reason out the answer), this may hint that public policy and similar domains may be more difficult to predict than other scientific results.

In another study regarding LLM forecasting of food policy, the \emph{direction} (positive vs negative sign of the intervention’s impact) was much more easily predictable than the absolute effect of the intervention. The fine-tuned version with a small prompt was found to have a 79\% success rate at predicting the direction on the held-out test set, handily besting the random-effects model baseline rate of success of 66\%. However, the $\mu_e$ average error was -.051, while much better than -1.92 for the random effects baseline for estimating the Cohen's \emph{d} effect size, is not encouraging in absolute terms. 

The use of LLMs to inform decision making for outcome prediction in developmental aid and cooperation interventions affecting the environment\ comes also with several downsides. Notably, LLMs do not reason like humans, and are prone to ``hallucinations''  where facts are fabricated. These hallucinations can be either factual fabrications attributed to external source material, or false statements which come intrinsically from the model \citep{huangSurveyHallucinationLarge2025}. For the purposes of probabilistic reasoning, LLMs are not typically skilled at ensuring probabilities sum to 100\%, or related quantitative skilled, even after fine-tuning on the task of probability predictions \citep{lyuCalibratingLargeLanguage2025}. As mentioned previously, LLMs are more computationally costly than other ML methods. There are also issues (which we will leave for the \hyperref[sec:conclusion_outlook]{Conclusion \& Outlook} section) with overly trusting LLMs, false beliefs from users of LLMs that they are less biased than humans or not biased at all, and issues with AI safety, if LLMs begin to replace or distort, rather than augment, human decision making.


Furthermore, the majority of work thus far has focused on either classification or fixed categories. At best, assigning a numerical score to a list of fixed objectives  \citep{binaLargeLanguageModels2025}. Open-ended future event prediction will be increasingly necessary for specific event prediction which cannot be easily quantified into a series of rankings or clear outcome categories. Some of the most important outcomes of interventions are the unexpected effects and learnings from the work, which cannot be captured by rigid outcome category schemes. Past work has used LLMs such as GPT-4 to evaluate free-form event prediction on Accuracy, Completeness, Relevance (how pertinent the prediction is to the actual outcomes), Specificity (not overly broad nor vague), and Reasonableness (logical coherence and believability of the prediction) \citep{guanOpenEPOpenEndedFuture2024}. However, the work finds that accurately predicting future events in open-ended settings is challenging for existing LLMs, as predictions are often incomplete, underspecified, irrelevant, or illogical.

While much cheaper than prediction markets or IAMs, LLMs are also more computationally expensive than simpler ML models. When attempting to forecast whether results and effect sizes replicate in social sciences, simple neural network classifiers trained on millions of scientific abstracts and hundreds of full texts, the unordered semantic vectors of the words in the abstracts of the papers, combined with statistical  were sufficient to approach prediction market level accuracy of approximately 70\% accuracy in predicting which paper results would replicate, despite lacking fundamental logical relationships between words in the text or any deeper language comprehension of the methods of the abstracts \citep{yangEstimatingDeepReplicability2020}. This finding mirrors that of the neuroscience study \citep{xLargeLanguageModels2025} which finds that explicit reasoning through CoT is not strictly required to predict the outcomes in neuroscience abstracts. It is an open question in developmental aid and cooperation interventions affecting the environment\ whether LLMs are necessary, where maybe simpler ML techniques could be sufficient in many use-cases, although we leave it for future work.

There are also several limitations in extending best-performing or fine-tuned LLM forecasting systems to real-world use cases.

One issue is that the interventions in the literature are highly skewed toward a narrow range of topics, meaning the best performing system may succeed by being a specialist, rather than a generalist. For example, China has a much larger number of evaluations than other countries in the dataset, meaning that an LLM system may devote resources to becoming skilled at predicting Chinese development context, rather than development as a whole.

Model skill may not transfer when releasing a model into a real-world domain where the predicted outcome is truly in the future. Model cutoff dates are often not truly leakage free - some training, such as Reinforcement Learning from Human Feedback (RLHF) can introduce coarse details about events occurring after the model cutoff date. The system prompt (which cannot be directly inspected in closed-source LLMs such as GPT-3.5) can also contain unintended information leakage, and post-resolution documents in search results can further leak hints or the outcome itself\citep{palekaPitfallsEvaluatingLanguage2025}.

Even if there is no leakage, ranking forecasting skill using single scoring metrics can be misleading - each evaluation metric has its own issues \citep{palekaPitfallsEvaluatingLanguage2025} (See Table \ref{tab:forecasting-metrics-pitfalls} and section \ref{sub:scoringmetrics}). Therefore what may appear to be the best combination of accuracy-improving techniques and the best selection of base LLM may not in fact be the same outside of the test and validation sets.  Language models themselves contain both political and stereotype biases which can bleed into both the rationales and the probabilities a system outputs \citep{nadeemStereoSetMeasuringStereotypical2021} \citep{bangMeasuringPoliticalBias2024}. 

Language models also don't always report their true reasoning  - even if they reason something through scratchpads or CoT, the true reasons behind the answer may differ significantly. This can make using free-form reasoning for forecasts unreliable \citep{turpinLanguageModelsDont2023}. 


\clearpage
% \subsection{Predicting Outcomes with AI \DOMAINCAPITALIZEDINTERVENTION} % DON'T THINK I'LL KEEP THIS SECTION
 \section{Methods for LLM Forecasting}
\subsection{Selecting LLMs for Forecasting Outcomes in Development Cooperation Interventions Affecting the Environment}
% TODO: update this once we know whether we're fine-tuning
Multiple studies have measured zero-shot LLM forecasting capability against the base model performance, and found better general ability base models tend to perform better on forecasting tasks \citep{halawiApproachingHumanLevelForecasting2024} \citep{kargerForecastBenchDynamicBenchmark2024}: In one study with dozens of base models and a dynamically updating benchmark on prediction market forecasting questions, an inverse linear relationship was found between the human preference of a model's answer (in terms of an ELO score) and the Brier score, and similarly a log-linear inverse relationship between the compute used to train the model and the Brier score \citep{kargerForecastBenchDynamicBenchmark2024}.

In order to guard against leakage of information from the training, I selected deepseek %Llama 70B \citep{touvronLlama2Open2023}%
as my forecasting model, due to its strong performance comparable to other models which have similar training cutoff dates (2023 for Deepseek V3.2).

%Llama 70B is notable as it is a strong open source model with a relatively early training cutoff date of 2022, allowing us to inspect more directly the system prompt, the direct probabilities of sets of output tokens (the ``logits''), and the degree of memorization of the training via use of the zlib entropy and the perplexity ratio (See Section \ref{sub:strengths_and_weaknesses_of_this_forecasting_system} for more details).

\subsection{Data Sources} \label{sec:methods_for_llm_forecasting}

After considering several data sources for prediction, including the OpenAlex publication repository of peer-reviewed evaluation documents and abstracts, the IDEAL database of ex-post evaluations, the 3ie development database, I decided to use the IATI database, due to its substantial quantity of information available in textual format and extractable from the database records, and its sheer size. While ex-post evaluations may provide sufficient information to describe the activity, it may introduce ``future leakage'' to rely on language models to completely remove information about the eventual outcome. Furthermore, although many millions of evaluations are available, it proved difficult to reliably identify and de-duplicate academic papers regarding evaluations of environmental interventions. The IDEAL database and 3ie were in the dozens or hundreds of records for environmental topics.

The IATI database has reliable start and end dates, and typically several recorded outcomes, and usually an overall evaluation rating on a six point scale  within linked evaluation pdfs. It also reliably marks the reporting organization, allowing for an intelligent unification of the rating scales, and sometimes provides a ``results'' section where outcomes of an activity can sometimes be found. It is quite common for several activity information documents to be uploaded near the beginning of the activity, and several years later, at least one ex-post evaluation of the activity is uploaded as well, or results of key quantitative outcomes are sometimes directly recorded in the IATI database. The status of the activity is extremely commonly reported, including if it is in the planning or completion/finalization stages, which is helpful information for forecasting.

The downside of the IATI database is it is highly inconsistent between reporting organizations as to how the data are filled in, and the format of documents was not always PDF, requiring conversion scripts. Also, many download links were not functioning or required custom web-scraping scripts to properly extract project documents in pdf format from the original websites where project documents were hosted. Dates of documents and especially planned start or end dates, or actual start or end dates, were often missing, leading to frequent exclusion of projects. Furthermore, approximately 30\% of IATI activities do not have an activity category code, leading to a further exclusion of environmental or sustainability related activities.

\subsection{Data Filtering}

\textbf{IATI records for prediction}

Out of the approximately 800,000 international aid activities recorded in IATI, I first reduced the set of activities of interest to 7,575 records which aimed to improving the environment, sustainability, or climate adaptation in a developing country or countries, had an appraisal/intervention description document, and an outcome evaluation or progress report document, both of which could be converted to PDF format. Links to these documents were then downloaded where possible (see the next section for details).

Once documents were downloaded and converted to pdf format, activities were further filtered so that they had at least one document describing the activity, and had a metadata date before 1/4 of the activity implementation period, as well as at least one ex-ante activity at least 3/4 through the activity period. The latest activity document also had to have a metadata date at least one year before the earliest evaluation document. An exception was project appraisal documents from the World Bank, or Project Information Documents, which were found to reliably not leak future information, and this was judged to be more trustworthy than extracting the creation date embedded in the activity document. Activities not meeting these requirements were also excluded, leaving 3,225 activities.

After passing all these filters, I finally attempted to extract the activity rating from the evaluation document using two separate methods. Because rating tendencies are systematically different for different reporting orgs, I needed sufficient data for training, validation, and testing for all organizations. I also restricted activities to those that were marked as "completed" in order to ensure comparability between rating scales, as the only activities not marked as ``completed'' were relatively recent and would dominate the held-out test set. I determined there were sufficient data for four reporting organizations: The World Bank (957 activities), BMZ/KFW/GIZ (240 activities), the Asian Development Bank (ADB) (156 activities) and the UK Foreign Commonwealth and Development Office (FCDO) (127 activities).



The activity filtering for the topic was done by-hand to filter only those activities relating to improving the environment or sustainability. These were:
\begin{itemize}[noitemsep, topsep=0pt]
\item 14015: Water resources conservation (including data collection)
\item 14020: Water supply and sanitation - large systems
\item 14021: Water supply - large systems
\item 14022: Sanitation - large systems
\item 14032: Basic sanitation
\item 14050: Waste management/disposal
\item 23110: Energy policy and administrative management
\item 23111: Energy sector policy, planning and administration
\item 23112: Energy regulation
\item 23183: Energy conservation and demand-side efficiency
\item 23210: Energy generation, renewable sources - multiple technologies
\item 23220: Hydro-electric power plants
\item 23230: Solar energy for centralised grids
\item 23231: Solar energy for isolated grids and standalone systems
\item 23232: Solar energy - thermal applications
\item 23240: Wind energy
\item 23250: Marine energy
\item 23260: Geothermal energy
\item 23270: Biofuel-fired power plants
\item 23350: Fossil fuel electric power plants with carbon capture and storage (CCS)
\item 23360: Non-renewable waste-fired electric power plants
\item 23410: Hybrid energy electric power plants
\item 23510: Nuclear energy electric power plants and nuclear safety
\item 23610: Heat plants
\item 23630: Electric power transmission and distribution (centralised grids)
\item 23631: Electric power transmission and distribution (isolated mini-grids)
\item 23642: Electric mobility infrastructures
\item 31130: Agricultural land resources
\item 31210: Forestry policy and administrative management
\item 31220: Forestry development
\item 31281: Forestry education/training
\item 31282: Forestry research
\item 31291: Forestry services
\item 32174: Clean cooking appliances manufacturing
\item 41010: Environmental policy and administrative management
\item 41020: Biosphere protection
\item 41030: Biodiversity
\item 41081: Environmental education/training
\item 41082: Environmental research
\end{itemize}


% Figure: workflow 
\begin{figure}[h]
  \centering
  \includegraphics[width=1\textwidth]{assets/pie_plot_env_categories.png}
  \caption{The split of topics analyzed from the dataset.}
  \label{fig:workflow}
\end{figure}



\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\textwidth]{assets/splits_by_start_year.png}
  \caption{The activities included for predicting ratings, with the splits by count year. Incomplete activities, shown in red, were not used for prediction. Activity ids used for outcome prediction were given the same temporal boundaries.}
  \label{fig:splits}

\end{figure}



% Figure: predicted vs observed absolute error (scatter)
\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\textwidth]{assets/disbursement_by_year.png}
  \caption{Total disbursements for IATI activities used for ratings, by year. There is no clear trend over time for activity size in the database. }
  \label{fig:disbursementsbyyear}
\end{figure}

% \end{figure}
% Figure: predicted vs observed absolute error (scatter)
\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\textwidth]{assets/duration_by_split.png}
  \caption{The durations of activities per split. More recently starting activities tend to be shorter, as they have not yet had time to complete and be evaluated. The out-of-distribution nature of validation and test sets increase the challenge of generalizing patterns from the training data. }
  \label{fig:splitdurations}
\end{figure}



\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\textwidth]{assets/reporting_org_splits.png}
  \caption{The breakdown of reporting orgs in the dataset which were used for training, validation and testing. The validation and test set are in this way significantly out-of-distribution.}`'
  \label{fig:shapratings}
\end{figure}





\textbf{Document-related Filtering}

The IATI database contains a collection of thousands of links to pdfs, word documents, html documents, and other document formats. These were first automatically converted to pdf format via a custom python script, and subsequently needed to pass several criteria before being used as documents for forecasting.

I first wrote a script that directly downloaded these and converted them to pdf format. %Specifically for UNDP, I made a custom-query to convert their interactive HTML results pages into a pdf format, all others had the links for the documents downloaded directly with no additional processing. #well, didn't end up using any UNDP
Next, I look at the pdf metadata date, and determine the creation date of the pdf files. I find this is more often closer to the date of the specific activity description document or activity evaluation document (as determined by reading the document) than metadata at the url indicating upload date, or the date entered in IATI for the document.  UNDP results had the URL specifically included in the JSON payload populating their website, so the latest year indicated in that evaluation payload was used instead for the UNDP results.

All documents are tagged in IATI with one or more of the following tags per document:
``Pre- and post-project impact appraisal'', ``Objectives / Purpose of activity'', ``Intended ultimate beneficiaries'', ``Conditions'', ``Budget'', ``Summary information about contract'', ``Review of project performance and evaluation'', ``Results, outcomes and outputs'', ``Memorandum of understanding (If agreed by all parties)'', ``Tender'', or ``Contract''. 

I mark documents with ``Objectives / Purpose of activity'' or ``Summary information about contract'', tags as preliminary ``baseline'' documents - those representing information about the activity before it begins. Documents with ``Review of project performance and evaluation'' or ``Pre- and post-project impact appraisal'' tags are marked as preliminary evaluation documents. In order to provide sufficient information to forecast with and sufficient information to evaluate that forecast, I require at least one ``baseline'' document and at least one ``outcome'' document per activity, with the baseline document at least one year prior to the evaluation document (based on the uploaded document metadata date). I also require that the activity status code is not ``Pipeline/identification''. Instead, activities are allowed to be in implementation, finalization, closed, cancelled, or suspended, such that either a final or preliminary evaluation document is possible.

I filtered further to ensure that all activity document labels which were "Conditions", "Budget", "Tender", "Contract" with no other tags were excluded, as these were typically purely legal context, often containing very little evaluation or useful additional activity information.

The date for the documents were determined using (in descending preference where available) the pdf's ``created on'' or ``last modified'' in its metadata, or the date indicated in the IATI record for the document. This ordering preference was determined as the PDF metadata dates were found to more reliably match the stated date of authorship of the documents better than the ``IATI date'' recorded within the activity record. These dates were usually available in the metadata of the original PDF, ODT, DOC, or DOCX file. Experimenting with different date options revealed that out of 400 randomly selected PDFs, the closest available date to the true date of authoring the document was the ``created by'' date, then the ``last modified'' date, then the ``IATI date''. The median difference for the date when selecting this ordering was 22 days different than the date indicated in the document itself as determined by feeding the first 3 pages of each of those 400 PDF documents to \emph{gemini-2.5-flash}.

To ensure pdf metadata dates were appropriate, an analysis was undergone to ensure the procedure for selecting the date of activity documents was valid. If the date of the activity is too early, it could lead to documents authored well after the project start leaking future information. To test this, 400 random pdf documents downloaded were uploaded to gemini and the pdf metadata dates were inspected. PDF metadata dates were discovered to have a median difference of 22 days from the date indicated on the document as extracted by gemini. It was discovered that while approximately 10\% of the dates were more than a year after the actual date indicated on the document (such that an activity was actually authored earlier than the start date of the activity), a concerning 0.5\% of documents had a creation pdf metadata date later than the date extracted directly from the document. 

In order to ensure the forecasts were all based on project information available only roughly at the beginning of the activity, a search was undergone through the information available to the model when forecasting to ensure the forecasting was based only on what could have been known at the beginning of the activity. Approximately 10 activities with pdf metadata dates more than a year earlier than the true authoring of activity documents would be expected, based on the 0.5\% rate of ``>1 year too early'' errors from the date analysis.

To prevent any information leakage, which could be due to incorrect dates as well as incorrect marking of the start date of the activity in IATI, or significant progress being made within the first quarter of the activity where documents are allowed, approximately 40 random chatgpt-generated activity summaries (see the next section) were inspected, with none indicating advanced progress, indicating less than 2.5\% of activities should be of concern. A selective search for phrases revealed some activities had made clear progress on targeted outcomes. Consequently, a python script with 6,800 separate search terms was used to further search for inappropriate documents. Exact string search terms were made, with variants of phrases including, ``on track'', ``ongoing project has been performing'', ``ongoing project is performing'', ``already made considerable progress'', ``key milestones already achieved'', ``significant progress had been made'', ``the programme has already made considerable progress'', etc. This led to the review of approximately 150 additional activities, and the discovery of 21 activities with clear progress on key project milestones. Progress such as the formation of planning committees or initial disbursements of funds to the implementing organization were not considered grounds for exclusion, given that these milestones are unlikely to be substantially informative. However, extension activities or Phase II / Phase III activities were not excluded, unless significant progress had already been made on the extension or phase being evaluated.


In order to properly extract accurate overall success ratings for each activity and useful textual information about the project for forecasting, I processed each pdf document using the following data processing pipeline:

\subsection{Preliminary Data Processing}

All document pages had their rotation detected, and were rotated to vertical before processing via the Gemini API. Documents with ``.odt'', ``.doc'', or ``.docx'' extensions were converted to pdfs with a custom script. The pages when converted to pdfs were counted and zero-page documents were excluded.

\textbf{1. Ranking documents}
Documents were ranked from most to least useful for forecasting the outcome, or evaluating the results, respectively. \emph{gemini-2.5-flash} structured output with direct pdf input was used to make the rankings. Only documents with c- or better grades on a grading scale from a+ to f were considered for the next stage. Also, the documents were ranked from most to least informative for forecasting among the baseline documents, and most to least valuable for ex-post evaluation among the outcome documents. Baseline documents that were closest to the activity start, and the latest outcome documents were preferenced. Documents with sufficient detail but not excessive lengths, such as executive summaries, were prioritized. Documents that were duplicates in a non-English language were excluded if the equivalent was available in English. For outcomes, if there were multiple progress reports, all the earlier ones were excluded and only the latest were kept in the rankings. After ranking, 2,312 documents had sufficiently informative activity information and activity evaluation documents.

\textbf{2. Categorizing pages within documents}
The highest ranking documents were then split into 3-page chunks. Each 3-page chunk was sent in pdf form to \emph{gemini-2.5-flash}. The pages were categorized differently based on whether the document was a baseline or outcome. Categories for outcomes allowed retrieval based on whether final evaluation in quantitative or qualitative form are present on the page, deviations from plans or other types of outcomes were detailed, or if the pages were simply overviews of the activity. Specifically, the allowed categorizations were ``condensed summary'', ``sub activities outlined'', ``detailed implementation plans'', ``broad objectives'', ``possible outcomes'', ``quantitative targets'', ``qualitative targets'', ``risks as word or numeric'', ``risks or dangers generally'', ``plans to address key risks'', ``positive indicators'', ``progress reports'', ``similar cases outcomes'', ``implementation context country'', ``contextual challenges'', ``financing details'', ``budget and legal'', ``who implements'', ``whether part of larger program'', ``partner identity or skill'', ``whether skin in the game'', ``other stakeholder engagement'', or ``activity monitoring details'' for baseline document pages, and  ``expected outcomes'', ``deviation from plans'', ``preliminary results'', ``final outcomes'', ``delays or early completion'', ``over or under spending'', ``overview as was planned'', or ``unrelated to evaluation'' for outcome document pages. Only one category choice among these was possible per page.

In order to exclude irrelevant pages, the pages were also given a second category, for outcome document pages as ``glossary'', ``blank  page'', ``table of contents'', ``outcome evaluation'', ``activity description'', ``references'', or ``other'', and for baseline document pages the same categories were options, in addition to ``core activities'', ``theory of change'', ``targets'', ``broader context'', and ``preliminary results''. Only one category choice among these was possible per page.


\textbf{3. Extracting Ratings}
Two separate methods were used to extract rankings. The first method sent each individual outcome page ranked above 7/10 for relevance to evaluation, or with a ``quantitative targets'' categorization, to \emph{gemini-2.5-flash} to extract any overall ratings, and a second script summarized the overall ratings into a single value for the document. However, this was often insufficient to capture the overall ratings. Another ``fallback'' script involved a custom generated word search with approximately 500 different rephrasings of ``overall rating'', ``final result'', ``synthesized score'', etc, in English, and searched the pdfs directly for an exact match on those terms, prioritizing pages with one or more exact text matches of such terms. Otherwise, if such words could not be found, the earliest pages in the document which were not categorized as ``blank page'', ``appendix'', ``glossary'', ``table of contents'', ``references'', or ``activity description'' were included and \emph{gemini-2.5-flash} was queried to extract the overall rating from the documents.

%A slightly different process was done for UNDP activities. The fallback script was used again, but documents were judged by \emph{gemini-2.5-flash} for whether the outcomes represented an ``overall successful'' or ``overall unsuccessful'' activity, and this was entered as the result, as UNDP rarely delivers directly an overall rating for the activity. # we don't use these anymore

For BMZ/GIZ/KFW documents, activity baseline documents were extremely rare. For this reason, the evaluation document was treated as a baseline document for the purposes of forecasting activity success. Categorization for these evaluations also was via the ``baseline'' document method described above. When grading or summarizing the features of the evaluation document, \emph{gemini-2.5-flash} was instructed to only describe what could have been known at the beginning of the activity, and to under no circumstances reveal the final outcome of the activity.

\textbf{4. Interpreting Ratings}
Ratings were reported both with the rating itself, as well as a maximum and minimum possible rating. The World Bank rating scale from 1 (``Highly Unsatisfactory'') to 6 (``Highly Satisfactory'') was used as the template rating, and other ratings were attempted to match against this scale. Notably, BMZ/GIZ/KFW ratings were inverted to reach this scale. A ``Satisfactory'' score was considered equivalent to scores such as ``successful'', ``On Track'', or ``met expectations''. Scores listed as percentages or fractions were re-scaled to the 1 to 6 scale as well. In order to ensure ratings were fairly compared, only the top four most common organizations with ratings were included for training and validation of the forecasting system. %Because UNDP results were always binary from the rating of \emph{gemini-2.5-flash}, and the Asian Development Bank was nearly always rated either "successful" or "needs improvement" % todo: check this
%, both of these were collapsed to the binary ``satisfactory''/``unsatisfactory'' ratings.


\FloatBarrier
\subsubsection{Outcome Extraction}
In addition to extracting ratings, a similar approach was used with \emph{gemini-2.5-flash} to extract quantitative outcomes. I extracted quantitative outcomes from all pages which were categorized as outcomes, and marked as containing quantitative information (see Figure \ref{fig:quant-extraction-prompt}). Unlike with ratings, I did not limit to the top 4 reporting organizations, as reporting ratings is more susceptible to between-reporting-organization variation and gaming than the reportable quantitative outcomes of projects.

% =========================
% FIGURE X: Quantitative outcome extraction prompt (structured JSON)
% =========================
\begin{figure}[htbp]
  \centering
  \begin{tcolorbox}[left=4pt, right=4pt, top=4pt, bottom=4pt, width=0.96\textwidth]
\ttfamily\footnotesize
SYSTEM:\newline
You are extracting structured quantitative information from evaluation PDFs.
Return only valid JSON that matches the provided schema. Do not include any other text.\newline

USER:\newline
You are extracting information from the attached single  page of an activity evaluation PDF.\newline
Extract quantitative outcomes that are due to the activity.\newline\newline

CONTEXT:\newline
- ACTIVITY TITLE: \{activity\_title\}\newline
- ACTIVITY DESCRIPTION: \{activity\_description\_truncated\}\newline
- PAGE NUMBER: \{page\_number\}\newline
- ACTIVITY TIMEFRAME: \{start\_date\} to \{end\_date\}\newline\newline

INSTRUCTIONS:\newline
- For each indicator/metric found, add an entry to \texttt{quantitative\_outcomes}.\newline
- Each entry must include a short \texttt{description} of the metric and the \texttt{outcome\_value}.\newline
- Where explicitly available, also include:\newline
\ \ \ \texttt{baseline\_value} (before the activity) and \texttt{target\_value} (intended for the activity).\newline
- Use \texttt{units} only when the page clearly specifies them; keep units short (e.g., \%, count, people, USD).\newline
- Only report quantities that reflect the conclusion of the activity or the full activity period.\newline
- Leave out entries where it is unclear how to correctly enter them.\newline\newline

OUTPUT JSON SHAPE (schema-enforced):\newline
\ \ "quantitative\_outcomes": [\newline
\ \ \ \ \ "description": "...",\newline
\ \ \ \ \ "baseline\_value": <number optional>,\newline
\ \ \ \ \ "target\_value": <number optional>,\newline
\ \ \ \ \ "outcome\_value": <number required>,\newline
\ \ \ \ \ "units": "... optional"\newline
\ \ ]\newline
  \end{tcolorbox}
  \caption{Quantitative outcome extraction prompt used with structured JSON output. The model receives a single-page PDF slice and returns a list of outcome metrics with optional baseline, target, and units where explicitly stated.}
  \label{fig:quant-extraction-prompt}
\end{figure}


Once these PDF pages were extracted, I employed a combination of manual examination of the extracted outcomes and a report of common bigrams to identify outcome variables that could be compared between projects. For each common outcome category, I came up with a list of words and phrases that would commonly match reports of these outcome variables in the description, as well as appropriate units. 


\paragraph{Keyword- and unit-based outcome parsers.}
For each outcome category, I implemented a dedicated parser that scans the extracted quantitative description, baseline, target, outcome, and units. Each parser follows the same general pipeline:
(i) \emph{filtering} by description keywords and unit constraints,
(ii) \emph{sanitization} of numeric values (e.g., dropping negative sentinels or implausible magnitudes),
(iii) \emph{normalization} to a canonical unit (e.g., hectares, tonnes, people),
and (iv) \emph{aggregation} to a single activity-level outcome.

\paragraph{Comparable outcome categories.}
Using manual inspection and frequency statistics over common bigrams, I defined a set of outcome categories that recur across evaluations and are interpretable across projects. The final set included:
\begin{itemize}
  \item \textbf{Cost--benefit ratios (B/C):} benefit-cost ratio outcomes.
  \item \textbf{Rates of return:} economic rate of return (ERR/EIRR) and financial rate of return (FRR/FIRR), in percent.
  \item \textbf{Beneficiaries:} counts of beneficiaries (people).
  \item \textbf{Emissions reductions:} CO\textsubscript{2} or CO\textsubscript{2}e reductions (total or per-year) in tonnes.
  \item \textbf{Water and sanitation connections:} counts of service connections, either new or repaired.
  \item \textbf{Pollution load removed:} wastewater pollutant load reductions (e.g., BOD/COD/nutrients), in tonnes and categorized by time basis (total, per-year, per-day).
  \item \textbf{Forest indicators:} trees/seedlings planted (counts) and area-based forest outcomes (reforested, under management, protected) in hectares.
  \item \textbf{Irrigation outcomes:} increases in irrigated area (hectares), computed as a positive increase relative to baseline where available.
  \item \textbf{Energy outcomes:} installed generation capacity, in MW (or occasionally GWh where the source reported capacity in energy units).
  \item \textbf{Air quality (PM2.5):} PM2.5 reductions reported as concentration, emissions, or percent, kept as separate distributions.
  \item \textbf{Clean cooking stoves:} counts of stoves distributed/installed.
  \item \textbf{Agricultural yields:} yield increases expressed either as level changes (normalized to tonnes per hectare when possible) or as percent increases.
\end{itemize}

\paragraph{Implementation details by category.}
Each category required a custom python script, primarily because evaluation reports often contain multiple related indicators (e.g., component-level versus project-level rates of return) and frequently use heterogeneous units or phrasing.

\subparagraph{Benefit--cost ratios (B/C).}
B/C ratio records were identified via a description keyword list (e.g., ``benefit/cost ratio'', ``B/C'') and a strict unit constraint: retained records must have empty units (since B/C is dimensionless). Non-numeric entries and implausible magnitudes were discarded. When multiple B/C indicators were present for an activity, regex heuristics were used to ranked records that explicitly referred to whole-project or overall ratios above component-specific ratios. The final activity-level B/C ratio was computed as the mean outcome across all indicators at the highest scope tier, as multiple extracted B/C quantities sometimes report the same project-wide metric in slightly different wording.

\subparagraph{ERR/EIRR and FRR/FIRR.}
Rates of return were detected using keyword lists for economic and financial variants, with an explicit unit requirement of percent. Similar to B/C, I assigned a scope score to prioritize project-level rates over subcomponent rates. Where both before-tax and after-tax versions appeared, I preferentially retained before-tax values when both were present. In some evaluations, both ``with'' and ``without'' environmental co-benefits variants are reported; to improve comparability, I preferentially retained ``without/excluding'' versions when available (and otherwise dropped ``with'' versions only if doing so did not eliminate all candidates). Final activity-level rates were computed by averaging outcomes within the best scope tier.

\subparagraph{Beneficiaries.}
Beneficiary outcomes were identified by requiring beneficiary language in the description and count-like units (e.g., people/persons/beneficiaries, including ``thousands'' or ``millions'') while excluding monetary and percentage units. Values were normalized to people by applying multipliers implied by the unit string. For each activity, I selected the maximum realized beneficiary count among candidate records, reflecting the common structure of reporting multiple beneficiary subgroups where the overall total is typically the largest.

\subparagraph{CO\textsubscript{2}/CO\textsubscript{2}e emission reductions.}
Emissions reductions were detected by combining (i) CO\textsubscript{2}/GHG cues in either units or the description and (ii) reduction/avoidance language (e.g., ``reduced'', ``avoided'', ``abated''). Unit parsing normalized values into tonnes using textual multipliers (thousand/million), and standard abbreviations (kt, Mt, MMT), plus kg-to-tonnes conversion when needed. Each record was classified as either a total reduction or a per-year reduction based on explicit annual language in the description or unit denominators. For each activity, I selected the largest reduction value within the preferred type tier (favoring totals when available, otherwise annual rates), yielding a single activity-level reduction with a canonical unit label.

\subparagraph{Water and sanitation connections.}
Connections are prone to false positives (e.g., electricity grid connections, gas hookups, staffing-per-connection ratios). To reduce such errors, I required: (i) a connection token match (including multilingual variants), (ii) explicit water/wastewater/sanitation context in the description, and (iii) count-like units (or empty units when the description clearly conveyed the meaning). Candidate records were scored to prioritize household/domestic service connections and newly provided/functional connections. The chosen activity-level value was the highest-scoring record, breaking ties by larger realized counts.

\subparagraph{Pollution load removed (wastewater).}
Pollution load outcomes were identified using reduction language (removed/reduced/abated/avoided) combined with ``load''/``pollution''/``discharge'' terms and mass-like units (tonnes/kg and variants). Because ``load reduction'' appears in unrelated contexts, I imposed a wastewater context filter (e.g., sewage, WWTP, effluent) and/or explicit pollutant tags (BOD, COD, nitrogen/phosphorus/TSS). I classified the time basis (per-year, per-day, or total) using unit and description patterns (e.g., ``t/yr'', ``per annum''). Numeric values were normalized to tonnes. Records were grouped by (activity, pollutant tag) and selected by a priority rule emphasizing wastewater context and explicit per-year (or per-day) reporting; within the best tier, I selected the maximum normalized outcome.

\subparagraph{Forest outcomes.}
Forest-related indicators were split into conceptually distinct distributions:
(i) trees/seedlings planted (counts),
(ii) area re/afforested or restored (hectares),
(iii) forest area under management (hectares),
and (iv) area protected (hectares).
Area records were retained only for pure area units (excluding per-hectare denominators and monetary units) and normalized using thousand/million modifiers. Tree-count records were retained only when units and description aligned with tree/seedling language and did not resemble beneficiary or other count metrics. For each activity and forest subcategory, I selected the maximum realized outcome (and retained associated baseline/target when available), reflecting the reporting convention that the most comprehensive figure is typically the largest.

\subparagraph{Irrigated area increases.}
Irrigation outcomes were detected by irrigation/drainage keywords plus area units. Values were normalized to hectares using unit-specific conversions (e.g., acres, feddan, mu, and thousand/million hectare abbreviations). To express the outcome as an increase attributable to the activity, I computed \texttt{outcome\_ha - baseline\_ha} when baseline existed; otherwise, I treated the outcome as the increase (common in ``area provided with improved irrigation'' indicators). I retained only positive increases and then selected the maximum increase per activity.

\subparagraph{Generation capacity.}
Energy generation capacity indicators were identified through ``capacity'' language combined with power-sector context (electricity/power/renewable/plant/turbine) and explicit power or energy units. I excluded common false positives such as ``capacity building'' and transport/wastewater contexts. Values were normalized to MW (or, in a small number of cases, to GWh when the reports used energy units to describe capacity), and per-activity selection favored total (stock) capacity over per-year variants where both appeared.

\subparagraph{PM2.5 reductions and clean cooking stoves.}
PM2.5 indicators were detected using explicit ``PM2.5'' mentions and then split into three separate distributions based on units: concentration reductions ($\mu$g/m$^3$), emissions reductions (tons), and percent reductions (\%). When baseline and outcome were available and the description did not already imply a reduction amount, I computed reductions as \texttt{baseline - outcome} where outcome improved relative to baseline. Clean cookstove outcomes were detected by stove keywords and count-like units, normalized using thousand/million modifiers, and aggregated as the maximum stove count per activity.

\subparagraph{Agricultural yields.}
Yield-related outcomes were split into (i) level changes that could be normalized to tonnes per hectare, and (ii) percent yield increases. For level yields, I required crop/agriculture context to avoid financial ``yield'' false positives and normalized common yield units (e.g., tons/ha, tons/feddan, kg/mu) into t/ha. Yield increases were computed as \texttt{outcome - baseline} and retained only when positive. For each activity, I aggregated by taking the mean increase across multiple yield indicators of the same type, yielding one per-activity value for t/ha increases and one for percent increases.

\paragraph{Aggregation and comparability strategy.}
Finally, I used the total disbursement for the activity reported by IATI and determined a rough estimate of the USD per unit outcome, except for Benefit-Cost Ratios, Rates of Return, and agricultural yield outcomes. I found on investigation that most outcomes were approximately log-normally distributed. I took the log10 of all categories except Benefit-Cost ratios, rates of return, and agricultural yields.

\subparagraph{Splitting disbursements}
Unfortunately, I did not have access to outcome-level funding splits from the IATI database. In order to roughly represent the fact that dollar-per-unit spending can be allocated across several outcomes, I wrote a custom algorithm to evenly allocate total activity expenditures to what are usually distinctly funded outcomes. My procedure assigns each activity's total expenditure across the outcome components it reports, so later cost-per-unit calculations do not implicitly treat multi-outcome activities as having multiple full budgets. Components fall into three behaviors: outcomes relating to ``beneficiaries reached'' are always assigned the full budget, benefit/cost ratios and economic and financial rate of return are excluded from monetary allocation , and the rest of the outcomes are eligible for splitting. To avoid double-counting when two indicators are simply alternative measurements of the same underlying result, closely related indicators are first grouped into shared conceptual buckets, such as pairing protected area with area under management, pairing different yield-increase measures, and pairing tree planting with reforested area.

Once the components are bucketed, the algorithm gives each bucket an equal share of the activity’s allocatable funding. Every component inside a bucket inherits that same share, meaning components that are “alternative measures of the same thing” share one slice rather than each taking a slice.

Carbon dioxide reductions are handled as a special case because they can act as a summary metric that overlaps with other mitigation outputs. If CO$_2$ reductions are reported without any closely linked mitigation outputs (such as improved stoves, added generation capacity, or forest-related actions like planting or management), then CO$_2$ reductions receive an equal share like any other bucket. If CO$_2$ reductions are reported alongside any of those linked outputs, it inherits the combined allocation already assigned to the linked outputs present for that activity. This prevents CO$_2$ from inflating allocated spending when it is essentially a derivative or co-reported consequence of the underlying mitigation actions.

\subsection{Baseline Methods}

Three relatively simple baseline methods were attempted, to ensure the relatively complex and expensive LLM-based methods are better than simpler approaches. I choose three simple baseline methods, in order to ensure the predictions were significantly better than the baseline methods for activity success forecasting.

\textbf{Prediction baseline: always predict the most common rating for the reporting organization}
This baseline technique provides a sanity check that more sophisticated methods are worthwhile. Because the prediction task is inherently difficult with much of the variation in outcomes unable to be forecasted at the outset of the activity, this is a relatively strong baseline.
% \subsubsection{Prediction baseline: Random Forest Regression}
% The random forest regression method is a widely applied method to predict unseen data in machine learning. It has the benefit of requiring relatively little compute, performing quite well for a wide range of dataset sizes, and it doesn't tend to overfit as much as neural-network based machine learning models \cite{randomforest}. As random forest cannot use natural language of the prompt, I intend to instead supply the algorithm with fields in the OECD Open Data with the proposed policy and demographic and trend data from the World Bank dataset. The random forest will then predict a value in tonnes CO$_2$.

% However, random forest cannot use the semantics in the natural language prompt, which leads us to our next baseline.

\textbf{Prediction baseline: GLM Trained with non-LLM categories}
In order to justify the addition of non-LLM categories, we use the baseline statistical categories apparent in prior literature and train a General Linear Model (GLM) on the outputs. Features include
\begin{itemize}
 \item planned activity duration
 \item planned total disbursement
 \item whether the activity is primarily loan or grant-based
 \item the one-hot encoded reporting organization % of the top 4 most common organizations in the database (The World Bank (957 activities), BMZ/KFW/GIZ (240 activities), UNDP (257 activities), and the Asian Development Bank (156 activities))
 \item the Country Policy and Institutional Assessment (CPIA) score from the World Bank for that country
 \item the scope of the activity on a scale from 1-7, ranging from local to global
 \item the $log(\text{GDP} / \text{capita})$ of the countries where the activity takes place weighted by the percentage of the activity performed in each country. %todo: complete the list of features and countries
 \item \emph{gemini-2.5-flash}-generated evaluation on a score from 0 to 100 of:
 \begin{itemize}
 \item  how well financed the activity is
 \item  the activity integratedness within the broader activity ecosystem
 \item  the expected implementer performance
 \item  the ease of targeted outcomes
 \item  the degree of contextual challenge
 \item  the overall risk level
 \item  the activity's overall technical complexity.
\end{itemize}
\end{itemize}
The activity start date was not used, as there was no clear linear pattern with regards to overall activity success over time in the training data.

%\textbf{Prediction baseline: Zero-shot LLM}
%In order to ensure the series of improvements on the LLM system in fact genuinely improve accuracy above simply querying the model with some basic activity information and requesting a prediction, we insert the summarized information about the category along all of the evaluation axes where the model could find relevant information. This show that the methods used to improve accuracy are indeed increasing accuracy above simply a single generated prediction by ChatGPT-3.5.

\subsection{Experimental methods}
\subsubsection{Non-Parametric Bootstrap}
The non-parametric bootstrap is a method used to diversify the training data, increasing the diversity in models that are trained many times. It can be used both for ensuring methods robustly improve performance on a diversity of different training setups, and in the case of training the random forest, increases independence between trees. This works by randomly sampling the same number of samples as exist in the training set, with replacement (the same training point may repeat more than once, at random).

\subsubsection{GLM using IATI Features and Grades}

Similar to the baseline prediction, the GLM is trained with ridge regression to reduce overfitting on noise and improve generalization.


\subsubsection{Nearest Neighbor (Vector Similarity)} \label{ssub:nearest_neighbor_vector_similarity_}

I first constructed a similarity test using features including countries of the activity, GDP per capita as described previously, the scope of the activity, and the implementing and funding organization ID.
I found however that this similarity test significantly underperformed compared to the semantic similarity of the \emph{gemini-2.5-flash}-generated summary of the activity documents. 
I first weight the similarity proportional to its embedding semantic similarity score, and tested a cutoff for averaging 1, 3, 7, 10, 15, and 20 nearest neighbors using the Gemini embeddings model \emph{gemini-embedding-001}. I found 15 nearest neighbors was the highest-performing using this method, and thus use the weighted average of the nearest neighbor ratings to predict the overall activity score. Although the nearest neighbor method was used to collect examples for the LLM prompt, it was found that simply taking the weighted mean of ratings underperformed the ``most common rating'' method.

\subsubsection{Random Forest}

The Random Forest method is a statistical algorithm which constructs an ensemble of decision trees which would produce the correct output on the training data, and averages those decision trees. The averaging nature of the random forest algorithm reduces overfitting on the training data. The algorithm is inherently "regularized", penalizing an overly complex decision tree. The decision trees split based on value ranges of the features. By reducing the depth of the trees (the number of decision points where the decision tree splits), we can reduce the memorization of the training data from the trees, and improve generalization of the model. Each decision also only considers a random fraction of the features, encouraging each tree to be more independent of each other and improving generalization further. The bootstrap method is also used to train trees, encouraging tree independences.

\subsubsection{LLM Forecasting Method}

The LLM forecasting method was decided upon by iteratively inspecting both the quality of the response, and the overall accuracy of the predictions made by the LLM. To generate the LLM forecasting methods, \emph{gemini-2.5-flash} was prompted with a series of ``mock forecasts'', generated by \emph{gemini-2.5-pro}. The ``mock forecast'' used relevant pages retrieved by ranking the categorized topics by forecast informativeness %todo: specify exactly how
and retrieving 10 pages of the most relevant activity data and 10 pages of the most relevant evaluation data, prioritizing pages marked as ``deviations from plans'', ``delays or early completion'', or ``over or under spending'' with a minimum forecasting relevance score of 3/10, and otherwise returning the pages with the highest forecasting relevance score.

Figure~\ref{fig:mock-forecast-prompt} shows the prompt template used to generate the retrospective ``mock forecasts'' (using \emph{gemini 2.5-pro}), conditioned on retrieved baseline and outcome document excerpts for the same activity.

To generate each mock forecast, we constructed a retrieval-augmented input consisting of up to 10 baseline pages and up to 10 outcome/evaluation pages per activity. Baseline pages were selected from high-scoring passages in predefined ``forecast-informative'' categories (e.g., objectives, implementation plans, risks, financing details, contextual challenges, and stakeholder/implementer information), using a high relevance threshold (minimum categorization score of 9) and including nearby pages when insufficient high-scoring pages were available. Outcome pages were selected from outcome documents emphasizing deviations from plans (including deviations, delays/early completion, and over/under-spending), using a lower relevance threshold (minimum score of 3) and likewise including surrounding pages to reach the target count when needed. We then merged these retrieved excerpts with activity metadata (title, scope, planned start/end dates, planned financing totals when present) and brief model-generated baseline summaries (activity description and risk summary) before prompting Gemini to write a forecast from the ex-ante perspective. Importantly, the prompt required the model to end by outputting the \emph{known} final evaluation rating for that activity (derived from the merged ratings file and converted into scale-specific text via \texttt{get\_ratings\_text}), while also instructing it to ground the narrative in the retrieved evaluation pages and to return ``NO RESPONSE'' if the evaluation excerpts did not contain sufficient justification for the assigned rating.


The most semantically relevant activities which ended approximately at or before the start of the activity being forecasted was then retrieved (see Section \ref{ssub:nearest_neighbor_vector_similarity_}. In addition, the activity ``risks'' were inserted before each mock forecast, to provide context for the example. Each mock forecast was structured in a way similar to the highest performing scratchpad method from \citep{halawiApproachingHumanLevelForecasting2024}.

A series of features including the activity title, start date, and activity location were injected into the prompt to provide context for the activity, as well as a \emph{gemini-2.5-flash}-generated summary.

% For Method \#2, the most relevant pages of the activity documents were directly converted to text and inserted into the prompt.

Finally, the distribution of rating outcomes was inserted into the prompt, in order to prevent collapse towards only a few ratings.

\subsubsection{Further Details on LLM Forecasting Method}

The full prompt template for the LLM Forecast is shown in Figure~\ref{fig:forecast-prompt-multistage}.

\textbf{Few-Shot Block}
In both methods, I use a $k$-nearest-neighbors (KNN) few-shot block of semantically similar activities in the training data (see Section \ref{ssub:nearest_neighbor_vector_similarity_} for how semantic similarity was determined). I selected a range of nearest neighbors. I asked the language model to extrapolate lessons about rating scales for the most similar ``Highly Unsatisfactory'', ``Unsatisfactory'' or ``Moderately Unsatisfactory'', the most similar ``Moderately Satisfactory'', and the most similar ``Satisfactory'' or ``Highly Satisfactory'' rated examples in the training data. A selection of $k=3$ summarized mock forecasts was found to perform better $k=1$, $5$, or $7$.

Each example activity in the few-shot block included (i) key metadata (title and, where available, location and a brief summary), (ii) a short ``risks'' summary, (iii) the retrospective mock forecast analysis, and (iv) the final evaluation outcome label.

\textbf{Additional Prompts}
Two additional prompts were given, and inserted into the final forecast: (1) reasons the activity may have been evaluated as ``Moderately Satisfactory'' or worse, (2) reasons the activity may have been evaluated as ``Moderately Satisfactory'' or better.

The forecasting prompt required a structured response format that explicitly considered both lower- and higher-outcome arguments on the rating scale and ended with a single-line prediction. Concretely, the model was instructed to: (1) provide reasons the overall success might be rated \texttt{\{midpoint\_low\_text\}} or lower, (2) provide reasons it might be rated \texttt{\{midpoint\_high\_text\}} or higher, (3) aggregate considerations and select exactly one of the \texttt{\{num\_options\}} outcomes, and (4) output the final forecast on the last line beginning with \texttt{FORECAST: } followed by only the chosen option.

Finally, I appended a short description of the empirical distribution of rating outcomes in the training data. This was found to reduce mode-collapse toward a narrow subset of ratings.


% \begin{figure}[htbp]
%   \centering
%   \begin{subfigure}{0.48\textwidth}
%     \begin{tcolorbox}[left=4pt, right=4pt, top=4pt, bottom=4pt]
% \ttfamily\footnotesize
% SYSTEM:\newline
% You are an experienced international aid decision maker with a quantitative mindset.
% Forecast the overall evaluation rating from the options:
% \{options\_text\}.\newline
% USER:\newline
% Forecast what the outcome will be for this activity.\newline
% \#\#\# EXAMPLE ACTIVITIES \#\#\#\newline
% [For each neighbor: title; risks; example forecast;
% rating scale; final evaluation outcome]\newline
% \#\#\# NEW ACTIVITY TO FORECAST \#\#\#\newline
% ACTIVITY ID: \{activity\_id\}\newline
% ACTIVITY TITLE: \{activity\_title\}\newline
% ORIGINAL PLANNED START DATE: \{planned\_start\}\newline
% ORIGINAL PLANNED END DATE: \{planned\_end\}\newline
% ACTIVITY LOCATION(S): \{location\}\newline
% ACTIVITY DESCRIPTION (SUMMARY): \{gemini\_generated\_summary\}\newline
% ACTIVITY RISKS: \{risks\_summary\}\newline
% Provide the following format for your response:\newline
% 1. Provide reasons why the overall success might be rated \{midpoint\_low\_text\} or lower.\newline
% 2. Provide reasons why the overall success might be rated \{midpoint\_high\_text\} or higher.\newline
% 3. Aggregate your considerations, and decide on the final outcome among the \{num\_options\} options.\newline
% 4. Provide the final forecast on the last line beginning with 'FORECAST: '
% followed by only the forecast with no extra words.\newline
% [Append: training-set rating distribution text]\newline
% Respond only in English.
%     \end{tcolorbox}
%     \caption{Method \#1 (summary injection).}
%   \end{subfigure}\hfill
%   \begin{subfigure}{0.48\textwidth}
%     \begin{tcolorbox}[left=4pt, right=4pt, top=4pt, bottom=4pt]
% \ttfamily\footnotesize
% SYSTEM:\newline
% You are an experienced international aid decision maker with a quantitative mindset.
% Forecast the overall evaluation rating from the options:
% \{options\_text\}.\newline
% USER:\newline
% Forecast what the outcome will be for this activity.\newline
% \#\#\# EXAMPLE ACTIVITIES \#\#\#\newline
% [Same few-shot block as Method \#1]\newline
% \#\#\# NEW ACTIVITY TO FORECAST \#\#\#\newline
% ACTIVITY ID: \{activity\_id\}\newline
% ACTIVITY TITLE: \{activity\_title\}\newline
% ORIGINAL PLANNED START DATE: \{planned\_start\}\newline
% ORIGINAL PLANNED END DATE: \{planned\_end\}\newline
% ACTIVITY LOCATION(S): \{location\}\newline
% EXCERPTS FROM BASELINE ACTIVITY DOCUMENTS:\newline
% \{pdf\_to\_text\_excerpts\}\newline
% ACTIVITY RISKS: \{risks\_summary\}\newline
% Provide the following format for your response:\newline
% 1. Provide reasons why the overall success might be rated \{midpoint\_low\_text\}.\newline
% 2. Provide reasons why the overall success might be rated \{midpoint\_high\_text\}.\newline
% 3. Aggregate your considerations, and decide on the final outcome among the \{num\_options\} options.\newline
% 4. Provide the final forecast on the last line beginning with 'FORECAST: '
% followed by only the forecast with no extra words.\newline
% [Append: training-set rating distribution text]\newline
% Respond only in English.
%     \end{tcolorbox}
%     \caption{Method \#2 (raw text injection).}
%   \end{subfigure}
%   \caption{Prompt templates for LLM forecasting Methods \#1 and \#2. The methods share the same scaffold (few-shot examples, metadata, risks, and response-format constraints) and differ only in how activity context is injected (summary vs.\ raw document text).}
%   \label{fig:forecast-prompts}
% \end{figure}




\begin{figure}[htbp]
  \centering
  \begin{tcolorbox}[left=4pt, right=4pt, top=4pt, bottom=4pt, width=0.96\textwidth]
\ttfamily\footnotesize
SYSTEM:\newline
You are an experienced international aid decision maker with a quantitative mindset.
Respond as if you were forecasting at the beginning of the activity what the outcome would be,
ultimately arriving at \{final\_result\_for\_prompt\}, from the options of \{options\_text\}.\newline
USER:\newline
You are generating example forecasts that will be used to fine tune a language model.
Using the uploaded pages from activity documents available for the following activity, respond as if you were forecasting only based on activity documents and original information from the start what the outcome would be.
You will provide a well-reasoned forecast written from the perspective of an international aid evaluator at the beginning of the activity, only at the very end of your response arriving at the correctly forecasted evaluation success rating of '\{final\_result\_for\_prompt\}'.
Your response will be balanced and comprehensive, including consideration of the information from the uploaded activity documents.
Your mock forecast must adhere to the actual reasons for the overall evaluation, as described in the pages from the uploaded evaluation documents.\newline
Provide the following format for your response:\newline
1. Provide reasons why the overall success might be rated \{midpoint\_low\_text\}.\newline
2. Provide reasons why the overall success might be rated \{midpoint\_high\_text\}.\newline
3. Aggregate your considerations, and decide on the final outcome among the \{num\_options\} options (finally arriving at \{final\_result\_for\_prompt\}).\newline
4. Provide the final forecast on the last line beginning with 'FORECAST: ' followed by only the forecast with no extra words.\newline
\newline
The final prediction should not be made until the very end of the mock forecast.
Your mock forecast must reflect the uncertainty which would be inherent given the information at the start of the activity.
If there is insufficient information describing why the '\{final\_result\_for\_prompt\}' evaluation was assigned, respond only with: "NO RESPONSE".
Respond only in English.\newline
ACTIVITY TITLE: \{activity\_title\}\newline
ACTIVITY SCOPE: \{activity\_scope\}\newline
ORIGINAL PLANNED START DATE: \{planned\_start\}\newline
ORIGINAL PLANNED END DATE: \{planned\_end\}\newline
ORIGINAL PLANNED TOTAL DISBURSEMENT: \{disbursement\_total\} \{disbursement\_units\}\newline
ORIGINAL PLANNED TOTAL LOANS AND CREDIT: \{loan\_total\} \{loan\_units\}\newline
ACTIVITY DESCRIPTION FROM START: \{chatgpt\_description\}\newline
ACTIVITY RISKS SUMMARY FROM START: \{risks\_summary\}\newline
[Uploaded context: up to 10 pages of baseline excerpts + up to 10 pages of outcome/evaluation excerpts]
  \end{tcolorbox}
  \caption{Prompt template used to generate retrospective ``mock forecasts'' (Gemini 2.5-pro) from retrieved baseline and outcome/evaluation document pages. Bracketed text indicates injected retrieved excerpts rather than literal prompt text.}
  \label{fig:mock-forecast-prompt}
\end{figure}


% =========================
% FIGURE 1: Multi-stage final forecast prompt (single method)
% =========================
\begin{figure}[htbp]
  \centering
  \begin{tcolorbox}[left=4pt, right=4pt, top=4pt, bottom=4pt, width=0.96\textwidth]
\ttfamily\footnotesize
SYSTEM:\newline
You are an experienced international aid decision maker with a quantitative mindset.
Respond with a comprehensive, thorough forecast of what the overall evaluation rating of the activity will be,
from the options of \{options\_text\}.\newline

USER:\newline
Forecast what the outcome will be for this activity.\newline

\#\#\# Lessons from similar activities \#\#\#\newline
\{knn\_summary\_text\}\newline
% (If no summary is available, this block can instead be the raw few-shot examples:) \newline
% \{few\_shot\_example\_activities\}\newline
\#\#\# End lessons \#\#\#\newline

\#\#\# Additional specific information about the activity that you summarized \#\#\#\newline
\{rag\_synthesis\_additional\_info\}\newline
\#\#\# End of additional information you summarized \#\#\#\newline

ACTIVITY ID: \{activity\_id\}\newline
ACTIVITY TITLE: \{activity\_title\}\newline
ORIGINAL PLANNED START DATE: \{planned\_start\}\newline
ORIGINAL PLANNED END DATE: \{planned\_end\}\newline
ACTIVITY SCOPE: \{activity\_scope\}\newline
PLANNED TOTAL DISBURSEMENT (USD): \{planned\_total\_disbursement\_usd\}\newline
ACTIVITY LOCATION(S): \{locations\}\newline
LOCATION GDP PER CAPITA, USD: \{gdp\_percap\}\newline
PARTICIPATING ORGANIZATIONS: \{reporting\_orgs\}\newline
IMPLEMENTING ORGANIZATION CATEGORY: \{either "Government" or "NGO", otherwise line not inserted\}\newline

ACTIVITY DESCRIPTION: \{chatgpt\_description\}\newline
ACTIVITY TARGETS: \{targets\_summary\}\newline
ACTIVITY CONTEXT: \{activity\_context\}\newline
ACTIVITY COMPLEXITY: \{complexity\_details\}\newline
ACTIVITY INTEGRATEDNESS: \{how\_integrated\_description\}\newline
FINANCING DETAILS: \{finance\_summary\}\newline
IMPLEMENTER PERFORMANCE CONTEXT: \{implementer\_performance\_text\}\newline
ACTIVITY RISKS:\newline
\{risks\_summary\}\newline
ACTIVITY POSSIBILITIES: \{possibilities\_summary\}\newline


% -------------------------
% Final-stage (s3) instruction scaffold (this is the prompt whose structure you show)
% -------------------------
\{training-set rating distribution text\}\newline
Here are a few reasons that you said the answer might be ``Moderately Satisfactory'' or worse:\newline
\{insert\_stage\_s1\_answer\_here\}\newline
Here are a few reasons that you said the answer might be ``Satisfactory'' or better:\newline
\{insert\_stage\_s2\_answer\_here\}\newline

YOUR TASK:\newline
Aggregate your considerations above. Think like a superforecaster (e.g.\ Nate Silver).
On the very last line of your response, write `FORECAST: ' followed by exactly one option from this rating scale with no extra words:\newline
\{options\_text\}\newline

Respond only in English.
  \end{tcolorbox}
  \caption{Single-method multi-stage forecasting prompt. Stages s1 and s2 are run as separate calls, and their outputs are inserted into the final (s3) prompt via \{\texttt{insert\_stage\_s1\_answer\_here}\} and \{\texttt{insert\_stage\_s2\_answer\_here}\}.}
  \label{fig:forecast-prompt-multistage}
\end{figure}


% =========================
% FIGURE 2: RAG pipeline prompts (phrase-gen + synthesis)
% =========================
\begin{figure}[htbp]
  \centering
  \begin{tcolorbox}[left=4pt, right=4pt, top=4pt, bottom=4pt]
\ttfamily\footnotesize
SYSTEM:\newline
You are an experienced international aid decision maker with a quantitative mindset.
Your job is to identify missing-but-important information from activity information documents and produce search phrases to find them in the documents.
Only target facts that would be knowable at the start of the activity; ignore later implementation results.
You format the final lines of your response with exactly 5 phrases, with one line each per phrase:\newline
PHRASE 1: <query>\newline
PHRASE 2: <query>\newline
PHRASE 3: <query>\newline
PHRASE 4: <query>\newline
PHRASE 5: <query>\newline

USER:\newline
First, consider what information is available, and what is generally unavailable but would be useful to know,
in order to forecast the activity outcome on the following scale: \{options\_text\}.\newline
Second, generate five short search phrases to look up in the activity's documents, customized to fill key informational gaps.\newline

EXAMPLE QUERY PHRASES:\newline
[Injected: randomized list of example phrases]\newline

ACTIVITY ID: \{activity\_id\}\newline
ACTIVITY TITLE: \{activity\_title\}\newline
ORIGINAL PLANNED START DATE: \{planned\_start\}\newline
ORIGINAL PLANNED END DATE: \{planned\_end\}\newline
ACTIVITY SCOPE: \{activity\_scope\}\newline
PLANNED TOTAL DISBURSEMENT (USD): \{planned\_total\_disbursement\_usd\}\newline
ACTIVITY LOCATION(S): \{locations\}\newline
LOCATION GDP PER CAPITA, USD: \{gdp\_percap\}\newline
PARTICIPATING ORGANIZATIONS: \{reporting\_orgs\}\newline
IMPLEMENTING ORGANIZATION CATEGORY: \{implementing\_org\_type\}\newline
ACTIVITY DESCRIPTION: \{chatgpt\_description\}\newline
ACTIVITY TARGETS: \{targets\_summary\}\newline
ACTIVITY CONTEXT: \{activity\_context\}\newline
ACTIVITY COMPLEXITY: \{complexity\_details\}\newline
ACTIVITY INTEGRATEDNESS: \{how\_integrated\_description\}\newline
FINANCING DETAILS: \{finance\_summary\}\newline
IMPLEMENTER PERFORMANCE CONTEXT: \{implementer\_performance\_text\}\newline
ACTIVITY RISKS:\newline
\{risks\_summary\}\newline
ACTIVITY POSSIBILITIES: \{possibilities\_summary\}\newline

Provide the following format for your response:\newline
COMPREHENSIVE REASONING ABOUT GOOD PHRASES: <extensive reasoning>\newline
PHRASE 1: ...\newline
PHRASE 2: ...\newline
PHRASE 3: ...\newline
PHRASE 4: ...\newline
PHRASE 5: ...\newline
Respond only in English.
    \end{tcolorbox}
    \caption{RAG phrase generation prompt (produces 5 document search phrases).}
  \label{fig:rag-prompt-1}
\end{figure}


\begin{figure}[htbp]
  \centering
  \begin{tcolorbox}[left=4pt, right=4pt, top=4pt, bottom=4pt]
\ttfamily\footnotesize
SYSTEM:\newline
You are an experienced international aid decision maker with a quantitative mindset.
Provide information related to forecasting the activity outcomes based on the query phrases below using only the evidence excerpts provided. Do not exclude any relevant information.
Only include facts that would be knowable at the start of the activity; ignore later progress or results.\newline

USER:\newline
ACTIVITY ID: \{activity\_id\}\newline

PHRASES:\newline
1. \{phrase\_1\}\newline
2. \{phrase\_2\}\newline
3. \{phrase\_3\}\newline
4. \{phrase\_4\}\newline
5. \{phrase\_5\}\newline

EVIDENCE EXCERPTS:\newline
PHRASE: \{phrase\_1\}\newline
- [\{source\_doc\_a\} | \{doc\_type\}] \{retrieved\_snippet\_a\}\newline
- [\{source\_doc\_b\} | \{doc\_type\}] \{retrieved\_snippet\_b\}\newline
PHRASE: \{phrase\_2\}\newline
- [\{source\_doc\_c\} | \{doc\_type\}] \{retrieved\_snippet\_c\}\newline
\ldots\newline

Respond with relevant information about the activity from the excerpts that would be useful to forecasting the eventual success of the activity, without losing any relevant information or context. Focus on providing information relevant to the 5 phrases above.\newline

Respond only in English.
    \end{tcolorbox}
    \caption{RAG synthesis prompt (summarizes retrieved evidence into forecast-relevant activity facts).}
  \label{fig:rag-prompt-2}
\end{figure}


% =========================
% FIGURE 3: KNN few-shot summarization prompt (summarize\_knn)
% =========================
\begin{figure}[htbp]
  \centering
  \begin{tcolorbox}[left=4pt, right=4pt, top=4pt, bottom=4pt, width=0.96\textwidth]
\ttfamily\footnotesize
SYSTEM:\newline
You are an experienced international aid decision maker with a quantitative mindset.
Provide a balanced and thoughtful assessment of how similar activities were rated. Do not attempt to forecast the current activity outcome.
Only include information that would be knowable at the start of the activity; ignore later implementation results. Under no circumstances reveal actual ex-post information.\newline

USER:\newline
You are extracting and analyzing information from the examples below as applies to the current activity.\newline

\#\#\# EXAMPLE ACTIVITIES \#\#\#\newline
[Injected few-shot block; for each neighbor: title; risks; example forecast; rating scale; final evaluation outcome]\newline
\#\#\# END EXAMPLE ACTIVITIES \#\#\#\newline

ACTIVITY ID: \{activity\_id\}\newline
ACTIVITY TITLE: \{activity\_title\}\newline
ORIGINAL PLANNED START DATE: \{planned\_start\}\newline
ORIGINAL PLANNED END DATE: \{planned\_end\}\newline
ACTIVITY SCOPE: \{activity\_scope\}\newline
PLANNED TOTAL DISBURSEMENT (USD): \{planned\_total\_disbursement\_usd\}\newline
ACTIVITY LOCATION(S): \{locations\}\newline
LOCATION GDP PER CAPITA, USD: \{gdp\_percap\}\newline
PARTICIPATING ORGANIZATIONS: \{reporting\_orgs\}\newline
IMPLEMENTING ORGANIZATION CATEGORY: \{implementing\_org\_type\}\newline

Please respond as follows:\newline
List all the separate reasons that the example forecasts described above went well or badly, given their evaluations, if such considerations could apply to this activity.
Rate the applicability of each consideration.
What are the relevant lessons that can be learned as could apply to forecasting the outcome of this activity?
Describe the key reasons each example was given the rating they were.
Ensure the only information given is that which could be known or reasonably forecasted at the start of the activity.\newline

Respond only in English.
  \end{tcolorbox}
  \caption{Prompt template used to summarize KNN few-shot examples into a compact ``lessons from similar activities'' block (used downstream in the final forecast prompts).}
  \label{fig:knn-summary-prompt}
\end{figure}


% \begin{figure}[htbp]
%   \centering
%   \begin{subfigure}{0.48\textwidth}
%     \begin{tcolorbox}[left=4pt, right=4pt, top=4pt, bottom=4pt]
% \ttfamily\footnotesize
% What is the intervention that is described in the abstract? 
% This is an abstract for an impact evaluation report about an intervention in a LMIC. 
% Do not mention the outcomes or analysis method, only describe the intervention with as much detail as is present in the abstract. 
% Ensure to include any contextual information about what was done and where in your response. 
% If nothing is said about the intervention write: No Intervention Described.
%     \end{tcolorbox}
%     \caption{Intervention extraction prompt.}
%   \end{subfigure}\hfill
%   \begin{subfigure}{0.48\textwidth}
%     \begin{tcolorbox}[left=4pt, right=4pt, top=4pt, bottom=4pt]
% \ttfamily\footnotesize
% What does the abstract say regarding the \[outcome\_category\] outcome? 
% Be sure to include relevant quantitative or categorical information where present. 
% If nothing is said about the outcome write: No Information.
%     \end{tcolorbox}
%     \caption{Outcome extraction prompt.}
%   \end{subfigure}
%   \caption{Prompts used to extract (a) intervention descriptions and (b) outcome statements from abstracts.}
%   \label{fig:prompts-ib}
% \end{figure}

% \subsection{Techniques for Improving Forecasting Skill}
% Forecasting context was restricted to RAG context obtained, the GPT-generated intervention description, and the name of the outcome metric.

% We proceed to discuss how each technique for improving the composite forecasting skill metric was implemented.

% \textbf{Scratchpad}
% More details will come, once I am sure exactly how I plan to implement the methods. %TODO

% \textbf{RAG}
% More details will come, once I am sure exactly how I plan to implement the methods. %TODO

\textbf{Ensembling}
Ultimately, I found that ensembling was too expensive of a method to use for forecasting.
%All methods which demonstrated above-chance skill in forecasting were averaged using the mean value of the forecast. This was found to robustly outperform any individual forecasting method, although with only a modest overall improvement due to high autocorrelation between individual forecasts. Ensembles of LLM-generated forecasts using differing random seeds and differently selected K-Nearest-Neighbors. %(with a different weight on most recent vs most semantically similar). A summary of the 10 pages was also included (method \#3).
% More details will come, once I am sure exactly how I plan to implement the methods. %TODO

% \textbf{Fine-tuning}
% The Llama 70B model was fine-tuned using past paper results and pairings collected before the model cutoff date. In past work, even though data were in the training data, fine-tuning significantly improved prediction performance \citep{wenPredictingEmpiricalAI2025}. \\


% NOTE: MORE DETAILS ON FINE-TUNING TO COME, ONCE ESTABLISH THAT I REALLY HAVE TIME TO DO THIS %TODO

% \textbf{}

\subsection{Scoring Metrics} \label{sub:scoringmetrics}

% We combine Brier score, calibration, and accuracy into a composite forecasting skill metric to attempt to mitigate the various issues in the individual metrics. In general, we only proceed with adding an accuracy boosting feature if it does not worsen any individual metric. We also utilize a held-out test set to ensure the validation metrics remain similarly performant in the final dataset. In event forecasting, scoring Metrics are typically used to quantify forecaster skills. A scoring rule is ``proper'' if the forecaster maximizes the expected score for an observation drawn from the distribution F if they issue the probabilistic forecast F, rather than G $\neq$ F \citep{gneitingStrictlyProperScoring2007}. Brier score and log-loss are both strictly proper, but accuracy is not, limiting its comparability to other domains. However, accuracy has the advantage of intuitive simplicity.

\textbf{Accuracy}
The percent of the time the correct rating is forecasted. Non-integers are rounded to integers.

\textbf{Side Accuracy}
The percent of correctly predicted ``Satisfactory'' or above vs ``Moderately Satisfactory'' or below (above or below 3.5). Approximately 50\% of the training dataset sits above and approximately 50\% sits below this boundary.

\textbf{RMSE (Root Mean Square Error)}
Take the square of the difference between every prediction and the true value, take the mean of all such squared values, then take the square root. Measure of “average” distance. Lower is better. On a scale from 0 to 5, therefore worst possible value is 5, best possible value is zero. This method heavily penalizes predictions that are significantly incorrect.

\textbf{Coefficient of Determination ($R^2$)}
$R^2$: Coefficient of determination. Theoretically equals zero, if we always choose the mean (however using the training set mean results in a lower score on the test set in the baseline measure below). If more than 1 regressors are included, $R^2$ is the square of the coefficient of multiple correlation and can be negative. Measures proportion of the variation in the dependent variable that is predictable from the independent variable. Higher is better. This method generally does not penalize outliers significantly.

\textbf{Adjusted $R^2$}
Adjusted $R^2$ is a version of $R^2$ that accounts for the number of regressors in the model. Unlike plain $R^2$, it penalizes adding predictors that do not meaningfully improve fit, making it more appropriate when comparing models with different numbers of features. It can decrease when irrelevant regressors are included, and it can be negative. Higher is better. While it penalizes extra parameters that may lead to overfitting, adjusted $R^2$ within a training set does not reflect model skill as accurately as out-of-distribution $R^2$.


% \begin{table}[htbp]
% \centering
% \caption{Caution when ranking forecasting systems using single metrics: each evaluation metric has its own issues \citep{palekaPitfallsEvaluatingLanguage2025}.}
% \label{tab:forecasting-metrics-pitfalls}
% \renewcommand{\arraystretch}{1.2}
% \setlength{\tabcolsep}{5pt}
% \begin{tabular}{|p{0.18\textwidth}|p{0.28\textwidth}|p{0.48\textwidth}|}
% \hline
% \textbf{Metric} & \textbf{Method} & \textbf{Pitfalls when using for ranking outcomes} \\
% \hline
% Calibration &
% Consider all questions where the forecaster predicts a probability close to $p$. 
% The forecaster has good calibration if the proportion of questions where the outcome 
% is ``Yes'' is close to $p$. 
% It is usually measured over ``bins'' of questions based on the predicted probability. 
% $\Pr(Y=1 \mid \hat{p}=p) \approx p$. &
% Calibration can penalize useful forecasting. \emph{Card example:} Guessing a card suit and rank in a 52 card deck. A base-rate forecaster that predicts $1/52$ for every card is perfectly calibrated. A more discerning forecaster assigns $10\%$ to five ``front-runners'' and $0.5\%$ to the remaining 47. If their ranking is genuinely informative so that the true card lies among the top five in $60\%$ of rounds, the observed success rates are about $12\%$ in the $10\%$ bin and $0.8\%$ in the $0.5\%$ bin so calibration looks worse. Yet this forecaster is far more useful.\\
% \hline
% Accuracy &
% Share of correct binary classifications after thresholding probabilities 
% (e.g., predict ``Yes'' if $p \ge 0.5$, otherwise ``No''). 
% $\displaystyle \text{Acc} = \tfrac{1}{N}\sum_{i=1}^N [\hat{y}_i = y_i]$. &
% Accuracy rises when there are fewer options, and when the outcome itself occurs more often. Therefore, it should not be used to compare between outcomes with variable statistical distributions. \\
% \hline
% Brier score &
% Mean squared error between predicted probability and outcome, averaged across questions 
% (lower is better). 
% $\displaystyle \text{Br} = \tfrac{1}{N}\sum_{i=1}^N (p_i - y_i)^2$. &
% Averages of Brier score overweight mid-probability discrimination relative to rare-event skill.

% \emph{Example:} Out of 120 ``Good'' or ``Bad'' outcomes, 60 are mid-probability with a 40\% base rate (half 60\% ``Good'' events and half are 20\% ``Good'' events). The remaining 60 are rare, events with a 2\% ``Good'' rate.  
% \begin{minipage}[t]{\linewidth}\begin{itemize}
% \item Forecaster A is perfect on rare events but predicts 0.4 on all mid-prob questions, yielding an average Brier of $0.12$. 
% \item Forecaster B is baseline on rare events (predicts 0.02) but discriminates mid-prob questions (predicts 0.6 for 60\% cases and 0.2 for 20\% cases), yielding an average Brier of $\approx 0.1$. 
% Despite being useless on rare events, B is ranked better overall.
% \end{itemize}\end{minipage}
% \\ \hline
% \end{tabular}
% \end{table}

% % NOTE: CHAT_GPT BASED LOG-LOSS - NEED TO DOUBLE-CHECK %TODO
% \begin{table}[htbp]
% \centering
% \caption{Caution when ranking forecasting systems using single metrics: each evaluation metric has its own issues \citep{palekaPitfallsEvaluatingLanguage2025}.}
% \label{tab:forecasting-metrics-pitfalls}
% \renewcommand{\arraystretch}{1.2}
% \setlength{\tabcolsep}{5pt}
% \begin{tabular}{|p{0.16\textwidth}|p{0.28\textwidth}|p{0.20\textwidth}|p{0.30\textwidth}|}
% \hline
% \textbf{Metric} & \textbf{Method} & \textbf{Equation} & \textbf{Pitfalls when using for comparing forecasting skill} \\
% \hline
% Calibration &
% Consider all questions where the forecaster predicts a probability close to $p$; compare predicted and observed frequencies across bins. &
% $\Pr(Y=1 \mid \hat{p}=p) \approx p.$ &
% Can penalize useful forecasting; depends on binning; see text below. \\
% \hline
% Accuracy &
% Percent of correct classifications after thresholding probabilities (e.g., predict ``Yes'' if $p \ge 0.5$). &
% $\displaystyle \mathrm{Acc}=\frac{1}{N}\sum_{i=1}^N [\hat{y}_i = y_i].$ &
% Rises with class imbalance and fewer options; not comparable across outcomes with different base rates. \\
% \hline
% Brier score &
% Mean squared error between predicted probability and outcome (lower is better). A brier score is strictly proper. More outcome categories will raise the brier score (as the correct outcome is more difficult to predict)&
% $\displaystyle \mathrm{Br}=\frac{1}{N}\sum_{i=1}^N (p_i - y_i)^2.$ &
% Overweights mid-probability discrimination relative to rare-event skill; see text below. \\
% \hline

% Logarithmic score &
% Strictly proper scoring rule for multi-category outcomes (often reported as negative log-loss). &
% $ \ell(\mathbf{p}, y)=\log{p_{y}}$ &
% Extremely sensitive to overconfident errors; undefined at $p_{y}=0$ without clipping; scale depends on log base. \\
% \hline


% % Logarithmic score &
% % Proper log scoring rule (often reported as negative log-loss). &
% % $\displaystyle \ell(p,y)=y\log p + (1-y)\log(1-p).$ &
% % Extremely sensitive to overconfident errors; undefined at $p\in\{0,1\}$ without clipping; scale depends on log base. \\
% \hline


% \end{tabular}
% \end{table}

% Each form of evaluating the quality of forecasts has its own limitations. 

% \paragraph{Calibration issues}
% Calibration can penalize useful forecasting. For example, guessing a card suit and rank in a 52-card deck. A base-rate forecaster that predicts $1/52$ for every card is perfectly calibrated. A more discerning forecaster assigns $10\%$ to five ``front-runners'' and $0.5\%$ to the remaining 47. If their ranking is genuinely informative so that the true card lies among the top five in $60\%$ of rounds, the observed success rates are about $12\%$ in the $10\%$ bin and $0.8\%$ in the $0.5\%$ bin so calibration looks worse, yet this forecaster is far more useful.

% \paragraph{Brier score issues}
% Out of 120 ``Good'' or ``Bad'' outcomes, 60 are mid-probability with a 40\% base rate (half 60\% ``Good'' events and half are 20\% ``Good'' events). The remaining 60 are rare, events with a 2\% ``Good'' rate.  
% \begin{itemize}
% \item Forecaster A is perfect on rare events but predicts 0.4 on all mid-prob questions, yielding an average Brier of $0.12$. 
% \item Forecaster B is baseline on rare events (predicts 0.02) but discriminates mid-prob questions (predicts 0.6 for 60\% cases and 0.2 for 20\% cases), yielding an average Brier of $\approx 0.1$. 

% Despite being useless on rare events, B is ranked better overall.

% \end{itemize}
% Consider 120 ``Good''/``Bad'' outcomes. Sixty are mid-probability with a 40\% base rate (half are 60\% ``Good'' and half are 20\% ``Good''). The remaining 60 are rare events with a 2\% ``Good'' rate.
% \begin{itemize}
% \item Forecaster A is perfect on rare events but predicts $0.4$ on all mid-probability questions $\Rightarrow$ average Brier $=0.12$.
% \item Forecaster B is baseline on rare events (predicts $0.02$) but discriminates mid-probability questions (predicts $0.6$ for the 60\% cases and $0.2$ for the 20\% cases) $\Rightarrow$ average Brier $\approx 0.10$.
% \end{itemize}
% Despite being useless on rare events, B is ranked better overall.

% \paragraph{Logarithmic score issues}
% The logarithmic score \citep{gneitingStrictlyProperScoring2007} is proper but has practical issues:
% \begin{itemize}
% \item \textbf{Extreme penalties:} overconfident mistakes with $p\approx 0$ when $y=1$ (or $p\approx 1$ when $y=0$) dominate the average.
% \item \textbf{Undefined at the boundaries:} $\log 0$ is undefined, so implementations clip $p\in[\epsilon,1-\epsilon]$, and results depend on $\epsilon$.
% \item \textbf{Unit dependence:} the scale changes with the log base, complicating comparisons across papers.
% \item \textbf{Dataset mix sensitivity:} comparisons can be distorted when outcome prevalence differs across evaluation sets.
% \end{itemize}

% \paragraph{Logarithmic score issues}
% \begin{itemize}
% \item \textbf{Extreme penalties:} overconfident mistakes with $p_y\!\approx\!0$ dominate the average.
% \item \textbf{Undefined at the boundaries:} $\log 0$ is undefined, so implementations clip $p_y\in[\epsilon,1-\epsilon]$ and results depend on $\epsilon$.
% \item \textbf{Unit dependence:} the scale changes with the log base, complicating comparisons across papers.
% \item \textbf{Dataset mix sensitivity:} comparisons can be distorted when outcome prevalence differs across evaluation sets.
% \end{itemize}


% \subsection{Outcome Grading}
% Next, each outcome was evaluated in multiple ways.

% First, a grading scheme was identified as a useful taxonomy, which could allow easy comparison between the predictions and the test set:
% \begin{itemize}
% \item [1.] \textbf{Very significant}: Substantial improvement with robust evidence
% \item [2.] \textbf{Significant}: Noticeable improvement with moderate evidence
% \item [3.] \textbf{Neutral/mixed results}: Some improvement but limited or unclear
% \item [4.] \textbf{No effect}: No discernible impact
% \item [5.] \textbf{Outcome was worsened}: Negative impact
% \end{itemize}

% Grading for free-form prediction was also allowed, whereby GPT-o4-mini was used to directly compare a free-form prediction of the outcome, to the outcome described in the abstract. NOTE: FURTHER DETAILS WILL COME UPON IMPLEMENTING THIS METHODOLOGY %TODOf


% For each forecast, a qualitative free-form forecast is generated. Subsequently, a grade on the 5-point scale is also generated.

% \subsection{Grading Forecast Accuracy}

% \ifdev{ \subsection{NOTE: YES THEY REALLY WANT THIS: Predicting the Confidence of a Forecast}
% In addition to forecasting the outcome of an intervention, it is also useful to classify just how confident the forecast is. To do so, we prompt the model to produce a confidence score. OPTIONAL, NOT SURE HOW MANY OF THESE IDEAS TO USE Research on LLM confidence scoring shows that ``consistency'' based approaches are more accurate than eliciting confidence directly from the model \citep{lyuCalibratingLargeLanguage2025}. Agreement-based consistency works best for
% open-source models. Because I already produce $K$ independent forecasts using differing reasoning prompts, the empirical variance across sample predictions provides an agreement-based forecast confidence for Llama 70B. For Llama 70B I also use the probability of the classified output token as a further reported metric where applicable and determine the correlation between the consistency of a prediction and the logit-based token probabilities.
% We use the FSD approach for closed-source models as this approach has been shown to . 

% Finally, I separately fine-tune a small language model to predict forecastability using the training dataset based on the intervention description and the log-loss of the outcome score. I report how correlated all of these metrics are and use a weighted combination to provide a final composite ``forecastability'' prediction.
% }


\subsection{Conformal Prediction}

The outputs of statistical model predictions can be very helpful when a given minimum confidence interval (CI) is required, such as in developmental aid and cooperation interventions affecting the environment. ``Conformal prediction'' is a framework that provides distribution-free coverage guarantees: given a calibration set, predictions will contain the ground truth within a specified probability for future data generated from the same process as the training distribution, regardless of the underlying predictive model \citep{cherianLargeLanguageModel2024}. This is helpful so that users of the system can have high confidence that the forecast is correct. This also provides a significant advantage over human forecasters, who are unable to provide theoretical coverage guarantees with statistical backing.

\subsubsection{Model For Fixed Width Conformal Prediction}

To produce the fixed width conformal prediction, I reserved 10\% of the training set ratings for the calibration, and trained a separate RF model on the remaining 90\% of the train date. The result was the same error bar size for every prediction, theoretically guaranteed to cover 90\% of outcome ratings assuming process exchangeability.

Consider a sequence of observations $\{(x_t, y_t)\}_{t=1}^T$ and a predictive model producing point forecasts $\hat{y}_t$. Define the residuals
\[
e_t = y_t - \hat{y}_t.
\]

In its simplest form, conformal prediction constructs prediction intervals using a fixed-width correction. Let
\[
q_{1-\alpha} = \text{Quantile}_{1-\alpha}\left( |e_1|, \ldots, |e_T| \right).
\]
where $\operatorname{Quantile}_{1-\alpha}(\cdot)$ returns the smallest real number
$q$ such that at least a fraction $1-\alpha$ of the inputs are less than or equal to $q$. Because the process which generates $q$ hasn't changed, for a new input $x_{T+1}$, the conformal prediction interval is given by
\[
\hat{y}_{T+1} \pm q_{1-\alpha}.
\]

This interval achieves marginal coverage at level $1-\alpha$, but its width is constant across time and does not adapt to changing uncertainty in the data.

\subsubsection{Variance-Adaptive Conformal Prediction}

 I initially tried to produce such an error prediction by implementing a Bayesian Additive Regression Trees (BART) model \citep{quirogaBayesianAdditiveRegression2023}, which can be considered a bayesian version of the random forest model, where confidence intervals are added for each prediction. However, BART is relatively high-compute to train, and requires more tuning to reach the performance of a Random Forest model.

\subsubsection{Variance Adaptive Conformal Prediction with a Ridge Regression Error Model}

Because BART is computationally expensive to train and may miss out on essential features for confidence prediction, I also attempted my own CI model.

Therefore, I also tested a more customized approach to the problem domain. I theorized that most of the variance can be captured by the following features already available within the data:
\begin{itemize}
  \item The count of missing features for a record which had to be median-imputed (\emph{n\_missing})
  \item The standard deviation in predictions of the best predictor model (the random forest), with 30 randomly sampled bootstrapped data points from the earliest 70\% of the training dataset  (\emph{bag\_std})
  \item The standard deviation of tree predictions within the RF model (\emph{tree\_std})
  \item The magnitude of difference between the RF and the ridge regression model predictions (\emph{abs\_rf\_minus\_ridge})
  \item The prediction of the random forest itself (\emph{yhat\_rf})
\end{itemize}

I then calibrated this to produce a conformal prediction CI, where 90\% of predictions fall within that interval. In order to generate the variable width, Ridge Regression predictions, The latest 30\% of the training data was set aside. The first 2/3 of this set-aside data (123 points) was used to train the Ridge Regression model. The remaining 1/3 (68 points) was used to calibrate the error prediction model. As with the BART model, rather than a fixed 90\% CI, I report a per-row confidence interval.

In order to train the error model, it was necessary to re-train an RF and Ridge regression model on the first 70\% of the data in train, and use the next 20\% to train the ridge CI prediction.
Assuming that the process to generate the validation set is interchangeable with the process to generate the test set, conformal prediction theory tells us that we can say rigorously that the error bars I have calibrated against the last 10\% of the training set will cover at least 90\% of the true predictions. In other words, it is always <10\% chance that the true rating will fall outside of the 90\% confidence interval I generate. [NOTE: when i train on the held-out set, I will be able to put this theory to an even better test].

\subsubsection{Model For Variance Adaptive Conformal Prediction}

To account for heteroskedasticity, conformal prediction can be combined with model-implied uncertainty estimates. Suppose the predictive model provides an estimate of the absolute error $\hat{\sigma}_t$ for an activity that started at time $t$. I define the normalized residuals as:
\[
z_t = \frac{e_t}{\hat{\sigma}_t}.
\]

I define the fixed halfwidth coefficient $q_{1-\alpha}$ as
\[
q_{1-\alpha} = \text{Quantile}_{1-\alpha}\left( |z_1|, \ldots, |z_T| \right).
\]
Because of the process exchangeability condition between activities starting at time $T$ in the training dataset and at time $T+1$ outside of the dataset, the behavior of $\hat{\sigma}_T$ will be equivalent to $\hat{\sigma}_{T+1}$. The resulting prediction interval for $y_{T+1}$ becomes
\[
\hat{y}_{T+1} \pm q_{1-\alpha}\,\hat{\sigma}_{T+1}.
\]

where $\hat{\sigma}_t$ denotes the model-implied estimate of the absolute error of the rating at time $t$. This construction preserves the distribution-free coverage guarantees of conformal prediction while allowing interval widths to adapt dynamically based on the model’s own uncertainty estimates.


% \subsection{Conformal Prediction of Confidence Intervals} 

% In order to use conformal prediction, a prediction for the error of a forecast is required. I list several such methods below.







% \textbf{Conformal Prediction with LLMs}
% In addition to forecasting the outcome of an intervention, it is also useful to classify just how confident the forecast is. Research on LLM confidence scoring shows that ``consistency'' based approaches are more accurate than verbally eliciting confidence directly from the model \citep{lyuCalibratingLargeLanguage2025}. Agreement-based consistency works best for open-source models and Codex, while entropy works best for GPT-3.5 \citep{lyuCalibratingLargeLanguage2025}. Because I already produce $K$ independent forecasts using differing reasoning prompts, the empirical variance across sample predictions provides an agreement-based forecast confidence. % for Llama 70B. For Llama 70B I also use the probability of the classified output token as a further reported confidence metric where applicable. We use the entropy metric for those closed-source models.

%PROBABLY BETTER:within the training set, and training a separate model to predict the error of a record in the next 15\% of the training set. I then calibrate this error model with the remaining, latest 15\% of the training set to produce a calibration between the output of the error model, and the true error. [NOTE: this will be trained from the validation set, once I report results on the held-out set]. I train the confidence predicting ridge regression model on:

% %TODO: check the math (was GPT-generated based on image)
% \paragraph{Agreement-based.} For a multiset of $K$ answers $a={a\_i}\_{i=1}^{K}$ with most-voted answer $\bar{a}=\mathrm{mode}(a)$,

% $$
% \mathrm{Agree}(a)=\frac{1}{K}\sum_{i=1}^{K}[a_i=\bar{a}].
% $$

% \paragraph{Entropy-based.} Let the unique-answer set be $U(a)={u_j}*{j=1}^{m}$ with empirical frequencies
% $p_j=\frac{1}{K}\sum{[a_i=u_j]}{i=1}^{K}$ and $m=|U(a)|$. Define

% $$
% \mathrm{Ent}(a)=1-\frac{H(p)}{\log{m}}, \qquad H(p)=-\sum{p_j}_{j=1}^{m} \log{p_j}.
% $$

%Finally, I separately fine-tune a small language model to predict forecastability using the training dataset based on the intervention description and the log-loss of the outcome score. I report how correlated all of these metrics are and use a weighted combination to provide a final composite ``forecastability'' prediction.


% \ifdev{
% \subsection{Predicting the Confidence of a Forecast}
% \label{sec:forecast_confidence}

% \begin{enumerate}
%   \item \textbf{Self–reported confidence ($p_{\text{self}}$).} We elicit a numerical probability from the model alongside the forecast using precise instruction (e.g., ``report a single number in [0,1] for your probability that the graded label is correct''). Because self-reports are often miscalibrated or overconfident, especially for LLMs \citep{desaiCalibrationPretrainedTransformers2020, jiangHowCanWe2021, kadavathLanguageModelsKnow2022, lyuCalibratingLargeLanguage2025}, we post-hoc calibrate with temperature scaling or isotonic regression on the validation set \citep{guoOnCalibrationModern2017, zadroznyObtainingCalibratedProbability2002}.
%   \item \textbf{Token-logit confidence ($p_{\text{logit}}$).} For Llama-70B we read the normalized softmax probability of the chosen class token(s) and the \emph{logit margin} between the top two classes; we also compute predictive entropy $H$ over the five outcome grades. These quantities correlate with correctness but require calibration and are sensitive to prompt and label semantics \citep{hendrycksBaselineDetectingMisclassified2017, guoOnCalibrationModern2017, desaiCalibrationPretrainedTransformers2020}.
%   \item \textbf{Ensemble agreement ($p_{\text{ens}}$).} We produce $K$ independent forecasts using diverse reasoning prompts. The empirical variance across samples is an estimator of forecast confidence.
%   \item \textbf{Evidence support ($s_{\text{rag}}$).} From the retrieved context we derive simple, model-agnostic support features: number of distinct sources, agreement between sources (cosine similarity of extracted rationales), and recency relative to the intervention. These are mapped to $[0,1]$ via a logistic model fit on validation (higher when more, newer, and mutually consistent sources support the forecast).
%   \item \textbf{Forecastability score ($f_{\text{diff}}$).} We train a small classifier/regressor on the training set that predicts \emph{difficulty} (expected log-loss or probability of error) from the intervention description and metadata (domain, specificity, geographic scope, pre/post design cues). This follows the selective prediction / risk–coverage literature, enabling principled abstention \citep{geifmanSelectiveClassificationDeep2017, geifmanSelectiveNetDeep2019}.
% \end{enumerate}

% \paragraph{Meta-calibration and the composite score.}
% Each raw signal is first individually calibrated on a validation split: $p_{\text{self}}^{\star}=\phi_{\text{iso}}(p_{\text{self}})$, $p_{\text{logit}}^{\star}=\phi_{\tau}(p_{\text{logit}})$, $p_{\text{ens}}^{\star}=\phi_{\text{iso}}(p_{\text{ens}})$, where $\phi_{\text{iso}}$ is isotonic regression and $\phi_{\tau}$ is temperature scaling \citep{guoOnCalibrationModern2017, zadroznyObtainingCalibratedProbability2002}. We then fit a logistic meta-model on the same validation features (with nested CV to avoid leakage) to produce the composite confidence
% \[
% \hat{c} \;=\; \sigma\!\Big(w_0 + w_1 p_{\text{self}}^{\star} + w_2 p_{\text{logit}}^{\star} + w_3 (1\!-\!\tfrac{H}{\log 5}) + w_4 p_{\text{ens}}^{\star} + w_5 s_{\text{rag}} + w_6 (1\!-\!f_{\text{diff}})\Big).
% \]
% This stacked calibration makes the score interpretable as an empirical probability of correctness under proper scoring rules \citep{gneitingStrictlyProperScoring2007}.

% \paragraph{Selective prediction and coverage guarantees.}
% To support risk-aware use, we add a conformal prediction layer to provide finite-sample coverage guarantees \citep{angelopoulosGentleIntroductionConformal2023}. Using nonconformity scores derived from $1-\hat{c}$ on a calibration set, we compute a threshold $q_{\alpha}$ such that the \emph{set-valued} prediction has marginal coverage $1-\alpha$. In practice, we expose (i) the scalar $\hat{c}$, (ii) a binary \emph{abstain} decision when $1-\hat{c}>q_{\alpha}$, and (iii) an optional two-grade prediction set when needed.

% \paragraph{Llama-70B specifics.}
% For grade classification we map labels to single, unambiguous tokens (e.g., \texttt{[VSIG]}, \texttt{[SIG]}, \texttt{[MIX]}, \texttt{[NOE]}, \texttt{[NEG]}) to stabilize logit-based confidence; we report the top-1 softmax, margin, and entropy. Because token probabilities can spuriously rise on memorized or near-duplicate texts, we diagnose leakage by correlating $p_{\text{logit}}$ and margin with (a) zlib compressibility of the intervention text, (b) perplexity ratios against a de-duplicated corpus, and (c) publication recency; spikes indicate possible memorization \citep{carliniExtractingTrainingData2021, palekaPitfallsEvaluatingLanguage2025}. We report these diagnostics alongside $\hat{c}$.

% \paragraph{Evaluation.}
% We evaluate confidence quality with: (i) \emph{negative log-likelihood} (proper), (ii) \emph{Brier score} and its reliability/resolution decomposition \citep{murphyDecompositionBrierScore1973}, (iii) \emph{Expected Calibration Error} (ECE) and reliability diagrams \citep{guoOnCalibrationModern2017}, and (iv) \emph{risk–coverage} curves for selective prediction \citep{geifmanSelectiveClassificationDeep2017}. We compare (a) uncalibrated self-reports, (b) token-logit only, (c) ensemble agreement only, and (d) the stacked composite. In our ablations, stacking consistently improves NLL and Brier and lowers ECE relative to any single signal, while conformalization achieves target coverage within statistical error.

% \paragraph{Summary of what we report for each forecast.}
% \begin{itemize}
%   \item The predicted grade and a free-form rationale (as in the main method).
%   \item $\hat{c}$ (calibrated probability of correctness) with a 68/90\% credible interval from a Beta–Binomial model over ensemble votes.
%   \item An \emph{abstain} flag at user-chosen risk level $\alpha$ (default $\alpha=0.1$) with conformal coverage.
%   \item Diagnostics: calibrated $p_{\text{self}}^{\star}$, $p_{\text{logit}}^{\star}$, ensemble variance, evidence support $s_{\text{rag}}$, difficulty $f_{\text{diff}}$, and leakage indicators.
% \end{itemize}

% \paragraph{Notes on limitations.}
% LLM self-reports can anchor on prompt wording; token probabilities depend on label lexicalization; ensemble variance may conflate epistemic and prompt-induced variability; and RAG support features are proxy signals. These are mitigated (not eliminated) by post-hoc calibration, label normalization, prompt diversity, and validation on held-out interventions \citep{palekaPitfallsEvaluatingLanguage2025, desaiCalibrationPretrainedTransformers2020}. Finally, confidence is not causality: high $\hat{c}$ indicates predictive reliability, not ground-truth policy effectiveness.

% }

\clearpage
\section{Results \& Discussion}
I provide a summary of the data made available from this work, provide an analysis of the success of the various methods at forecasting evaluation ratings, and provide preliminary results on the ability for the model to forecast specific outcomes.

\subsection{Database of Evaluations}
\label{sub:database_of_evaluations}
In addition to producing a useful LLM forecasting system, this work has also produced a large collection of intervention outcomes in developmental aid and cooperation interventions affecting the environment. Given the absence of academic publications investigating the IATI dataset specifically regarding evaluations, this thesis provides flexible, powerful tools to gain insights from the IATI dataset. This database of is shared publicly on zenodo at \url{https://zenodo.org/records/XXYYZZ}. [NOTE: I will release once held-out set is analyzed.] %TODO: publish abstracts

\subsection{Predicting Overall Ratings}
Overall, the forecasting system I produce is capable of predicting outcomes significantly above chance on out-of-distribution activities. Compared to prior work \citep{ashtonPuzzleMissingPieces2023}, I report a  value consistent with an adjusted $R^2$ with training set (I report an adjusted $R^2$ of 0.450 for within-training set correlations on primarily world bank ratings, while others report at maximum an adjusted $R^2$ of ~0.3 \citep{goldembergMindingGapAid2025}. I was not able to identify comparable out-of-distribution, time-ordered split analysis in the literature. As expected for prediction under distribution shift, my results on the out-of-time validation set were considerably weaker, with an $R^2$ of 0.17 for the random forest model, and an $R^2$ of 0.19 when incorporating the language model forecasting results correction + recency model.

\subsubsection{Overfitting Corrections}
$R^2$ was chosen as the ``Adjusted $R^2$'' has been used in similar work to evaluate model performance in the development aid literature and penalize overfitting by reducing the reported $R^2$ as a function of the number of input parameters. Mirroring similar reported methods in the literature, I calculated adjusted $R^2$ on the training points. While this is sensitive to overfitting, it is a common practice in the development aid literature. However, I find adjusted $R^2$ within the training set is highly sensitive to the specific parameters of the RF model and the subsequent degree of overfitting, such that adjusted $R^2$ increases to above 0.6 with default random forest parameters, while performance on the validation set drops (See Table \ref{tab:method_comparison_validation}. I conclude that adjusted $R^2$ should not be used as a measure of forecast skill.

Relative to the default \texttt{RandomForestRegressor} configuration (e.g., \texttt{n\_estimators}=100, \texttt{max\_depth}=\texttt{None}, \texttt{min\_samples\_split}=2, \texttt{min\_samples\_leaf}=1, \texttt{max\_features}=1.0, \texttt{bootstrap}=\texttt{True}, \texttt{ccp\_alpha}=0.0), the specification used here deliberately constrains model capacity in ways that typically reduce overfitting. Trees are explicitly depth-limited (\texttt{max\_depth}=14 rather than unbounded) and splits are only permitted when nodes contain substantially more data (\texttt{min\_samples\_split}=20 and \texttt{min\_samples\_leaf}=10), which smooths predictions by limiting fine-grained partitioning of the feature space. In addition, using a smaller feature subset at each split (\texttt{max\_features}=0.488) increases tree diversity and reduces variance relative to the default that considers all features. The model also uses row subsampling (\texttt{max\_samples}=0.86), further reducing variance by injecting additional randomness into each tree's training set. Increasing the number of trees (\texttt{n\_estimators}=638) primarily improves stability via variance reduction rather than increasing effective complexity, while the pruning parameter (\texttt{ccp\_alpha}=$1.26\times 10^{-6}$) remains close to the default of no pruning. Overall, compared to defaults, these choices trade some bias for a meaningful reduction in variance, making the fitted ensemble less susceptible to inflated in-sample fit and the attendant drop in validation performance.

\subsubsection{Language Model Features}

The language model derived features modestly aided prediction accuracy, in aggregate providing an improvement of about 4\% additional explanation of the variance of outcomes out of the 9\% discoverable by the RF model (See Table \ref{tab:method_comparison_validation}). The  finance, integratedness, implementer\_performance, targets, context, risks, and complexity features were directly inserted as grades from the model. 

\subsubsection{Embedding features}
One LLM feature I add is the embedding of the LLM-generated ``targets'' field as a semantic representation of what each activity is trying to accomplish. This follows \citep{goldembergMindingGapAid2025} in extracting the World Bank Project Development Objectives (PDO), as I find they are very similar to the LLM-generated outputs. I also attempted PDO extraction using regex methods, but found the results were noisy on matching, especially on non-world-bank projects, and did not improve prediction performance as much as embeddings on the llm-generated targets. I first normalize the LLM-extracted targets text into a stable canonical form (removing formatting artifacts, unescaping, splitting on separators, dropping “NO RESPONSE” tails, and deduplicating near-identical chunks). I then embed the cleaned targets text for each activity with the \emph{gemini-embedding-001} model, yielding a single high-dimensional vector per activity that summarizes activity objectives (targets) in a continuous latent space.

I then replicate \citep{goldembergMindingGapAid2025} and compress target embeddings using a two-stage dimensionality reduction pipeline. First, I apply PCA to reduce the embedding vectors to 50 dimensions and fit UMAP on the PCA outputs to produce 2D and 3D coordinate maps. I find the 3D embeddings (umap\_x, umap\_y, umap\_z) perform better on prediction than 2D. This preserves enough local topology that activities with similar targets remain near each other in the compressed space. I also find qualitatively, that sectors with similar 2D vectors cluster around the activity environmental category, as in \citep{goldembergMindingGapAid2025}. While  \citep{goldembergMindingGapAid2025} find significant signal in deviations from average embeddings for countries or sectors, I do not find these theorized degree of ``contextualization'' features aid forecasting skill when I add them to my model.
% =======================
% PREAMBLE (add once)
% =======================
\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{4}

% LaTeX has no \subsubsubsection by default
\newcommand{\subsubsubsection}[1]{\paragraph{#1}\mbox{}\\}


% =======================
% BODY (paste in thesis.tex)
% =======================


\subsubsubsection{Recency and LLM Adjustment Ridge Regression}
\label{subsubsub:recency_llm_adjustment}

I wanted to both correct the random forest model for temporal distribution shift (e.g., changing reporting practices, evaluation standards, portfolio composition, and macro conditions), and incorporate any usable information from the direct LLM forecasts. Even when the input features are stable, the conditional relationship $p(y \mid x)$ can drift, so a model trained on older activities can become mis-calibrated on newer ones. Furthermore, I found the LLM forecast predictions were significantly correlated with prediction error in the validation set.

\paragraph{Residual-correction formulation.}
Let $\hat{y}^{\text{RF}}_i$ be the random forest prediction for activity $i$, and let $\hat{y}^{\text{LF}}_i$ denote the LLM Forecast. I define the random-forest residual on the i'th activity as:
\[
r_i := y_i - \hat{y}^{\text{RF}}_i .
\]
I then fit a ridge regression model to predict residuals from a small feature vector consisting of the RF prediction and (optionally) the LLM Forecast:
\[
\hat{r}_i := \beta_0 + \beta_1 \hat{y}^{\text{RF}}_i + \beta_2 \hat{y}^{\text{LF}}_i ,
\]
with an $\ell_2$ penalty on $(\beta_1,\beta_2)$ controlled by \texttt{alpha} (ridge strength). The corrected prediction is:
\[
\hat{y}^{\text{corr}}_i := \operatorname{clip}_{[0,5]}\!\left(\hat{y}^{\text{RF}}_i + \lambda \hat{r}_i\right),
\]
where $\lambda$ is a scaling factor (set to 1.0 in my experiments) and clipping enforces the valid rating range between 0 and 5.

This is a simple stacked model: the RF provides the base signal, and ridge learns a low-capacity adjustment to remove systematic residual structure that appears in the recent/LLM-covered slice.

I tested two separate methods:
\begin{enumerate}
\item Recency correction (RF re-calibration on recent activities).
In this variant I remove the LLM forecast entirely fixing $\beta_2=0$, but still calculate an offset $\beta_0$ and scaling $\beta_1$ on the 150 latest training examples.

\item LLM-informed correction (recency + LLM Forecast).
In this variant, the ridge model uses both the RF prediction and the LLM Forecast as covariates on the activities where $\hat{y}^{\text{LF}}_i$ is available. This allows the correction layer to learn a mapping from $(\hat{y}^{\text{RF}}_i, \hat{y}^{\text{LF}}_i)$ to the residual $r_i$, effectively learning when the LLM Forecast contains signal about systematic RF error on the recent slice. The correction is applied only to activities where $\hat{y}^{\text{LF}}_i$ exists; otherwise, predictions fall back to the uncorrected RF output.
\end{enumerate}



% requires: \usepackage{booktabs}
\begin{table}[t]
\centering
\small
\setlength{\tabcolsep}{6pt}
\renewcommand{\arraystretch}{1.15}

\caption{Validation performance in predicting ratings across forecasting methods. Rows are sorted by ascending $R^2$ (higher is better). RMSE and Brier are lower-is-better; Side Accuracy is higher-is-better. Bold indicates the best value in each metric column. Variation between side-accuracy methods (``Moderately Satisfactory'' or lower vs ``Satisfactory'' or lower) were not statistically significant. Side metrics use a binary split at rating $\ge 3.5$ vs $<3.5$ . The ``recency correction'' variants combine models using the 150 latest-starting activities in the training set for calibration/combination. The ``no LLM features'' Random Forest excludes the following features: finance, integratedness, implementer\_performance, targets, context, risks, complexity, umap3\_x, umap3\_y, umap3\_z.}
\label{tab:method_comparison_validation}

\begin{tabular}{lrrrrr}
\toprule
Method & $R^2$ $\uparrow$ & RMSE $\downarrow$ & MAE $\downarrow$ & Side Acc. $\uparrow$ & Acc. $\uparrow$ \\
\midrule
Mode of reporting-org score baseline & 0.102 & 0.897 & \textbf{0.579} & 0.700 & 0.530 \\
Random Forest only & 0.173 & 0.861 & 0.651 & 0.703 & 0.513 \\
Random Forest, no LLM features & 0.143 & 0.876 & 0.679 & 0.677 & 0.490 \\
Ridge GLM & 0.148 & 0.873 & 0.654 & 0.683 & 0.513 \\
Random Forest (default params) & 0.157 & 0.869 & 0.653 & 0.693 & 0.497 \\
Ridge GLM + Random Forest (mean) & 0.173 & 0.861 & 0.644 & 0.710 & \textbf{0.537} \\
RF + recency correction & 0.183 & 0.855 & 0.629 & \textbf{0.733} & 0.523 \\
RF + LLM Forecast + recency correction & \textbf{0.193} & \textbf{0.850} & 0.623 & 0.730 & 0.527 \\
% Method & $R^2$ $\uparrow$ & RMSE $\downarrow$ & Brier $\downarrow$ & Side Accuracy 
% $\uparrow$ \\
% \midrule
% Mode of reporting-org score baseline & 0.102 & 0.897 & 0.30 & 0.70 \\
% Random forest, no LLM features & 0.142 & 0.878 & 0.33 & 0.67 \\
% Ridge GLM & 0.148 & 0.873 & 0.30 & 0.70 \\
% Random Forest (default params) & 0.158 & 0.869 & 0.32 & 0.68 \\
% Ridge GLM + Random Forest (mean) & 0.170 & 0.862 & 0.30 & 0.70 \\
% Random Forest only & 0.171 & 0.862 & 0.30 & 0.70 \\
% RF + recency correction & 0.179 & 0.858 & 0.30 & 0.70 \\
% RF + LLM Forecast + recency correction & \textbf{0.190} & \textbf{0.853} & 0.30 & 0.70 \\
% \bottomrule
\end{tabular}

\end{table}
\begin{table}[t]
\centering
\footnotesize
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.15}

\begin{threeparttable}
\caption{Random-forest drop-one feature importance, sorted by impact. Each row reports the decrease in validation $R^2$ when the feature is removed ($\Delta R^2$\tnote{*}), along with the share of training rows that were median-imputed for that feature (\%\ missing\tnote{**}) and the training-set mean/SD (in the model feature space). The largest impacts come from planned budget and planned duration; governance indicators and semantic/LLM-derived summaries add moderate incremental signal; \texttt{rep\_org\_*} are one-hot reporting-organization indicators.}
\label{tab:rf_dropone_feature_importance}

\begin{tabularx}{\linewidth}{@{}l r r r r@{}}
\toprule
Feature & $\Delta R^2$\tnote{*} & \% missing\tnote{**} & Mean (train) & SD (train) \\
\midrule
planned\_expenditure                  & 0.0940 & 38.18 & 18.026 & 1.273 \\
planned\_duration                     & 0.0840 & 0.15  & 8.806  & 3.371 \\
finance\_is\_loan                     & 0.0320 & 4.26  & 0.635  & 0.482 \\
rep\_org\_2                           & 0.0310 & 0.00  & 0.648  & 0.478 \\
wgi\_regulatory\_quality\_est         & 0.0290 & 6.90  & -0.401 & 0.488 \\
finance                              & 0.0290 & 2.50  & 81.634 & 9.268 \\
targets                              & 0.0280 & 0.15  & 66.625 & 13.442 \\
rep\_org\_0                           & 0.0270 & 0.00  & 0.051  & 0.221 \\
umap3\_y                              & 0.0260 & 9.84  & -1.615 & 1.023 \\
wgi\_government\_effectiveness\_est   & 0.0230 & 6.90  & -0.386 & 0.473 \\
wgi\_rule\_of\_law\_est               & 0.0230 & 6.90  & -0.521 & 0.492 \\
rep\_org\_1                           & 0.0230 & 0.00  & 0.059  & 0.235 \\
gdp\_percap                           & 0.0230 & 7.34  & 8.265  & 0.996 \\
umap3\_x                              & 0.0220 & 9.84  & 0.460  & 2.776 \\
wgi\_control\_of\_corruption\_est     & 0.0220 & 6.90  & -0.550 & 0.467 \\
wgi\_political\_stability\_est        & 0.0210 & 6.90  & -0.570 & 0.789 \\
activity\_scope                       & 0.0200 & 0.00  & 3.185  & 1.397 \\
risks                                & 0.0190 & 0.59  & 64.439 & 19.752 \\
context                              & 0.0190 & 0.15  & 74.029 & 16.487 \\
cpia\_score                           & 0.0170 & 48.75 & 3.499  & 0.382 \\
complexity                           & 0.0160 & 0.15  & 61.912 & 17.406 \\
umap3\_z                              & 0.0160 & 9.84  & 1.599  & 1.869 \\
implementer\_performance              & 0.0150 & 0.29  & 82.044 & 11.749 \\
integratedness                        & 0.0090 & 1.32  & 83.266 & 5.568 \\
\bottomrule
\end{tabularx}

\begin{tablenotes}\footnotesize
\item[*] $\Delta R^2$ is computed as $(R^2_{\text{full}} - R^2_{\text{dropped}})$ on the same validation split; larger values indicate greater marginal contribution under this ablation test.
\item[**] Percent of training rows with missing values for the feature, filled via median imputation (as used in the model pipeline).
\end{tablenotes}
\end{threeparttable}
\end{table}

% % Figure: predicted vs observed absolute error (scatter)
% \begin{figure}[h]
%   \centering
%   \includegraphics[width=0.9\textwidth]{assets/splits_by_start_year.png}
%   \caption{The activities included for predicting ratings, with the splits by count year. Incomplete activities, shown in red, were not used for prediction.}
%   \label{fig:splits}


% Figure: predicted vs observed absolute error (scatter)
\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\textwidth]{assets/shap_rf_ratings.png}
  \caption{A SHAP analysis of ratings on the validation set from the RF. Red indicates an increase in the value of the feature, while blue indicates a below-average value. Points to the right of zero shift ratings up, points to the left of zero shift ratings down. }
  \label{fig:shapratings}
\end{figure}



% Figure: observed vs predicted ratings with 90% PI bars
\begin{figure}[h]
  \centering
  \includegraphics[width=1.0\textwidth]{assets/results_plot_ci_bars.png}
  \caption{Observed versus predicted overall ratings (Random Forest) with calibrated 90\% prediction interval (PI) bars ($R^2$ 0.17, RMSE 0.862). The dashed diagonal indicates perfect agreement between observed and predicted ratings, and the green line shows the fitted trend. Random jitter on this plot's X axis was introduced to blue points to improve readability for confidence intervals. The orange points represent the few ratings which did not land on an integer between 0 and 5. See Section \ref{sub:conformal_prediction_results} for how the calibrated error bars were computed (Ridge Regression Error Model).}
  \label{fig:val-rf-ci-bars}
\end{figure}



\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\textwidth]{assets/llm_corrected_rf_val.png}
  \caption{The scatter of observed vs predicted points on the held-out set for the LLM-corrected RF prediction. $R^2$ of 0.19, RMSE of 0.853.   }
  \label{fig:scatterllmcorrected}
\end{figure}


\FloatBarrier

\subsection{Predicting Outcomes}
In general, outcome predictions were weaker than ratings. Initially, I thought outcomes would be more predictable than ratings. A similar work found an adjusted $R^2 >0.7$ for outcome ratings including in predicting beneficiaries reached \citep{goldembergMindingGapAid2025}, but this was including actual rather than planned durations, actual rather than planned financial disbursements, and several features including breakdowns of per-sector funding for activities and manager performance ratings from AidData that I did not include in my dataset. Furthermore the paper did not predict on cost-effectiveness, making outcome prediction a much easier task when given overall program spending, and lastly I find adjusted $R^2$ for within distribution is highly sensitive to model overfitting. As mentioned in previous sections, AidData was laboriously double-coded data entry, which introduces less error than the LLM and regex data extraction techniques I used. Furthermore, the outcomes with high detected correlation measured a lagged 5-year country-level indicator, while my data were extracted directly from the outcomes.

Several factors contributed to the difficulty of predicting specific outcomes from extracted IATI data:
\begin{itemize}
  \item Unclear apportioning of funding towards each outcome
  \item Inconsistent measurement styles and definitions of terms like Benefit-Cost Ratio, which effects would be included in Economic Rate of Returns.
  \item Incorrect aggregation of multiple ratings within the documents
\end{itemize}

\paragraph{Outcome model training and evaluation.}
For each outcome distribution in Table~\ref{tab:outcome_model_performance}, I trained a random-forest regression model that is identical (same features, preprocessing, and hyperparameters) to the model used for predicting ratings, but with the rating target replaced by the relevant activity-level outcome. A dummy variable for which outcome was also included. The ratings were included as a feature to aid learning about activity success. Models were trained using activity IDs in the training split with non-missing outcomes and evaluated on the validation activities. The counts reported in Table~\ref{tab:outcome_model_performance} correspond to the number of activities available in the validation split for each outcome. Outcome distributions with fewer than 10 activities in either the training or validation split were excluded.

In addition, a single aggregate Z-score was calculated, which subtracts the mean value of each outcome (including ratings) and divides by the standard deviation in the training set. For each activity, the mean value of the z-scores was taken for all dependent variables, including the rating. Due to its high predictability, I theorize that the z-score is a stronger indicator of activity success than activity rating alone, due to the prevalence of ``gaming'' activity ratings \citep{goldembergMindingGapAid2025}.

In keeping with the results of \citep{goldembergMindingGapAid2025}, I do not find a strong correlation between activity ratings and z-scored outcomes. 

% \begin{table}[t]
% \centering
% \caption{Predictive performance of random-forest outcome models on the validation set. For cost-effectiveness outcomes, the target is \emph{USD per unit outcome} (total disbursement divided by the activity-level outcome) and is modeled in log space; for the remaining outcomes (benefit--cost ratios, rates of return, and agricultural yield outcomes) the targets are modeled on their original (unlogged) scale. Reported errors (RMSE/MAE) are in the same scale as the modeled target; $N_{\mathrm{val}}$ is the number of activities in the validation split used for each outcome.}
% \label{tab:outcome_model_performance}
% \begin{tabular}{lllrccc}
% \toprule
% Outcome (readable) & Units & Target transform & $R^2$ & RMSE & MAE & $N_{\mathrm{val}}$ \\
% \midrule
% Beneficiaries (cost per beneficiary) & people & $\log(\mathrm{USD/person})$ & 0.057 & 0.962 & 0.670 & 102 \\
% CO$_2$/CO$_2$e emissions reductions (cost-effectiveness) & tonnes CO$_2$e & $\log(\mathrm{USD/tCO_2e})$ & -0.227 & 1.566 & 1.171 & 27 \\
% Generation capacity (cost-effectiveness) & MW & $\log(\mathrm{USD/MW})$ & -0.083 & 1.016 & 0.866 & 30 \\
% Water and sanitation connections (cost-effectiveness) & connections & $\log(\mathrm{USD/connection})$ & 0.099 & 0.688 & 0.480 & 29 \\
% Benefit--cost ratio & ratio & none & -0.044 & 0.385 & 0.270 & 18 \\
% Economic rate of return & percent & none & -0.225 & 24.189 & 17.549 & 128 \\
% Financial rate of return & percent & none & -0.752 & 19.973 & 16.068 & 49 \\
% Agricultural yield increase (percent) & percent & none & -0.319 & 37.686 & 29.880 & 10 \\
% \bottomrule
% \end{tabular}
% \end{table}
% preamble (once)


% table
\begin{table}
\caption{Updated predictive performance of random-forest outcome models on the validation set. For cost-effectiveness outcomes, the target is USD per unit outcome and is modeled in log space; for the remaining outcomes the targets are modeled on their original scale. Outcomes are sorted by $R^2$ (descending).}
\label{tab:outcome_model_performance}

\footnotesize
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.15}

\begin{tabularx}{\linewidth}{@{}>{\raggedright\arraybackslash}X >{\raggedright\arraybackslash}p{1.7cm} >{\raggedright\arraybackslash}p{3.0cm} r r r >{\raggedleft\arraybackslash}p{2.9cm}@{}}

\toprule
Outcome & Units & Target transform & $R^2$ & RMSE & MAE & $N_{\mathrm{val}}/N_{\mathrm{train}}/N_{\mathrm{heldout}}$ \\
\midrule
All selected outcomes (z-aggregate activity mean) & z & none (z-score) & 0.22 & 0.72 & 0.53 & 326 / 737 / 248 \\
Rating & rating & none & 0.15 & 0.88 & 0.67 & 324 / 732 / 244 \\
Water and sanitation connections (cost-effectiveness) & connections & $\log(\mathrm{USD/connection})$ & 0.11 & 0.69 & 0.49 & 30 / 84 / 10 \\
Beneficiaries (cost per beneficiary) & people & $\log(\mathrm{USD/person})$ & 0.05 & 0.97 & 0.67 & 102 / 220 / 59 \\
Economic rate of return & percent & none & 0.0084 & 22.0 & 15.0 & 128 / 379 / 65 \\
Agricultural yield increase (percent) & percent & none & -0.0053 & 33.0 & 26.0 & 10 / 33 / 9 \\
Benefit--cost ratio & ratio & none & -0.045 & 0.38 & 0.27 & 18 / 49 / 16 \\
Generation capacity (cost-effectiveness) & MW & $\log(\mathrm{USD/MW})$ & -0.074 & 1.0 & 0.83 & 30 / 76 / 16 \\
CO$_2$/CO$_2$e emissions reductions (cost-effectiveness) & tonnes CO$_2$e & $\log(\mathrm{USD/tCO_2e})$ & -0.14 & 1.5 & 1.2 & 27 / 47 / 13 \\
Financial rate of return & percent & none & -0.28 & 17.0 & 14.0 & 49 / 170 / 21 \\
\bottomrule
\end{tabularx}
\end{table}

% \begin{table}[t]
% \centering
% \small
% \setlength{\tabcolsep}{4pt}
% \caption{Predictive performance of random-forest outcome models on the validation set. For quantity outcomes with wide dynamic range (beneficiaries, CO$_2$/CO$_2$e reductions, generation capacity, trees planted, and water/sanitation connections), targets are modeled in log space; for benefit--cost ratios and rates of return, targets are modeled on their original (unlogged) scale. Reported errors (RMSE/MAE) are in the same scale as the modeled target; $N_{\mathrm{val}}$ is the number of activities in the validation split used for each outcome.}
% \label{tab:outcome_model_performance}

% \begin{tabularx}{\linewidth}{@{}>{\raggedright\arraybackslash}X l l r r r r@{}}
% \toprule
% Outcome (readable) & Units & Target transform & $R^2$ & RMSE & MAE & $N_{\mathrm{val}}$ \\
% \midrule
% Benefit--cost ratio & ratio & none & -0.044 & 0.385 & 0.270 & 18 \\
% Economic rate of return & percent & none & -0.225 & 24.189 & 17.549 & 128 \\
% Financial rate of return & percent & none & -0.752 & 19.973 & 16.068 & 49 \\
% \bottomrule
% \end{tabularx}
% \end{table}


I do find that failing to divide by the total disbursement as marked in iati increases predictability - the z-score aggregate correlation moves from 0.27 to 0.23 when we move from predicting the raw outcome to the cost-effectiveness (dollars per unit outcome). When dividing by each activity's disbursement for those that are marked as dollar-per-unit in Table \ref{tab:outcome_model_performance}, and looking at all outcomes except ratings, the correlation on z-scores drops to -0.01. However, including the ratings in the training (but not evaluating performance on predicting ratings) leads to an $R^2$ of 0.05 (n$_{val}$=163), which indicates training on ratings modestly improves performance in predicting outcomes, despite the lack of a clear linear relationship between z-scored ratings and z-scored outcomes.




% Figure: predicted vs observed absolute error (scatter)
\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\textwidth]{assets/zagg_prediction_scatter.png}
  \caption{Validation: predicted versus observed absolute error. Each point corresponds to the mean of the random forest z-score prediction for ratings and all other outcomes listed in table \ref{tab:outcome_model_performance}, for activities in the validation set where at least one rating or outcome were predicted. The dashed line indicates perfect calibration ($y=x$).}
  \label{fig:val-abs-err-scatter}
\end{figure}


% Figure: predicted vs observed absolute error (scatter)
\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\textwidth]{assets/rating_outcome_corr_all.png}
  \caption{All activity's z-scored ratings plotted vs z-scored outcomes. There is a very low positive correlation (Pearson correlation of 0.044). The outliers near "8" averages a 433 percent Economic rate of return and a benefit-cost ratio of 1.6 for the "Jakarta Urgent Flood Mitigation Project", an unusually economically effective project, that should regardless not be removed simply for being an outlier. The project's efficiency was substantial thanks to significant loan savings. However, due to ``the lack of clarity in the reported data, the limited application of FMIS findings, and the unfulfilled activities added at restructuring'', the Overall Outcome rating for this outlier project to ``Moderately Satisfactory''.  See \url{https://datastore.iatistandard.org/activity/44000-P111034}.}
\end{figure}
\label{fig:val-abs-err-scatter}


Overall, little can be concluded from individual outcome correlations. For more detailed work, expert coding is likely required for robust extraction of outcomes, and funding breakdowns for projects should be used to more accurately evaluate cost-effectiveness, rather than the course assumption that all funding for a project goes to all outcomes. For example, for  ``beneficiaries reached'', such results could be systematically different for different definitions of ``Beneficiaries'' - compare for example, indirect vs direct beneficiaries which were not disambiguated. New water or sanitation piping connections is a more clearly comparable outcome, although there may be systematic differences in the costs of sanitation connections and water supply that are not disambiguated by the model.

Despite these limitations, it appears that even a coarse coding of directly comparable activity outcomes is likely to provide a better sense of overall activity sense than evaluator ratings alone. I theorize the key reason is that while outcomes are difficult to code and reliably extract compared to ratings, ratings are more subjective, and prone to systematic differences between evaluators and reporting organizations. Given that these two errors are relatively uncorrelated, the meaned z-scored ratings+outcome aggregate indicator may be a better measure of activity success than any other available or derived metric from the IATI database.

\FloatBarrier
\subsection{Conformal Prediction Results} \label{sub:conformal_prediction_results}
\subsubsection{Bayesian Additive Regression Trees}
I found that the BART model was not only significantly worse at explaining the variance, and with a higher RMSE than the RF model, its confidence predictions were badly miscalibrated. A single model like BART appears to unable to properly capture the uncertainty of the training data under distribution shift. Furthermore, the BART model was found to be two orders of magnitude slower to train than the significantly higher performing RF model (as measured by $R^2$ and RMSE performance training on the full train set and checking against validation).

\subsubsection{Ridge Regression Error Model Parameter Influence}
I find relatively even influence of the different error terms in my CI prediction model (see Table \ref{tab:abs-influence}). This explains why the BART was doing so badly at confidence interval prediction: it only had access to \emph{tree\_std}, and was therefore was missing 75\% of the information about the source of confidence.


% preamble:
% \usepackage{booktabs}
\begin{table}[htbp]
\centering
\caption{Absolute influence on 90\% CI per $1\sigma$ shift in the input variable.}
\label{tab:abs-influence}
\begin{tabular}{@{}l r@{}}
\toprule
Input variable & Absolute influence \\ 
\midrule
\emph{tree\_std}              & 1.4 \\
\emph{abs\_rf\_minus\_ridge}  & 1.1 \\
\emph{bag\_std}               & 1.1 \\
\emph{yhat\_rf}               & 1.0 \\
\emph{n\_missing}             & 1.0 \\
\bottomrule
\end{tabular}
\end{table}





\subsubsection{Ridge Regression Error Model}

The Ridge Regression model does show statistically significant predictive power on the validation set (Spearman $\rho$ = 0.189, p = 0.001) and can distinguish uncertainty at a coarse level: high-uncertainty tercile predictions have 35\% higher mean absolute errors than low-uncertainty predictions. The distance between its prediction and the error of a prediction on validation is 0.406 on average, compared to ``always predict the mean error'', which has a distance of 0.432 from the true error, representing a small 6.2\% improvement. This suggests the approach has potential but requires more training data to compete with simpler baselines. One explanation is the 70\% of the train data was is insufficient to train a model better than picking the most common per-org rating, and thus the models used for the variable width CI error model were insufficiently similar to the RF model to reproduce its error modes [NOTE: we will see how this changes when running on the held-out set!]

Figure~\ref{fig:val-abs-err-scatter} shows how the Ridge Regression confidence prediction model's expected absolute error aligns with absolute error on the validation set, while Figure~\ref{fig:val-abs-err-calibration} aggregates this relationship into quantile bins (with bin counts annotated). 


% Figure: predicted vs observed absolute error (scatter)
\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\textwidth]{assets/scatter_confidence.png}
  \caption{Validation: predicted versus observed absolute error. Each point corresponds to a record; the x-axis is the meta-model prediction of $\lvert y - \hat{y}_{\mathrm{rf}}\rvert$ (expected absolute error) and the y-axis is the realized $\lvert y - \hat{y}_{\mathrm{rf}}\rvert$. The dashed line indicates perfect calibration ($y=x$).}
  \label{fig:val-abs-err-scatter}
\end{figure}

% Figure: binned calibration curve
\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\textwidth]{assets/confidence_buckets.png}
  \caption{Validation: binned calibration of predicted versus observed absolute error. Points are bin means (with bin counts annotated). The dashed line indicates perfect calibration ($y=x$).}
  \label{fig:val-abs-err-calibration}
\end{figure}


\subsubsection{Fixed Width}

Overall, the simple 90\% fixed width performs better than either the BART or the Ridge Regression error model. It achieves closer to 90\% coverage of the ratings in the validation set (92\% fixed width vs 95\% from the variable width model) on the validation set while using significantly less width (2.3 rating points fixed vs a mean of 3.7 for the variable width), indicating the variable-width prediction ability is too weak to enable superior CI intervals. Notably, setting the fixed width to the same mean 3.7 width as the variable model also achieves better coverage (97\%) than the variable width model.

Figure~\ref{fig:val-rf-ci-bars} shows the resulting fixed width 90\% prediction intervals (PI) around the random forest point predictions on the validation set.




% \subsection{Strengths and Weaknesses of This Forecasting System}
% To be analyzed once results are available. %TODO
% \subsection{Evaluation of Techniques for Improving Forecast Accuracy} \label{sub:strengths_and_weaknesses_of_this_forecasting_system}
% In training a statistical model side-by-side with the LLM system, it became apparent that the statistical model was much easier to calibrate and use to investigate the importance of input features. At the same time, LLM forecasting allowed clear chains of reasoning about forecasts and in improving them, can guide human forecasters about the most appropriate lines of reasoning to pursue in making aid decisions. LLM forecasts inherently include and summarize relevant information and thus are more useful as a qualitative guide, summarizing lessons learned from similar activities and weighting their importance.

% \subsubsection{Selecting and validating GLM variables}
% I use $\ell_2$ (ridge) penalties to shrink coefficients. To assess selection stability, I perform a nonparametric bootstrap resampling of the training set (500 bootstrap replicates and compare models with and without each feature, and compare Ridge Regression, the Random Forest model, and between the custom Random Forest parameters and the baseline parameters.

%over the full set of 500 training sets enabled by the boostrap.



% @article{zouHastie2005elasticnet,
%   title   = {Regularization and Variable Selection via the Elastic Net},
%   author  = {Zou, Hui and Hastie, Trevor},
%   journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
%   volume  = {67},
%   number  = {2},
%   pages   = {301--320},
%   year    = {2005},
%   doi     = {10.1111/j.1467-9868.2005.00503.x}
% }

% @book{efronTibshirani1994bootstrap,
%   title     = {An Introduction to the Bootstrap},
%   author    = {Efron, Bradley and Tibshirani, Robert J.},
%   publisher = {Chapman and Hall/CRC},
%   year      = {1994},
%   doi       = {10.1201/9780429246593}
% }

% @book{good2000permutation,
%   title     = {Permutation Tests: A Practical Guide to Resampling Methods for Testing Hypotheses},
%   author    = {Good, Phillip I.},
%   publisher = {Springer},
%   year      = {2000},
%   doi       = {10.1007/978-1-4757-3235-1}
% }


%To be analyzed once results are available. %TODO
% \subsection{The Risk of Trusting This Forecasting System}
% \begin{itemize}
%   \item Published abstracts bias towards overoptimism TODO: FIND THIS PAPER FROM
%   \item Ensuring outcome has not been memorized in training \citep{xLargeLanguageModels2025}
%   \item Ensuring outcome has not been memorized in training \citep{hewittPredictingResultsSocial} does not see any correlation
%   \begin{itemize}
%     \item Z-lib score
%     \item Pearson Correlation between publication date and accuracy
%   \end{itemize}
%   \item Testing if result is already implied in the intervention wordings or tone
%   \item Many other pitfalls in evaluation are possible \citep{palekaPitfallsEvaluatingLanguage2025}

% \end{itemize}
% \subsection{The Risk of Trusting This Forecasting System}
% \label{subsec:risk_trusting_system}

% Even if a forecasting system performs well on the test set, several well‐known biases and failure modes can inflate apparent skill. First, there are many subtle pitfalls in evaluating forecasting systems discussed in Section \ref{sub:limitations}, which can inflate their expected abilities. Second, published abstracts themselves are not neutral evidence: they often overemphasize the significance of effect sizes, which can systematically bias both human and model judgments when training or evaluating on abstract‐level text \citep{duyxStrongFocusPositive2019}.  A large study of findings in economics determine an effect size overestimation factor due solely to publication bias of 1.62 in Medicine, Environmental 1.78, Psychology 1.39, and Economics 2.16 \citep{fFootprintPublicationSelection2024}. % TODO: Search for more findings
% In general, including unpublished working papers, while perhaps reducing rigor, may allow for significantly reduced effect size inflation especially in the field of economics.

% Some researchers claim that most published research findings are false. In our case flexibility in designing reported outcomes and analytical modes increase the chances that the study was ``gamed'' to report unrepresentative significance on that particular metric \citep{ioannidisWhyMostPublished2005}. Cases where large financial payouts are required for a significant result are also more likely to lead to false findings \citep{ioannidisWhyMostPublished2005}. The targeted selection of RCTs in our work increases the chance that the discovered outcomes are true, but many uses of RCTs are insufficient - especially if underpowered \citep{ioannidisWhyMostPublished2005}. We should accordingly more heavily weight outcomes from RCTs with higher effect sizes and large sample sizes. Ideally, outcomes are also from pre-registered studies that commit to the research and analysis methododolgies before reporting the results.

% Furthermore, people often ascribe objectivity to algorithmic outputs and therefore overweight automated advice leading to ``automation bias'' \citep{AlgorithmAppreciationPeople2019}

% \clearpage
% \section{Conclusion \& Outlook (NOTE: CURRENTLY LOW PRIORITY)} %TODO (finalize this)
% \label{sec:conclusion_outlook}
% \subsection{The State of AI and LLMs}
% \begin{itemize}
%   \item Timelines for AI surpassing human ability in forecasting \citep{AI4ResearchSurveyArtificial}\citep{leeAdvancingEventForecasting2025}
%   \item Scaling up language models improves few‑shot and task‑agnostic performance \citep{brownLanguageModelsAre2020}
%   \item AI safety and regulation
% \end{itemize}
% \subsection{Extensions of This Work}
% \begin{itemize}
%   \item Applications in health, policy, law, economics, advancing future scientific progress
%   \item strategic warning applications \citep{knackStateAIStrategic}
%   \item Applications to improve personal and organizational decision making
%   \item Futarchy \citep{arelDesigningArtificialWisdom2024} \citep{lizkaSummaryTakeawaysHansons2021} \citep{hansonShallWeVote2013}
%   \item Applications to reduce gridlock and polarization in the political domain
% \end{itemize}
% \subsection{Ways that the Current Forecasting Technique Could Be Improved}
% Comparing differing reasoning trajectories allows the use of reinforcement learning techniques to further improve upon AI forecasting, without additional externally derived training data \citep{turtelLLMsCanTeach2025}. This allows much smaller models to best larger model reasoning capabilities.


% \subsection{The Promise and Capabilities of AI Forecasting}

% As a clear disclaimer: \textbf{LLMs are not in general superior to humans at forecasting as of May 2025}. At the same time, their forecasting ability for short-term predictions is closing in at a rapid pace as AI capabilities have advanced [source]. Furthermore, predictions with a significant number of relevant news articles or very near to the date of a forecasting resolution can best teams of trained forecaster's aggregate predictions in prediction accuracy \citep{halawiApproachingHumanLevelForecasting2024}. It is currently unknown to what extent the ability of AI systems to forecast geopolitical and economic events can be extended to forecasting the impact of interventions with implications in the Earth system sciences. Exploring this domain opens a promising avenue to improve the efficacy of interventions in the Earth system sciences. In the remaining section, we discuss the beneficial aspects of the system developed, as well as the potential dangers or risks this system may pose.

% One co-benefit of a system fine-tuned on Earth system sciences is that by its cross-domain nature, the LLM will be able to identify a wide range of likely outcomes, and the degree of effect of those outcomes, on a wide range of quantitative and qualitative outcomes. When implementing interventions, researchers, policy-makers, and decision makers must always consider many relevant outcomes of their interventions. The similarity and vector search of the system allow users to quickly identify relevant documentation as well as outcomes of similar scientific research most relevant to their proposed intervention.

% Another benefit of the system is that AIs typically excel in domains where human experts are particularly challenged: when there is a very large range of relevant data or when predictions about the effect of an intervention involve carefully calibrated probabilities. AIs can also perform predictions in a way that human experts can learn from: introducing one piece of information can be used to quantify the effect on AI forecasts. AI forecasts can be ensembled arbitrary and at relatively little expense compared to humans. 

% \subsection{Risks, Biases, and Limitations}
% However, there are clear risks of using AI for evaluating the likely outcomes of interventions in the Earth system sciences. The most obvious issue may be that while AI can be accurate in some domains, current AI systems do not accurately present their confidence in their answers and can completely hallucinate events and facts which have no grounding in reality. The result is a misleading analysis, which in the space of Earth system sciences may lead to significant risks. Policy makers may trust AI more than is justified by its performance, or view it as an unbiased source, despite nearly all current AI systems having a well-documented political bias acknowledged by both the political left and political right [source]. 

% Another risk is that scientists may not perform research deemed to be unlikely to succeed, and thus the range of explored outcomes may be narrowed to the outcomes known to work in the past or deemed to be likely to be successful by the AI system. 

% While AI may be able to calibrate itself on many different domains and automatically pull in relevant information, it currently lacks the ability to reliably perform complex mathematical calculations or run long-term analysis. Furthermore, as AI becomes more advanced there is significant concern in the technology community that it may form its own goals and intrinsic values, out of alignment with its human operators. An AI that advises on AI policy may in fact present a conflict of interest, even if the AI is simply using heuristics mimicking human tendencies towards self-preservation and in-group preferences. 

% Finally, without the full text, there is a risk that the policy forecasting aspect may be quite limited. Without a sense of the scope of an intervention, which would not reliably be indicated in the abstract, the degree of impact of an intervention may difficult to ascertain by any forecasting system.

% \subsection{System Design and Risk Mitigation}
% We address these concerns by noting that as AI begins to become more accurate and lower cost than human researchers at forecasting the impact of policy outcomes, it becomes ever more important to have specifically designed systems that take steps to reduce the dangers of AI systems. We believe the system developed clearly fulfills this criterion. The system we use in this work specifically provides credible, peer-reviewed scientific information and news from reputable sources to the AI, rather than relying on general internet search as many current AI providers rely on.  Furthermore design our system to be calibrated via fine-tuning, meaning that some of the reliability concerns may be ameliorated. As AI systems advance, there appears to be a progression towards more agentic systems with more clear intermediate goals. A misalignment with human preferences (an example in this work might be downplaying the CO$_2$ effects of building more AI systems in order to increase the number of AI systems as an in-group preference) may occur and be missed by humans with extremely long thought chains and insufficient detection of misalignment. Our system by contrast allows the user to inspect the series of logical deductions performed by the model and view available sources the model used as scientific reference material. The system has been specifically quantified in terms of its bias, allowing users to have full knowledge of the likely failure modes when using the system, often absent in generally available AI chat interfaces. With an explicit attempt to correct these biases via fine-tuning, sycophantic behavior is also reduced compared to RLHF models.
% Another risk is that papers tend to have a bias, and the model will learn to replicate that bias. Papers are much more likely to have "significant" results than mixed effect or no effect. The optimistic bias towards positive bias published in journals should mean we interpret the prediction of the model cautiously, with knowledge that it will likely present a more optimistic version of the outcomes than is justified from a neutral observer's perspective. In order to counteract this risk, we are also looking at the accuracy of the quantitative result of the intervention, which is more valid to compare between abstracts and has a relatively smaller publisher bias [source]. 
% Finally, much of the promise of the AI forecasting approach relies on models continuing to become lower cost and more performant in general domains. While multiple empirical trends and the longstanding success of Moore's law clearly indicate this should continue, it is by no means guaranteed. If AI models cease to improve on relevant metrics, or otherwise become increasingly biased or unreliable, much of the promise of an AI forecasting tool for estimating interventions in the Earth system sciences goes away. Despite this risk, the system remains useful and informative for the scientific and public policy community as it provides a system with sources proven to provide useful information for the evaluation of policy outcomes, and introduces a framework by which the impact of interventions can be broken down for more accurate predictions. While there is a possibility that AIs may never reach the capabilities of humans in integrating the disparate sources of information, automated information search and a new tool that can synthesize relevant information can be a powerful tool for scientists and policy makers.
% Forecasting has the distinct benefit of disallowing training on any particular benchmarks and is a rather difficult-to-game metric compared to standard LLM performance benchmarks. In real-world forecasting, the true answers are genuinely unknown at the time of prediction unlike in other benchmark tasks where answers could be memorized from the training data \citep{schoeneggerLargeLanguageModel2023}. It will be increasingly useful to society to understand what the true capabilities of LLMs are and the rate of their improvement, both for the regulation of dangerous AI capabilities and the improved understanding where AI may be capable enough for reliable use in various critical domains such as automated medicine and driverless vehicles.

% \subsection{Broader Applications and Vision}
% The codebase and research done here can be repurposed from specifically Earth system science, to other domains where impact forecasts are clearly useful. A similar system with an expanded set of abstracts and data could be used with relatively little modification in domains such as public health, financial policy, and in a more general way to provide predictions for scientists about likely qualitative and quantitative outcomes of their scientific studies. The success of the model demonstrates that a great deal of opportunity to synthesize scientific findings and improve decision making on an institutional level is policy.
% One particularly promising avenue for expansion of the system would be as an application to Futarchy first proposed by Robin Hanson. Futarchy proposes to use prediction markets to allow policy makers or the general public to only have to agree on what they value and quantify as utility, not on how to maximize that utility. Several prediction markets in parallel are formed, creating a zero-sum game financially rewarding players that best predict the utility outcome conditional on a policy being implemented. To the extent that complex public policy can ever be reduced to a single utility function, that this function can be agreed on by a quorum of policy makers, Futarchy could significantly reduce gridlock and polarization in politics, at least in the domains in which the necessary conditions are useful and possible. In essence, Futarchy aids policy makers in coming to agreement on how to implement policies by reducing the scope of disagreement to what the set of possible policy implementations could be and how they would choose to quantify a successful outcome.
% If and when the system proposed is shown to exceed human ability in predicting policy, or if it can be shown that the system can be complementary to human predictions, cheaply improving their accuracy, this system could be integrated to a scheme for futarchy by replacing or augmenting prediction markets. This may be especially helpful in use-cases where AI succeeds and prediction markets fail: very low probabilities over long time periods (as the winners may choose to invest their money on a higher-return investments), predictions about long-run outcomes that are difficult to gain information about, particularly contentious outcomes, or issues where markets may be biased by particularly wealthy individuals who come in very late in the market and buy many more shares than expected.

% \subsection{AI Scientist Idea}
% Extending the system for searching for high-impact policies is possible, rather than simply using the fine-tuned model for forecasting. While use of reasoning models outside of the domain in which they are trained for often reduces their performance, it still may be possible to re-train the model for these use cases. For instance, the model could be prompted to generate many policy options for a given country to reduce CO$_2$ emissions, and each idea could have the emissions reduction forecasted. Seeding the model with many similar policies and suggesting that it think of a wide range of options may allow for consideration of a wide range of policy options. Next, only the ideas which are forecasted to have high emissions would be suggested to the user of the system. Such a system would be similar to the ``AI Scientist'' released by Google which iteratively generates new hypotheses and reasons over the hypotheses to discover better scientific theories behind biological phenomena \citep{luAIScientistFully2024}.

% \subsection{Avoiding Disempowerment}
% One additional risk to an AI forecasting system is the gradual disempowerment of humans. Many in the futurist community point to the real and currently destructive ``laziness'' engendered by a system that does all of the work for the human. For example, an aid grant maker may choose to simply accept all of the suggestions and ultimate forecast of an evaluation system, rather than critically examining the context and making the decisions independently. Even if an AI system could be more effective at producing a final verdict on the success of a system, human moral judgements should always be involved in the morally fraught work of international aid decision making. For example, the question of justice is an intrinsically human decision - if a country is undergoing a war, this may reduce the likelihood of success of a program, yet we must not simply aim for project success where the benefits to the people in the partner country would be enormous, if successful.

% For this reason, the final probability of success was not provided to the user in the final system. Instead, all of the most relevant resources and statistics were exposed, and both relevant literature sources, academic literature, and references to wikipedia are exposed to the user, as well as various questions posed to the system and its method of breaking down the forecasting problem.

% \subsection{Ideation: Extensions and Other Applications}
% \begin{itemize}
%     \item Improving prospects of futarchy to improve governance
%     \item Understanding how different sources of information contribute to effective forecasting of impact
%     \item Before the forecasting at all: collecting the information for forecasting all in one place, both resources to make reasonable forecasts, as well as creating structure out of unstructured papers in Earth systems science
%     \item Creating general hierarchies of impact for different categories of interventions
%     \item Ability to create "unbiased" forecasts that are both evidence based and listened to by both sides of the political spectrum
%     \item Increasing democratic understanding of the likely effects of laws from third party sources: allows non-experts to assess the efficacy of elected officials in accomplishing their goals
%     \item Automated scoring of introduced legislation 
%     \item Sufficient statistics to introduce confidence bars on the effects of political outcomes
%     \item Leveraging the advance of AI for good
%     \item Constraining the use of AI in a scientifically valid, constrained manner, which minimizes the risk that AI biases themselves influence policy decisions.  
%     \item Automated feedback on proposed interventions (registered studies): what are the likely things this has impact on? What are some relevant papers for their proposal?
% \end{itemize}






























\clearpage
\section*{Works Cited}
\printbibliography

% \addcontentsline{toc}{section}{Works Cited}
% Add your references or switch to biblatex later.

\clearpage
\section*{Erklärung zur akademischen Integrität / Declaration of Academic Integrity}
\addcontentsline{toc}{section}{Declaration of Academic Integrity}
Hiermit versichere ich an Eides statt, dass ich die vorliegende Arbeit selbstständig und nur mit den angegebenen Quellen und Hilfsmitteln (z. B. Nachschlagewerke oder Internet) angefertigt habe. Alle Stellen der Arbeit, die ich aus diesen Quellen und Hilfsmitteln dem Wortlaut oder dem Sinne nach entnommen habe, sind kenntlich gemacht und im Literaturverzeichnis aufgeführt. Weiterhin versichere ich, dass weder ich noch andere diese Arbeit weder in der vorliegenden noch in einer mehr oder weniger abgewandelten Form als Leistungsnachweise in einer anderen Veranstaltung bereits verwendet haben oder noch verwenden werden. Die Arbeit wurde noch nicht veröffentlicht oder einer anderen Prüfungsbehörde vorgelegt. / \emph{I hereby certify under penalty of law that I have prepared this thesis independently and only using the cited sources and resources (e.g., reference works or the internet). All passages of the thesis that I have taken from these sources and resources, either verbatim or in spirit, are cited and listed in the bibliography. Furthermore, I certify that neither I nor anyone else has used or will use this thesis, either in its present form or in a more or less modified form, as evidence in another course. This thesis has not yet been published or submitted to another examining authority.} \\
\\
Potsdam, \DateDDMonthYYYY

\end{document}