@misc{3ieDevelopmentEvidence,
  title = {3ie {{Development Evidence Portal}} {\textbar} 3ie},
  urldate = {2025-08-18},
  howpublished = {https://developmentevidence.3ieimpact.org/},
  keywords = {development},
  file = {/home/dmrivers/Zotero/storage/PLBQ5662/3ie Development Evidence Portal  3ie.pdf}
}

@article{abolghasemiHumansVsLarge2025,
  title = {Humans vs. Large Language Models: {{Judgmental}} Forecasting in an Era of Advanced {{AI}}},
  shorttitle = {Humans vs. Large Language Models},
  author = {Abolghasemi, Mahdi and Ganbold, Odkhishig and Rotaru, Kristian},
  year = {2025},
  month = apr,
  journal = {International Journal of Forecasting},
  volume = {41},
  number = {2},
  pages = {631--648},
  issn = {0169-2070},
  doi = {10.1016/j.ijforecast.2024.07.003},
  urldate = {2025-08-19},
  abstract = {This study investigates the forecasting accuracy of human experts versus large language models (LLMs) in the retail sector, particularly during standard and promotional sales periods. Utilizing a controlled experimental setup with 123 human forecasters and five LLMs---namely, ChatGPT-4, ChatGPT3.5, Bard, Bing, and Llama2---we evaluated forecasting precision through the absolute percentage error. Our analysis centered on the effect of the following factors on forecasters' performance: the supporting statistical model (baseline and advanced), whether the product was on promotion, and the nature of external impact. The findings indicate that LLMs do not consistently outperform humans in forecasting accuracy and that advanced statistical forecasting models do not uniformly enhance the performance of either human forecasters or LLMs. Both human and LLM forecasters exhibited increased forecasting errors, particularly during promotional periods. Our findings call for careful consideration when integrating LLMs into practical forecasting processes.},
  keywords = {AI forecasting attempts},
  note = {``Schoenegger and Park (2023) and Abolghasemi et al. (2023) evaluated GPT-4 and other LLMs on forecasting tournaments and found that they underperform the human crowd. This observation is in line with ours in Section 3.3. Unlike us, they make little efforts to improve these LMs on forecasting.'' (Halawi et al., 2024, p. 3)},
  file = {/home/dmrivers/Zotero/storage/73AI798K/Abolghasemi et al. - 2025 - Humans vs. large language models Judgmental forecasting in an era of advanced AI.pdf;/home/dmrivers/Zotero/storage/8Y5EPB6W/S0169207024000700.html}
}

@misc{AI4ResearchSurveyArtificial,
  title = {{{AI4Research}}: {{A Survey}} of {{Artificial Intelligence}} for {{Scientific Research}}},
  urldate = {2025-08-19},
  howpublished = {https://arxiv.org/html/2507.01903v1}
}

@article{AlgorithmAppreciationPeople2019,
  title = {Algorithm Appreciation: {{People}} Prefer Algorithmic to Human Judgment},
  shorttitle = {Algorithm Appreciation},
  year = {2019},
  month = mar,
  journal = {Organizational Behavior and Human Decision Processes},
  volume = {151},
  pages = {90--103},
  publisher = {Academic Press},
  issn = {0749-5978},
  doi = {10.1016/j.obhdp.2018.12.005},
  urldate = {2025-08-31},
  abstract = {Even though computational algorithms often outperform human judgment, received wisdom suggests that people may be skeptical of relying on them (Dawes,{\dots}},
  langid = {american},
  file = {/home/dmrivers/Zotero/storage/CI5TQDTU/S0749597818303388.html}
}

@article{arelDesigningArtificialWisdom2024,
  title = {Designing {{Artificial Wisdom}}: {{Decision Forecasting AI}} \& {{Futarchy}}},
  shorttitle = {Designing {{Artificial Wisdom}}},
  author = {Arel, Jordan},
  year = {2024},
  month = jul,
  urldate = {2025-08-20},
  abstract = {Introduction In this post I will describe one possible design for Artificial Wisdom (AW.) This post can easily be read as a stand-alone piece, howeve{\dots}},
  langid = {english},
  keywords = {future work; outlook},
  note = {Ultimately, AI will probably be dominating political decisions in the coming decades, and we have the choice of whether that happens in opaque ways, or formalizing such an outcome in a system such as AI-Futarchy as you propose.},
  file = {/home/dmrivers/Zotero/storage/MZUAT7IW/designing-artificial-wisdom-decision-forecasting-ai-and.html}
}

@article{atanasovDistillingWisdomCrowds2017,
  title = {Distilling the {{Wisdom}} of {{Crowds}}: {{Prediction Markets}} vs. {{Prediction Polls}}},
  shorttitle = {Distilling the {{Wisdom}} of {{Crowds}}},
  author = {Atanasov, Pavel and Rescober, Phillip and Stone, Eric and Swift, Samuel A. and {Servan-Schreiber}, Emile and Tetlock, Philip and Ungar, Lyle and Mellers, Barbara},
  year = {2017},
  month = mar,
  journal = {Management Science},
  volume = {63},
  number = {3},
  pages = {691--706},
  publisher = {INFORMS},
  issn = {0025-1909},
  doi = {10.1287/mnsc.2015.2374},
  urldate = {2025-08-31},
  abstract = {We report the results of the first large-scale, long-term, experimental test between two crowdsourcing methods: prediction markets and prediction polls. More than 2,400 participants made forecasts on 261 events over two seasons of a geopolitical prediction tournament. Forecasters were randomly assigned to either prediction markets (continuous double auction markets) in which they were ranked based on earnings, or prediction polls in which they submitted probability judgments, independently or in teams, and were ranked based on Brier scores. In both seasons of the tournament, prices from the prediction market were more accurate than the simple mean of forecasts from prediction polls. However, team prediction polls outperformed prediction markets when forecasts were statistically aggregated using temporal decay, differential weighting based on past performance, and recalibration. The biggest advantage of prediction polls was at the beginning of long-duration questions. Results suggest that prediction polls with proper scoring feedback, collaboration features, and statistical aggregation are an attractive alternative to prediction markets for distilling the wisdom of crowds. This paper was accepted by Uri Gneezy, behavioral economics.},
  keywords = {belief elicitation,crowdsourcing,forecasting,prediction markets}
}

@inproceedings{bangMeasuringPoliticalBias2024,
  title = {Measuring {{Political Bias}} in {{Large Language Models}}: {{What Is Said}} and {{How It Is Said}}},
  shorttitle = {Measuring {{Political Bias}} in {{Large Language Models}}},
  booktitle = {Proceedings of the 62nd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Bang, Yejin and Chen, Delong and Lee, Nayeon and Fung, Pascale},
  year = {2024},
  month = aug,
  pages = {11142--11159},
  doi = {10.18653/v1/2024.acl-long.600},
  urldate = {2025-08-31},
  abstract = {Yejin Bang, Delong Chen, Nayeon Lee, Pascale Fung. Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2024.},
  langid = {american},
  file = {/home/dmrivers/Zotero/storage/9NVNA9HH/Bang et al. - 2024 - Measuring Political Bias in Large Language Models What Is Said and How It Is Said.pdf}
}

@article{binaLargeLanguageModels2025,
  title = {On {{Large Language Models}} as {{Data Sources}} for {{Policy Deliberation}} on {{Climate Change}} and {{Sustainability}}},
  author = {Bina, Rachel and Luong, Kha and Mehta, Shrey and Pang, Daphne and Xie, Kristen and Chou, Christine and Kimbrough, Steven O.},
  year = {2025},
  month = feb,
  doi = {10.2139/ssrn.5123359},
  urldate = {2025-08-18},
  abstract = {Democracy begins with conversation.-John Dewey at his 90 th birthday party 1 Only that which has no history can be defined.-Frederick Nietzsche On the Genealogy},
  langid = {english},
  keywords = {AI forecasting attempts,NEED TO READ,sustainability},
  note = {Hence, we conclude that GPT-4 can be used as a credible input, even starting point, for subsequent deliberation processes on climate and sustainability policies.
\par
multiple criteria decision making
\par
(MCDM) models for comparative assessment of climate and sustainability policies
\par
in my work I extend past the purely ~ non-technical to include technical assessment

Other as-
\par
pects, particularly those focused on how a policy could impact well-being and quality of life in
\par
the community, perhaps should not be assessed by policy makers except through consultation with
\par
representatives of the general public. These well-being aspects of climate and sustainability policies
\par
are our main, but not exclusive, concern in this study.

WHAT KIND OF POLICY:

\par
The third kind of justification occurs when the policy in question fails or comes out uncertain
\par
on a strict cost--benefit basis but is judged to have sufficient co-benefits to overcome a cost--benefit
\par
shortcoming (Boyd et al., 2022; Creutzig et al., 2022; Dagnachew and Hof, 2022; Finn and Brockway,
\par
2023; Karlsson et al., 2020; Sharifi, 2021). An example of such a policy might be a ban on single-use
\par
plastic bags (yielding co-benefits of reduced litter in the streets and reduced landfill material) or a
\par
ban on gasoline-powered leaf blowers (yielding co-benefits of cleaner air, reduced ecological damage,
\par
and reduced noise).
\par
The present study centers upon this third kind of co-benefit-based warrant for climate and
\par
sustainability policies, and within it on factors that contribute to quality-of-life (alias well-being).
\par
Certain aspects of climate and sustainability plans, such as costs, emissions, degree of protection

etc., are technical by nature and best evaluated by climate scientists, city planners, engineers,
\par
and area experts in general, or obtained through careful assessment of the literature. Other as-
\par
pects, particularly those focused on how a policy could impact well-being and quality of life in
\par
the community, perhaps should not be assessed by policy makers except through consultation with
\par
representatives of the general public. These well-being aspects of climate and sustainability policies
\par
are our main, but not exclusive, concern in this study.
\par
Our study presumes that climate and sustainability policies are usefully---or even necessarily---
\par
compared across multiple evaluation criteria and that these criteria are inevitably in conflict, even
\par
limiting our attention to quality-of-life and well-being criteria. No policy is always and everywhere
\par
the best, and trade-offs are inevitable

STRATEGY:\\
Broke into {\textasciitilde}10 categories of policy assessment\\
e.g.\\
Moral considerations\\
Economy\\
Improving and creating destinations\\
etc\\
and scored from 1-10 on all}
}

@inproceedings{brownLanguageModelsAre2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
  year = {2020},
  volume = {33},
  pages = {1877--1901},
  publisher = {Curran Associates, Inc.},
  keywords = {introduction,methods},
  note = {show that language models improve as they scale -{$>$} they will keep getting better

''Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches.''
\par
Fine-Tuning (FT) - updates the weights of a pre-trained model by training on thousands of supervised labels specific to the desired task. The main advantage of fine-tuning is strong performance on many benchmarks. The main disadvantages are the need for a new large dataset for every task, the potential for poor generalization out-of-distribution [ MPL19 ], and the potential to exploit spurious features of the training data [GSL+18 , NK19]. We focus on task-agnostic performance, leaving fine-tuning for future work.}
}

@article{callaghanMachineLearningMap2025,
  title = {Machine Learning Map of Climate Policy Literature Reveals Disparities between Scientific Attention, Policy Density, and Emissions},
  author = {Callaghan, Max and Banisch, Lucy and {Doebbeling-Hildebrandt}, Niklas and Edmondson, Duncan and Flachsland, Christian and Lamb, William F. and Levi, Sebastian and {M{\"u}ller-Hansen}, Finn and Posada, Eduardo and Vasudevan, Shraddha and Minx, Jan C.},
  year = {2025},
  month = feb,
  journal = {npj Climate Action},
  volume = {4},
  number = {1},
  pages = {7},
  publisher = {Nature Publishing Group},
  issn = {2731-9814},
  doi = {10.1038/s44168-024-00196-0},
  urldate = {2025-08-24},
  abstract = {Current climate mitigation policies are not sufficient to meet the Paris temperature target, and ramping up efforts will require rapid learning from the scientific literature on climate policies. This literature is vast and widely dispersed, as well as hard to define and categorise, hampering systematic efforts to learn from it. We use a machine learning pipeline using transformer-based language models to systematically map the relevant scientific literature on climate policies at scale and in real-time. Our ``living systematic map'' of climate policy research features a set of 84,990 papers, and classifies each of them by policy instrument type, sector, and geography. We explore how the distribution of these papers varies across countries, and compare this to the distribution of emissions and enacted climate policies. Results suggests a potential stark under-representation of industry sector policies, as well as diverging attention between science and policy with respect to economic and regulatory instruments.},
  copyright = {2025 The Author(s)},
  langid = {english},
  keywords = {Climate-change mitigation},
  file = {/home/dmrivers/Zotero/storage/F6MEDTBM/Callaghan et al. - 2025 - Machine learning map of climate policy literature reveals disparities between scientific attention,.pdf}
}

@misc{chenPredictingFieldExperiments2025,
  title = {Predicting {{Field Experiments}} with {{Large Language Models}}},
  author = {Chen, Yaoyu and Hu, Yuheng and Lu, Yingda},
  year = {2025},
  month = may,
  number = {arXiv:2504.01167},
  eprint = {2504.01167},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2504.01167},
  urldate = {2025-08-19},
  abstract = {Large language models (LLMs) have demonstrated unprecedented emergent capabilities, including content generation, translation, and simulation of human behavior. Field experiments, on the other hand, are widely employed in social studies to examine real-world human behavior through carefully designed manipulations and treatments. However, field experiments are known to be expensive and time consuming. Therefore, an interesting question is whether and how LLMs can be utilized for field experiments. In this paper, we propose and evaluate an automated LLM-based framework to predict the outcomes of a field experiment. Applying this framework to 276 experiments about a wide range of human behaviors drawn from renowned economics literature yields a prediction accuracy of 78\%. Moreover, we find that the distributions of the results are either bimodal or highly skewed. By investigating this abnormality further, we identify that field experiments related to complex social issues such as ethnicity, social norms, and ethical dilemmas can pose significant challenges to the prediction performance.},
  archiveprefix = {arXiv},
  keywords = {AI forecasting attempts,development,NEED TO READ},
  note = {For example, our framework achieves nearly 100\% predic-
\par
tion accuracy on 71\% of conclusions while it completely fails to
\par
arXiv:2504.01167v3 [cs.CY] 21 May 2025
\par
SLW, August 6th, 2025, Toronto
\par
Yaoyu Chen1 Yuheng Hu1 Yingda Lu1
\par
1The Department of Information and Decision Sciences, College of Business Administration, University of Illinois at Chicago
\par
\{ychen563, yuhenghu, yingdalu\}@uic.com
\par
predict 18\% of the conclusions with close to 0\% accuracy.},
  file = {/home/dmrivers/Zotero/storage/VPJFMRGV/2504.html}
}

@article{cherianLargeLanguageModel2024,
  title = {Large Language Model Validity via Enhanced Conformal Prediction Methods},
  author = {Cherian, John J. and Gibbs, Isaac and Cand{\`e}s, Emmanuel J.},
  year = {2024},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {37},
  pages = {114812--114842},
  urldate = {2025-08-31},
  langid = {english},
  file = {/home/dmrivers/Zotero/storage/ZVRMAK5E/Cherian et al. - 2024 - Large language model validity via enhanced conformal prediction methods.pdf}
}

@misc{ClimatePolicyDatabase,
  title = {Climate {{Policy Database}} {\textbar} {{NewClimate Institute}}},
  urldate = {2025-08-19},
  abstract = {The Climate Policy Database (CDPB) is an open, collaborative tool to advance the data collection of the implementation status of climate policies.},
  howpublished = {https://newclimate.org/resources/tools/climate-policy-database},
  langid = {english},
  file = {/home/dmrivers/Zotero/storage/Y2CY9F4I/climate-policy-database.html}
}

@article{CosteffectiveControlAir2011,
  title = {Cost-Effective Control of Air Quality and Greenhouse Gases in {{Europe}}: {{Modeling}} and Policy Applications},
  shorttitle = {Cost-Effective Control of Air Quality and Greenhouse Gases in {{Europe}}},
  year = {2011},
  month = dec,
  journal = {Environmental Modelling \& Software},
  volume = {26},
  number = {12},
  pages = {1489--1501},
  publisher = {Elsevier},
  issn = {1364-8152},
  doi = {10.1016/j.envsoft.2011.07.012},
  urldate = {2025-08-24},
  abstract = {Environmental policies in Europe have successfully eliminated the most visible and immediate harmful effects of air pollution in the last decades. How{\dots}},
  langid = {american},
  file = {/home/dmrivers/Zotero/storage/JBJ7LRPP/S1364815211001733.html}
}

@misc{DatenlaborbmzAwesomedevelopmentcooperationdata2025,
  title = {Datenlabor-Bmz/Awesome-Development-Cooperation-Data},
  year = {2025},
  month = jun,
  urldate = {2025-08-22},
  howpublished = {Datenlabor BMZ}
}

@article{deepseek-aiDeepSeekR1IncentivizingReasoning2025,
  title = {{{DeepSeek-R1}}: {{Incentivizing Reasoning Capability}} in {{LLMs}} via {{Reinforcement Learning}}},
  shorttitle = {{{DeepSeek-R1}}},
  author = {{DeepSeek-AI} and Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and Zhang, Xiaokang and Yu, Xingkai and Wu, Yu and Wu, Z. F. and Gou, Zhibin and Shao, Zhihong and Li, Zhuoshu and Gao, Ziyi and Liu, Aixin and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Feng, Bei and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and Dai, Damai and Chen, Deli and Ji, Dongjie and Li, Erhang and Lin, Fangyun and Dai, Fucong and Luo, Fuli and Hao, Guangbo and Chen, Guanting and Li, Guowei and Zhang, H. and Bao, Han and Xu, Hanwei and Wang, Haocheng and Ding, Honghui and Xin, Huajian and Gao, Huazuo and Qu, Hui and Li, Hui and Guo, Jianzhong and Li, Jiashi and Wang, Jiawei and Chen, Jingchang and Yuan, Jingyang and Qiu, Junjie and Li, Junlong and Cai, J. L. and Ni, Jiaqi and Liang, Jian and Chen, Jin and Dong, Kai and Hu, Kai and Gao, Kaige and Guan, Kang and Huang, Kexin and Yu, Kuai and Wang, Lean and Zhang, Lecong and Zhao, Liang and Wang, Litong and Zhang, Liyue and Xu, Lei and Xia, Leyi and Zhang, Mingchuan and Zhang, Minghua and Tang, Minghui and Li, Meng and Wang, Miaojun and Li, Mingming and Tian, Ning and Huang, Panpan and Zhang, Peng and Wang, Qiancheng and Chen, Qinyu and Du, Qiushi and Ge, Ruiqi and Zhang, Ruisong and Pan, Ruizhe and Wang, Runji and Chen, R. J. and Jin, R. L. and Chen, Ruyi and Lu, Shanghao and Zhou, Shangyan and Chen, Shanhuang and Ye, Shengfeng and Wang, Shiyu and Yu, Shuiping and Zhou, Shunfeng and Pan, Shuting and Li, S. S.},
  year = {2025},
  month = jan,
  journal = {CoRR},
  urldate = {2025-08-29},
  abstract = {We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.},
  langid = {english},
  file = {/home/dmrivers/Zotero/storage/BII8NZRB/DeepSeek-AI et al. - 2025 - DeepSeek-R1 Incentivizing Reasoning Capability in LLMs via Reinforcement Learning.pdf}
}

@article{dellavignaPredictScienceImprove2019,
  title = {Predict Science to Improve Science},
  author = {DellaVigna, Stefano and Pope, Devin and Vivalt, Eva},
  year = {2019},
  month = oct,
  journal = {Science},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.aaz1704},
  urldate = {2025-08-22},
  abstract = {Systematic collection of predictions of research findings can provide many benefits},
  copyright = {Copyright {\copyright} 2019, American Association for the Advancement of Science},
  langid = {english},
  note = {\href{https://socialscienceprediction.org/predict/}{https://socialscienceprediction.org/predict/}

\href{https://blogs.worldbank.org/en/impactevaluations/how-can-you-use-the-social-science-prediction-platform-for-devel}{https://blogs.worldbank.org/en/impactevaluations/how-can-you-use-the-social-science-prediction-platform-for-devel}
\par
SSPP was launched to systematize such forecasts, collecting expectations from both experts and laypeople for studies in areas like poverty alleviation and education reform},
  file = {/home/dmrivers/Zotero/storage/BTZ4M2LH/science.html}
}

@article{dueriModelingImplicationsPolicy2024,
  title = {Modeling the Implications of Policy Reforms on Pesticide Risk for {{Switzerland}}},
  author = {Dueri, Sibylle and Mack, Gabriele},
  year = {2024},
  month = jun,
  journal = {The Science of the Total Environment},
  volume = {928},
  pages = {172436},
  issn = {1879-1026},
  doi = {10.1016/j.scitotenv.2024.172436},
  abstract = {Growing public awareness of the negative effects of pesticides on the environment, ecosystems, and human health has led governments to set targets for reducing pesticide risk. Switzerland introduced in 2023 two new policy measures to reduce pesticide risk by 50~\% by 2027: (1) voluntary direct payment programs supporting pesticide-reduced and pesticide-free but non-organic cropping systems for most crops on arable land, and (2) restrictions of harmful pesticides for farmers managing under Swiss cross-compliance standards. This study aims to (1) develop a method to assess pesticide risk on a national scale and (2) carry out an ex-ante impact assessment to predict whether these policies can effectively reduce pesticide risks in Switzerland. Therefore, we introduced crop-specific pesticide quantities and pesticide risk scores into a sample of 1907 bio-economic farm optimization models. The models were used to predict farmers' adoption decisions regarding voluntary direct payment programs from 2019 to 2030. By combining the bio-economic farm optimization models with an agent-based modeling approach, we assessed the evolution of pesticide-related risks at the national level. Simulations for pesticide risk from 2019 to 2022 reflected the observed pesticide risk monitored by the Swiss government. In surface waters and semi-natural habitats, achieving the target depends on reducing pyrethroids, a class of insecticides with high-risk potential. Further, we highlight significant uncertainty in projecting the risk potential for surface waters and semi-natural habitats due to uncertainty about the amounts of pyrethroid used for different crops. The results underline the need for comprehensive datasets on pesticide use in Switzerland.},
  langid = {english},
  pmid = {38615777},
  keywords = {Agriculture,Agro-environmental policy,Bio-economic model,Crops Agricultural,Environmental Policy,Farm models,Humans,Pesticide risk,Pesticides,Pyrethroids,Risk Assessment,Switzerland}
}

@article{duyxStrongFocusPositive2019,
  title = {The Strong Focus on Positive Results in Abstracts May Cause Bias in Systematic Reviews: A Case Study on Abstract Reporting Bias},
  shorttitle = {The Strong Focus on Positive Results in Abstracts May Cause Bias in Systematic Reviews},
  author = {Duyx, Bram and Swaen, Gerard M. H. and Urlings, Miriam J. E. and Bouter, Lex M. and Zeegers, Maurice P.},
  year = {2019},
  month = dec,
  journal = {Systematic Reviews},
  volume = {8},
  number = {1},
  pages = {1--8},
  publisher = {BioMed Central},
  issn = {2046-4053},
  doi = {10.1186/s13643-019-1082-9},
  urldate = {2025-08-31},
  abstract = {Research articles tend to focus on positive findings in their abstract, especially if multiple outcomes have been studied. At the same time, search queries in databases are generally limited to the abstract, title and keywords fields of an article. Negative findings are therefore less likely to be detected by systematic searches and to appear in systematic reviews. We aim to assess the occurrence of this `abstract reporting bias' and quantify its impact in the literature on the association between diesel exhaust exposure (DEE) and bladder cancer. We set up a broad search query related to DEE and cancer in general. Full-texts of the articles identified in the search output were manually scanned. Articles were included if they reported, anywhere in the full-text, the association between DEE and bladder cancer. We assume that the use of a broad search query and manual full-text scanning allowed us to catch all the relevant articles, including those in which bladder cancer was not mentioned in the abstract, title or keywords. We identified 28 articles. Only 12 of these (43\%) had mentioned bladder in their abstract, title or keywords. A meta-analysis based on these 12 detectable articles yielded a pooled risk estimate of 1.10 (95\% confidence interval [CI] 0.97--1.25), whereas the meta-analysis based on all 28 articles yielded a pooled estimate of 1.03 (95\% CI 0.96--1.11). This case study on abstract reporting bias shows that (a) more than half of all relevant articles were missed by a conventional search query and (b) this led to an overestimation of the pooled effect. Detection of articles will be improved if all studied exposure and outcome variables are reported in the keywords. The restriction on the maximum number of keywords should be lifted.},
  copyright = {2019 The Author(s).},
  langid = {english},
  file = {/home/dmrivers/Zotero/storage/NRUHJH6X/Duyx et al. - 2019 - The strong focus on positive results in abstracts may cause bias in systematic reviews a case study.pdf}
}

@misc{ElementsStatisticalLearning,
  title = {The {{Elements}} of {{Statistical Learning}}: {{Data Mining}}, {{Inference}}, and {{Prediction}}, {{Second Edition}} {\textbar} {{SpringerLink}}},
  urldate = {2025-08-19},
  howpublished = {https://link.springer.com/book/10.1007/978-0-387-84858-7?utm\_source=chatgpt.com},
  keywords = {methods},
  note = {for citing k-fold validation},
  file = {/home/dmrivers/Zotero/storage/XSDHEZHL/978-0-387-84858-7.html}
}

@misc{EvaluatingGermanDevelopment,
  title = {Evaluating {{German Development Cooperation}}},
  journal = {Federal Ministry for Economic Cooperation and Development},
  urldate = {2025-08-19},
  abstract = {The Federal Ministry for Economic Cooperation and Development is responsible for planning and implementing the German government's development policy. Its tasks focus on the following areas: helping to shape global framework conditions, developing bilateral and multilateral promotional strategies and supporting development programmes and projects in partner countries, promoting development cooperation among non-governmental organisations, monitoring success and controlling the use of funds.},
  howpublished = {https://www.bmz.de/resource/blob/194630/bmz193-strategiepapier-evaluierung-en.pdf},
  langid = {english},
  keywords = {development,NEED TO READ},
  file = {/home/dmrivers/Zotero/storage/VWGV72QU/bmz193-strategiepapier-evaluierung-en.html}
}

@article{fullerPollutionHealthProgress2022,
  title = {Pollution and Health: A Progress Update},
  shorttitle = {Pollution and Health},
  author = {Fuller, Richard and Landrigan, Philip J. and Balakrishnan, Kalpana and Bathan, Glynda and {Bose-O'Reilly}, Stephan and Brauer, Michael and Caravanos, Jack and Chiles, Tom and Cohen, Aaron and Corra, Lilian and Cropper, Maureen and Ferraro, Greg and Hanna, Jill and Hanrahan, David and Hu, Howard and Hunter, David and Janata, Gloria and Kupka, Rachael and Lanphear, Bruce and Lichtveld, Maureen and Martin, Keith and Mustapha, Adetoun and {Sanchez-Triana}, Ernesto and Sandilya, Karti and Schaefli, Laura and Shaw, Joseph and Seddon, Jessica and Suk, William and {T{\'e}llez-Rojo}, Martha Mar{\'i}a and Yan, Chonghuai},
  year = {2022},
  month = jun,
  journal = {The Lancet Planetary Health},
  volume = {6},
  number = {6},
  pages = {e535-e547},
  publisher = {Elsevier},
  issn = {2542-5196},
  doi = {10.1016/S2542-5196(22)00090-0},
  urldate = {2025-08-24},
  abstract = {{$<$}h2{$>$}Summary{$<$}/h2{$><$}p{$>$}The \emph{Lancet} Commission on pollution and health reported that pollution was responsible for 9 million premature deaths in 2015, making it the world's largest environmental risk factor for disease and premature death. We have now updated this estimate using data from the Global Burden of Diseases, Injuriaes, and Risk Factors Study 2019. We find that pollution remains responsible for approximately 9 million deaths per year, corresponding to one in six deaths worldwide. Reductions have occurred in the number of deaths attributable to the types of pollution associated with extreme poverty. However, these reductions in deaths from household air pollution and water pollution are offset by increased deaths attributable to ambient air pollution and toxic chemical pollution (ie, lead). Deaths from these modern pollution risk factors, which are the unintended consequence of industrialisation and urbanisation, have risen by 7\% since 2015 and by over 66\% since 2000. Despite ongoing efforts by UN agencies, committed groups, committed individuals, and some national governments (mostly in high-income countries), little real progress against pollution can be identified overall, particularly in the low-income and middle-income countries, where pollution is most severe. Urgent attention is needed to control pollution and prevent pollution-related disease, with an emphasis on air pollution and lead poisoning, and a stronger focus on hazardous chemical pollution. Pollution, climate change, and biodiversity loss are closely linked. Successful control of these conjoined threats requires a globally supported, formal science--policy interface to inform intervention, influence research, and guide funding. Pollution has typically been viewed as a local issue to be addressed through subnational and national regulation or, occasionally, using regional policy in higher-income countries. Now, however, it is increasingly clear that pollution is a planetary threat, and that its drivers, its dispersion, and its effects on health transcend local boundaries and demand a global response. Global action on all major modern pollutants is needed. Global efforts can synergise with other global environmental policy programmes, especially as a large-scale, rapid transition away from all fossil fuels to clean, renewable energy is an effective strategy for preventing pollution while also slowing down climate change, and thus achieves a double benefit for planetary health.{$<$}/p{$>$}},
  langid = {english},
  pmid = {35594895},
  file = {/home/dmrivers/Zotero/storage/V2CQLCX6/Fuller et al. - 2022 - Pollution and health a progress update.pdf}
}

@misc{ghasemlooInformedForecastingLeveraging2025,
  title = {Informed {{Forecasting}}: {{Leveraging Auxiliary Knowledge}} to {{Boost LLM Performance}} on {{Time Series Forecasting}}},
  shorttitle = {Informed {{Forecasting}}},
  author = {Ghasemloo, Mohammadmahdi and Moradi, Alireza},
  year = {2025},
  month = aug,
  number = {arXiv:2505.10213},
  eprint = {2505.10213},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.10213},
  urldate = {2025-08-21},
  abstract = {With the widespread adoption of Large Language Models (LLMs), there is a growing need to establish best practices for leveraging their capabilities beyond traditional natural language tasks. In this paper, a novel cross-domain knowledge transfer framework is proposed to enhance the performance of LLMs in time series forecasting -- a task of increasing relevance in fields such as energy systems, finance, and healthcare. The approach systematically infuses LLMs with structured temporal information to improve their forecasting accuracy. This study evaluates the proposed method on a real-world time series dataset and compares it to a naive baseline where the LLM receives no auxiliary information. Results show that knowledge-informed forecasting significantly outperforms the uninformed baseline in terms of predictive accuracy and generalization. These findings highlight the potential of knowledge transfer strategies to bridge the gap between LLMs and domain-specific forecasting tasks.},
  archiveprefix = {arXiv},
  note = {Shows gains from adding covariates + tailored prompting to LLM forecasters; relevant to your retrieval/feature-conditioning ablations. \href{https://arxiv.org/abs/2505.10213}{https://arxiv.org/abs/2505.10213}},
  file = {/home/dmrivers/Zotero/storage/DGSVE7QV/2505.html}
}

@article{gneitingStrictlyProperScoring2007,
  title = {Strictly {{Proper Scoring Rules}}, {{Prediction}}, and {{Estimation}}},
  author = {Gneiting, Tilmann and Raftery, Adrian E},
  year = {2007},
  month = mar,
  journal = {Journal of the American Statistical Association},
  volume = {102},
  number = {477},
  pages = {359--378},
  publisher = {ASA Website},
  issn = {0162-1459},
  doi = {10.1198/016214506000001437},
  urldate = {2025-08-19},
  abstract = {Scoring rules assess the quality of probabilistic forecasts, by assigning a numerical score based on the predictive distribution and on the event or value that materializes. A scoring rule is proper if the forecaster maximizes the expected score for an observation drawn from the distributionF if he or she issues the probabilistic forecast F, rather than G {$\neq$} F. It is strictly proper if the maximum is unique. In prediction problems, proper scoring rules encourage the forecaster to make careful assessments and to be honest. In estimation problems, strictly proper scoring rules provide attractive loss and utility functions that can be tailored to the problem at hand. This article reviews and develops the theory of proper scoring rules on general probability spaces, and proposes and discusses examples thereof. Proper scoring rules derive from convex functions and relate to information measures, entropy functions, and Bregman divergences. In the case of categorical variables, we prove a rigorous version of the Savage representation. Examples of scoring rules for probabilistic forecasts in the form of predictive densities include the logarithmic, spherical, pseudospherical, and quadratic scores. The continuous ranked probability score applies to probabilistic forecasts that take the form of predictive cumulative distribution functions. It generalizes the absolute error and forms a special case of a new and very general type of score, the energy score. Like many other scoring rules, the energy score admits a kernel representation in terms of negative definite functions, with links to inequalities of Hoeffding type, in both univariate and multivariate settings. Proper scoring rules for quantile and interval forecasts are also discussed. We relate proper scoring rules to Bayes factors and to cross-validation, and propose a novel form of cross-validation known as random-fold cross-validation. A case study on probabilistic weather forecasts in the North American Pacific Northwest illustrates the importance of propriety. We note optimum score approaches to point and quantile estimation, and propose the intuitively appealing interval score as a utility function in interval estimation that addresses width as well as coverage.},
  keywords = {methods},
  note = {PURPOSE:\\
provides equations and methods for quantitative forecasting

QUOTATION:\\
we contended that the
\par
goal of probabilistic forecasting is to maximize the sharpness
\par
of the predictive distributions subject to calibration. Calibration
\par
refers to the statistical consistency between the distributional
\par
forecasts and the observations, and is a joint property of the
\par
forecasts and the events or values that materialize. Sharpness
\par
refers to the concentration of the predictive distributions and is
\par
a property of the forecasts only.
\par
Scoring rules provide summary measures for the evaluation
\par
of probabilistic forecasts, by assigning a numerical score based
\par
on the predictive distribution and on the event or value that ma-
\par
terializes. In terms of elicitation, the role of scoring rules is
\par
to encourage the assessor to make careful assessments and to
\par
be honest (Garthwaite, Kadane, and O'Hagan 2005). In terms
\par
of evaluation, scoring rules measure the quality of the proba-
\par
bilistic forecasts, reward probability assessors for forecasting
\par
jobs, and rank competing forecast procedures}
}

@inproceedings{gruverLargeLanguageModels2023,
  title = {Large {{Language Models Are Zero-Shot Time Series Forecasters}}},
  booktitle = {Thirty-Seventh {{Conference}} on {{Neural Information Processing Systems}}},
  author = {Gruver, Nate and Finzi, Marc Anton and Qiu, Shikai and Wilson, Andrew Gordon},
  year = {2023},
  month = nov,
  urldate = {2025-08-19},
  abstract = {By encoding time series as a string of numerical digits, we can frame time series forecasting as next-token prediction in text. Developing this approach, we find that large language models (LLMs) such as GPT-3 and LLaMA-2 can surprisingly zero-shot extrapolate time series at a level comparable to or exceeding the performance of purpose-built time series models trained on the downstream tasks. To facilitate this performance, we propose procedures for effectively tokenizing time series data and converting discrete distributions over tokens into highly flexible densities over continuous values. We argue the success of LLMs for time series stems from their ability to naturally represent multimodal distributions, in conjunction with biases for simplicity, and repetition, which align with the salient features in many time series, such as repeated seasonal trends. We also show how LLMs can naturally handle missing data without imputation through non-numerical text, accommodate textual side information, and answer questions to help explain predictions. While we find that increasing model size generally improves performance on time series, we show GPT-4 can perform worse than GPT-3 because of how it tokenizes numbers, and poor uncertainty calibration, which is likely the result of alignment interventions such as RLHF.},
  langid = {english},
  note = {``Finally, there has been recent work on using transformer models or LMs for statistical time-series forecasting'' (Halawi et al., 2024, p. 3)}
}

@misc{guanOpenEPOpenEndedFuture2024,
  title = {{{OpenEP}}: {{Open-Ended Future Event Prediction}}},
  shorttitle = {{{OpenEP}}},
  author = {Guan, Yong and Peng, Hao and Wang, Xiaozhi and Hou, Lei and Li, Juanzi},
  year = {2024},
  month = aug,
  number = {arXiv:2408.06578},
  eprint = {2408.06578},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.06578},
  urldate = {2025-08-18},
  abstract = {Future event prediction (FEP) is a long-standing and crucial task in the world, as understanding the evolution of events enables early risk identification, informed decision-making, and strategic planning. Existing work typically treats event prediction as classification tasks and confines the outcomes of future events to a fixed scope, such as yes/no questions, candidate set, and taxonomy, which is difficult to include all possible outcomes of future events. In this paper, we introduce OpenEP (an Open-Ended Future Event Prediction task), which generates flexible and diverse predictions aligned with real-world scenarios. This is mainly reflected in two aspects: firstly, the predictive questions are diverse, covering different stages of event development and perspectives; secondly, the outcomes are flexible, without constraints on scope or format. To facilitate the study of this task, we construct OpenEPBench, an open-ended future event prediction dataset. For question construction, we pose questions from seven perspectives, including location, time, event development, event outcome, event impact, event response, and other, to facilitate an in-depth analysis and understanding of the comprehensive evolution of events. For outcome construction, we collect free-form text containing the outcomes as ground truth to provide semantically complete and detail-enriched outcomes. Furthermore, we propose StkFEP, a stakeholder-enhanced future event prediction framework, that incorporates event characteristics for open-ended settings. Our method extracts stakeholders involved in events to extend questions to gather diverse information. We also collect historically events that are relevant and similar to the question to reveal potential evolutionary patterns. Experiment results indicate that accurately predicting future events in open-ended settings is challenging for existing LLMs.},
  archiveprefix = {arXiv},
  keywords = {AI forecasting attempts},
  note = {Experiment results indicate that accurately predicting future events in open-ended settings is challenging for existing LLMs.},
  file = {/home/dmrivers/Zotero/storage/U49WKVCV/Guan et al. - 2024 - OpenEP Open-Ended Future Event Prediction.pdf;/home/dmrivers/Zotero/storage/DL8TN9Y6/2408.html}
}

@misc{hahnWelfareAnalysisPolicies2024,
  type = {Working {{Paper}}},
  title = {A {{Welfare Analysis}} of {{Policies Impacting Climate Change}}},
  author = {Hahn, Robert W. and Hendren, Nathaniel and Metcalfe, Robert D. and {Sprung-Keyser}, Ben},
  year = {2024},
  month = jul,
  series = {Working {{Paper Series}}},
  number = {32728},
  eprint = {32728},
  publisher = {National Bureau of Economic Research},
  doi = {10.3386/w32728},
  urldate = {2025-08-18},
  abstract = {What are the most effective ways to address climate change? This paper extends and applies the marginal value of public funds (MVPF) framework to help answer this question. We examine 96 US climate-related policy changes studied over the past 25 years. These policies span subsidies (wind, residential solar, electric and hybrid vehicles, vehicle retirement, appliance rebates, weatherization), nudges (marketing, energy conservation), and revenue raisers (fuel taxes, cap and trade). For each policy, we draw upon quasi-experimental or experimental evaluations of its causal effects and translate those estimates into an MVPF. We apply a consistent translation of these behavioral responses into measures of their associated externalities and valuations of those externalities. We also provide a new method for incorporating learning-by-doing spillovers. The analysis yields three main results: First, subsidies for investments that directly displace the dirty production of electricity, such as production tax credits for wind power and subsidies for residential solar panels, have higher MVPFs (generally exceeding 2) than all other subsidies in our sample (with MVPFs generally around 1). Second, nudges to reduce energy consumption have large MVPFs, with values above 5, when targeted to regions of the US with a dirty electric grid. By contrast, policies targeting areas with cleaner grids, such as California and the Northeast, have substantially smaller MVPFs (often below 1). Third, fuel taxes and cap-and-trade policies are highly efficient means of raising revenue (with MVPFs below 0.7). We contrast these conclusions with those derived from more traditional cost-per-ton metrics used in previous literature.},
  archiveprefix = {National Bureau of Economic Research},
  file = {/home/dmrivers/Zotero/storage/3UHMLELC/Hahn et al. - 2024 - A Welfare Analysis of Policies Impacting Climate Change.pdf}
}

@inproceedings{halawiApproachingHumanLevelForecasting2024,
  title = {Approaching {{Human-Level Forecasting}} with {{Language Models}}},
  booktitle = {The {{Thirty-eighth Annual Conference}} on {{Neural Information Processing Systems}}},
  author = {Halawi, Danny and Zhang, Fred and {Yueh-Han}, Chen and Steinhardt, Jacob},
  year = {2024},
  month = nov,
  urldate = {2025-08-18},
  abstract = {Forecasting future events is important for policy and decision making. In this work, we study whether language models (LMs) can forecast at the level of competitive human forecasters. Towards this goal, we develop a retrieval-augmented LM system designed to automatically search for relevant information, generate forecasts, and aggregate predictions. To facilitate our study, we collect a large dataset of questions from competitive forecasting platforms. Under a test set published after the knowledge cut-offs of our LMs, we evaluate the end-to-end performance of our system against the aggregates of human forecasts. On average, the system nears the crowd aggregate of competitive forecasters and, in a certain relaxed setting, surpasses it. Our work suggests that using LMs to forecasts the future could provide accurate predictions at scale and help to inform institutional decision making.},
  langid = {english},
  keywords = {AI forecasting attempts},
  file = {/home/dmrivers/Zotero/storage/X3QM3IPS/Halawi et al. - 2024 - Approaching Human-Level Forecasting with Language Models.pdf}
}

@article{hansonShallWeVote2013,
  title = {Shall {{We Vote}} on {{Values}}, {{But Bet}} on {{Beliefs}}?},
  author = {Hanson, Robin},
  year = {2013},
  journal = {Journal of Political Philosophy},
  volume = {21},
  number = {2},
  pages = {151--178},
  issn = {1467-9760},
  doi = {10.1111/jopp.12008},
  urldate = {2025-08-19},
  copyright = {{\copyright} 2013 John Wiley \& Sons Ltd},
  langid = {english},
  keywords = {future work; outlook},
  note = {Futarchy},
  file = {/home/dmrivers/Zotero/storage/A7RQMDZU/Hanson - 2013 - Shall We Vote on Values, But Bet on Beliefs.pdf;/home/dmrivers/Zotero/storage/RQZITBG9/jopp.html}
}

@article{hewittPredictingResultsSocial,
  title = {Predicting {{Results}} of {{Social Science Experiments Using Large Language Models}}},
  author = {Hewitt, Luke and Ashokkumar, Ashwini and Ghezae, Isaias and Willer, Robb},
  abstract = {To evaluate whether large language models (LLMs) can be leveraged to predict the results of social science experiments, we built an archive of 70 pre-registered, nationally representative, survey experiments conducted in the United States, involving 476 experimental treatment effects and 105,165 participants. We prompted an advanced, publicly-available LLM (GPT-4) to simulate how representative samples of Americans would respond to the stimuli from these experiments. Predictions derived from simulated responses correlate strikingly with actual treatment effects (r = 0.85), equaling or surpassing the predictive accuracy of human forecasters. Accuracy remained high for unpublished studies that could not appear in the model's training data (r = 0.90). We further assessed predictive accuracy across demographic subgroups, various disciplines, and in nine recent megastudies featuring an additional 346 treatment effects. Together, our results suggest LLMs can augment experimental methods in science and practice, but also highlight important limitations and risks of misuse.},
  langid = {english},
  keywords = {NEED TO READ},
  note = {``LLM-derived predictions remained highly accurate for studies that could not have been in the LLM training data given they were not published prior to the LLM training data cutoff date.'' (Hewitt et al., p. 7)},
  file = {/home/dmrivers/Zotero/storage/FLFLN5LF/Hewitt et al. - Predicting Results of Social Science Experiments Using Large Language Models.pdf}
}

@misc{HowCanYou,
  title = {How Can You Use the {{Social Science Prediction Platform}} for Development Papers?},
  journal = {World Bank Blogs},
  urldate = {2025-08-22},
  howpublished = {https://blogs.worldbank.org/en/impactevaluations/how-can-you-use-the-social-science-prediction-platform-for-devel},
  langid = {english},
  file = {/home/dmrivers/Zotero/storage/X35NGT3L/how-can-you-use-the-social-science-prediction-platform-for-devel.html}
}

@article{huangSurveyHallucinationLarge2025,
  title = {A {{Survey}} on {{Hallucination}} in {{Large Language Models}}: {{Principles}}, {{Taxonomy}}, {{Challenges}}, and {{Open Questions}}},
  shorttitle = {A {{Survey}} on {{Hallucination}} in {{Large Language Models}}},
  author = {Huang, Lei and Yu, Weijiang and Ma, Weitao and Zhong, Weihong and Feng, Zhangyin and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and Liu, Ting},
  year = {2025},
  month = mar,
  journal = {ACM Transactions on Information Systems},
  volume = {43},
  number = {2},
  pages = {1--55},
  issn = {1046-8188, 1558-2868},
  doi = {10.1145/3703155},
  urldate = {2025-08-20},
  abstract = {The emergence of large language models (LLMs) has marked a significant breakthrough in natural language processing (NLP), fueling a paradigm shift in information acquisition. Nevertheless, LLMs are prone to hallucination, generating plausible yet nonfactual content. This phenomenon raises significant concerns over the reliability of LLMs in real-world information retrieval (IR) systems and has attracted intensive research to detect and mitigate such hallucinations. Given the open-ended general-purpose attributes inherent to LLMs, LLM hallucinations present distinct challenges that diverge from prior task-specific models. This divergence highlights the urgency for a nuanced understanding and comprehensive overview of recent advances in LLM hallucinations. In this survey, we begin with an innovative taxonomy of hallucination in the era of LLM and then delve into the factors contributing to hallucinations. Subsequently, we present a thorough overview of hallucination detection methods and benchmarks. Our discussion then transfers to representative methodologies for mitigating LLM hallucinations. Additionally, we delve into the current limitations faced by retrieval-augmented LLMs in combating hallucinations, offering insights for developing more robust IR systems. Finally, we highlight the promising research directions on LLM hallucinations, including hallucination in large vision-language models and understanding of knowledge boundaries in LLM hallucinations.},
  langid = {english},
  file = {/home/dmrivers/Zotero/storage/UVFEZYDT/Huang et al. - 2025 - A Survey on Hallucination in Large Language Models Principles, Taxonomy, Challenges, and Open Quest.pdf}
}

@misc{ImpactAI,
  title = {{{ImpactAI}}},
  urldate = {2025-08-22},
  howpublished = {https://www.worldbank.org/en/about/unit/unit-dec/impactevaluation/ai/impact-ai},
  note = {(Like climsight but for development)

ImpactAI, our flagship project, harnesses the power of large language models (LLMs) to sift through swathes of impact evaluation research and accurately extract insights that provide effortless access to causal evidence for improved decision-making. With a generative AI layer, users can ask the tool questions in natural language and receive quick, reliable, actionable responses. Unlike existing AI-for-research tools, ImpactAI specializes in causal evidence and delivers aggregated, quantitative research summaries and interactive visualizations -- delivering months of analysis in seconds. Primary users include development practitioners, policymakers, academics, think tanks, and impact investors. As a one-stop source that lowers the barriers to accessing high-quality research, ImpactAI offers easy access to cutting-edge ideas, best practices, and empirical evidence, thereby enhancing the effectiveness of development projects worldwide.},
  file = {/home/dmrivers/Zotero/storage/8AIN6SJ8/impact-ai.html}
}

@incollection{intergovernmentalpanelonclimatechangeipccMitigationDevelopmentPathways2023,
  title = {Mitigation and {{Development Pathways}} in the {{Near}} to {{Mid-term}}},
  booktitle = {Climate {{Change}} 2022 - {{Mitigation}} of {{Climate Change}}},
  editor = {{Intergovernmental Panel On Climate Change (Ipcc)}},
  year = {2023},
  month = aug,
  edition = {1},
  pages = {409--502},
  publisher = {Cambridge University Press},
  doi = {10.1017/9781009157926.006},
  urldate = {2025-08-24},
  copyright = {https://www.cambridge.org/core/terms},
  isbn = {978-1-009-15792-6},
  file = {/home/dmrivers/Zotero/storage/C5WCBVWM/Intergovernmental Panel On Climate Change (Ipcc) - 2023 - Mitigation and Development Pathways in the Near to Mid-term.pdf}
}

@article{ioannidisWhyMostPublished2005,
  title = {Why {{Most Published Research Findings Are False}}},
  author = {Ioannidis, John P. A.},
  year = {2005},
  month = aug,
  journal = {PLOS Medicine},
  volume = {2},
  number = {8},
  pages = {e124},
  publisher = {Public Library of Science},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.0020124},
  urldate = {2025-08-31},
  abstract = {Summary There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
  langid = {english},
  keywords = {Cancer risk factors,Finance,Genetic epidemiology,Genetics of disease,Metaanalysis,Randomized controlled trials,Research design,Schizophrenia},
  file = {/home/dmrivers/Zotero/storage/HY8RSUE7/Ioannidis - 2005 - Why Most Published Research Findings Are False.pdf}
}

@article{jumperHighlyAccurateProtein2021,
  title = {Highly Accurate Protein Structure Prediction with {{AlphaFold}}},
  author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\v Z}{\'i}dek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and {Romera-Paredes}, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},
  year = {2021},
  month = aug,
  journal = {Nature},
  volume = {596},
  number = {7873},
  pages = {583--589},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-021-03819-2},
  urldate = {2025-08-24},
  abstract = {Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort1--4, the structures of around 100,000 unique proteins have been determined5, but this represents a small fraction of the billions of known protein sequences6,7. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence---the structure prediction component of the `protein folding problem'8---has been an important open research problem for more than 50~years9. Despite recent progress10--14, existing methods fall far~short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)15, demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Computational biophysics,Machine learning,Protein structure predictions,Structural biology},
  file = {/home/dmrivers/Zotero/storage/JDHBPAJW/Jumper et al. - 2021 - Highly accurate protein structure prediction with AlphaFold.pdf}
}

@inproceedings{kargerForecastBenchDynamicBenchmark2024,
  title = {{{ForecastBench}}: {{A Dynamic Benchmark}} of {{AI Forecasting Capabilities}}},
  shorttitle = {{{ForecastBench}}},
  booktitle = {The {{Thirteenth International Conference}} on {{Learning Representations}}},
  author = {Karger, Ezra and Bastani, Houtan and {Yueh-Han}, Chen and Jacobs, Zachary and Halawi, Danny and Zhang, Fred and Tetlock, Philip},
  year = {2024},
  month = oct,
  urldate = {2025-08-28},
  abstract = {Forecasts of future events are essential inputs into informed decision-making. Machine learning (ML) systems have the potential to deliver forecasts at scale, but there is no framework for evaluating the accuracy of ML systems on a standardized set of forecasting questions. To address this gap, we introduce ForecastBench: a dynamic benchmark that evaluates the accuracy of ML systems on an automatically generated and regularly updated set of 1,000 forecasting questions. To avoid any possibility of data leakage, ForecastBench is comprised solely of questions about future events that have no known answer at the time of submission. We quantify the capabilities of current ML systems by collecting forecasts from expert (human) forecasters, the general public, and LLMs on a random subset of questions from the benchmark (\$N=200\$). While LLMs have achieved super-human performance on many benchmarks, they perform less well here: expert forecasters outperform the top-performing LLM (\$p\$-value \${$<$}0.001\$). We display system and human scores in a public leaderboard at www.forecastbench.org.},
  langid = {english},
  file = {/home/dmrivers/Zotero/storage/MYTYDRHY/Karger et al. - 2024 - ForecastBench A Dynamic Benchmark of AI Forecasting Capabilities.pdf}
}

@misc{kfwdevelopmentbankIDEaL,
  title = {{{IDEaL}}},
  author = {{KfW Development Bank}},
  urldate = {2025-08-22},
  file = {/home/dmrivers/Zotero/storage/EX7YT4R4/ideal.html}
}

@article{knackStateAIStrategic,
  title = {The {{State}} of {{AI}} for {{Strategic Warning}}},
  author = {Knack, Anna and Balakrishnan, Nandita},
  urldate = {2025-08-22},
  langid = {english},
  note = {Advancements in \href{https://www.politico.eu/article/artificial-intelligence-conflict-war-prediction/}{machine learning},\textsuperscript{\href{https://cetas.turing.ac.uk/publications/state-ai-strategic-warning\#ftn3}{[3]}} \href{https://corescholar.libraries.wright.edu/cgi/viewcontent.cgi?article=1508&context=cse}{deep learning},\textsuperscript{\href{https://cetas.turing.ac.uk/publications/state-ai-strategic-warning\#ftn4}{[4]} }\href{https://www.benradford.com/images/publications/ViEWS_paper_Radford.pdf}{neural networks},\textsuperscript{\href{https://cetas.turing.ac.uk/publications/state-ai-strategic-warning\#ftn5}{[5]} }\href{https://www.researchgate.net/profile/Joost-Van-Oijen/publication/375584489_Empowering_Military_Decision_Support_through_the_Synergy_of_AI_and_Simulation/links/6550ae2fce88b87031df79e5/Empowering-Military-Decision-Support-through-the-Synergy-of-AI-and-Simulation.pdf}{foundation models}\textsuperscript{\href{https://cetas.turing.ac.uk/publications/state-ai-strategic-warning\#ftn6}{[6]} }and \href{https://blogs.icrc.org/law-and-policy/2023/10/24/algorithms-of-war-use-of-artificial-intelligence-decision-making-armed-conflict/}{generative AI}\textsuperscript{\href{https://cetas.turing.ac.uk/publications/state-ai-strategic-warning\#ftn7}{[7] }}all present immense long-term opportunities to increase both the speed and quality of strategic warnings. These benefits could include alerting analysts and policymakers to political events that are about to unfold in an otherwise unmonitored area, and warning them of military aggression in monitored regions faster than human analysts can.},
  file = {/home/dmrivers/Zotero/storage/JUHNF3PF/state-ai-strategic-warning.html}
}

@article{koldunovLocalClimateServices2024,
  title = {Local Climate Services for All, Courtesy of Large Language Models},
  author = {Koldunov, Nikolay and Jung, Thomas},
  year = {2024},
  month = jan,
  journal = {Communications Earth \& Environment},
  volume = {5},
  number = {1},
  pages = {13},
  publisher = {Nature Publishing Group},
  issn = {2662-4435},
  doi = {10.1038/s43247-023-01199-1},
  urldate = {2025-08-24},
  abstract = {Large language models can summarize, aggregate, and convey localized climate-related data to people in a cost-effective and expeditious manner. We have built a simple, proof-of-concept prototype and argue that the approach holds the potential to truly democratize climate information.},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {Climate and Earth system modelling,Climate-change impacts,Scientific community},
  file = {/home/dmrivers/Zotero/storage/JJPNZFVC/Koldunov and Jung - 2024 - Local climate services for all, courtesy of large language models.pdf}
}

@article{lamLearningSkillfulMediumrange2023,
  title = {Learning Skillful Medium-Range Global Weather Forecasting},
  author = {Lam, Remi and {Sanchez-Gonzalez}, Alvaro and Willson, Matthew and Wirnsberger, Peter and Fortunato, Meire and Alet, Ferran and Ravuri, Suman and Ewalds, Timo and {Eaton-Rosen}, Zach and Hu, Weihua and Merose, Alexander and Hoyer, Stephan and Holland, George and Vinyals, Oriol and Stott, Jacklynn and Pritzel, Alexander and Mohamed, Shakir and Battaglia, Peter},
  year = {2023},
  month = dec,
  journal = {Science},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.adi2336},
  urldate = {2025-08-24},
  abstract = {Machine learning leads to better, faster, and cheaper weather forecasting.},
  copyright = {Copyright {\copyright} 2023 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works},
  langid = {english},
  file = {/home/dmrivers/Zotero/storage/92TUET5Z/science.html}
}

@misc{leeAdvancingEventForecasting2025,
  title = {Advancing {{Event Forecasting}} through {{Massive Training}} of {{Large Language Models}}: {{Challenges}}, {{Solutions}}, and {{Broader Impacts}}},
  shorttitle = {Advancing {{Event Forecasting}} through {{Massive Training}} of {{Large Language Models}}},
  author = {Lee, Sang-Woo and Yang, Sohee and Kwak, Donghyun and Siegel, Noah Y.},
  year = {2025},
  month = jul,
  number = {arXiv:2507.19477},
  eprint = {2507.19477},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2507.19477},
  urldate = {2025-08-21},
  abstract = {Many recent papers have studied the development of superforecaster-level event forecasting LLMs. While methodological problems with early studies cast doubt on the use of LLMs for event forecasting, recent studies with improved evaluation methods have shown that state-of-the-art LLMs are gradually reaching superforecaster-level performance, and reinforcement learning has also been reported to improve future forecasting. Additionally, the unprecedented success of recent reasoning models and Deep Research-style models suggests that technology capable of greatly improving forecasting performance has been developed. Therefore, based on these positive recent trends, we argue that the time is ripe for research on large-scale training of superforecaster-level event forecasting LLMs. We discuss two key research directions: training methods and data acquisition. For training, we first introduce three difficulties of LLM-based event forecasting training: noisiness-sparsity, knowledge cut-off, and simple reward structure problems. Then, we present related ideas to mitigate these problems: hypothetical event Bayesian networks, utilizing poorly-recalled and counterfactual events, and auxiliary reward signals. For data, we propose aggressive use of market, public, and crawling datasets to enable large-scale training and evaluation. Finally, we explain how these technical advances could enable AI to provide predictive intelligence to society in broader areas. This position paper presents promising specific paths and considerations for getting closer to superforecaster-level AI technology, aiming to call for researchers' interest in these directions.},
  archiveprefix = {arXiv},
  keywords = {future work; outlook,NEED TO READ},
  note = {Potential Risks of Event Forecasting AI Expansion

\par
Therefore, based on
\par
these positive recent trends, we argue that the time is ripe for research on large-scale training
\par
of superforecaster-level event forecasting LLMs. We discuss two key research directions:
\par
training methods and data acquisition. For training, we first introduce three difficulties
\par
of LLM-based event forecasting training: noisiness-sparsity, knowledge cut-off, and simple
\par
reward structure problems. Then, we present related ideas to mitigate these problems:
\par
hypothetical event Bayesian networks, utilizing poorly-recalled and counterfactual events,
\par
and auxiliary reward signals. F},
  file = {/home/dmrivers/Zotero/storage/DFD9DA4M/Lee et al. - 2025 - Advancing Event Forecasting through Massive Training of Large Language Models Challenges, Solutions.pdf;/home/dmrivers/Zotero/storage/KHVH3SRF/2507.html}
}

@article{lippertCanLargeLanguage2024,
  title = {Can Large Language Models Help Predict Results from a Complex Behavioural Science Study?},
  author = {Lippert, Steffen and Dreber, Anna and Johannesson, Magnus and Tierney, Warren and {Cyrus-Lai}, Wilson and Uhlmann, Eric Luis and Collaboration, Emotion Expression and Pfeiffer, Thomas},
  year = {2024},
  month = sep,
  journal = {Royal Society Open Science},
  publisher = {The Royal Society},
  doi = {10.1098/rsos.240682},
  urldate = {2025-08-22},
  abstract = {We tested whether large language models (LLMs) can help predict results from a complex behavioural science experiment. In study 1, we investigated the performance of the widely used LLMs GPT-3.5 and GPT-4 in forecasting the empirical findings of a large-...},
  copyright = {{\copyright} 2024 The Author(s).},
  langid = {english},
  keywords = {NEED TO READ},
  note = {Lippert et al. (2024) evaluated GPT-4 on a complex behavioral science experiment about emotions and social perception. They reported that GPT-4's forecast of effect sizes achieved a correlation of 0.89 with realized results, versus 0.87 for a group of 119 human experts. In contrast, GPT-3.5 performed no better than chance (r = 0.07), emphasizing the leap in predictive power from GPT-3.5 to GPT-4. Intriguingly, Lippert et al. also found that human forecasters became significantly more accurate when provided access to a GPT-4-powered chatbot for assistance.},
  file = {/home/dmrivers/Zotero/storage/GSH2KF3G/Lippert et al. - 2024 - Can large language models help predict results from a complex behavioural science study.pdf;/home/dmrivers/Zotero/storage/G8GVVTQY/rsos.html}
}

@article{lizkaSummaryTakeawaysHansons2021,
  title = {Summary and {{Takeaways}}: {{Hanson}}'s ``{{Shall We Vote}} on {{Values}}, {{But Bet}} on {{Beliefs}}?''},
  shorttitle = {Summary and {{Takeaways}}},
  author = {Lizka},
  year = {2021},
  month = aug,
  urldate = {2025-08-20},
  abstract = {Introduction Robin Hanson's ``Shall We Vote on Values, But Bet on Beliefs?''[1] is a foundational paper that outlines some ways to use prediction marke{\dots}},
  langid = {english},
  keywords = {future work; outlook},
  note = {One of the major issues with Futarchy for public policy is that defining a welfare function that is universally acceptable is very difficult. Unfortunately, this is not addressed by replacing the prediction markets with AI.~
\par
Another issue with Futarchy is the information gotten when a certain vote resolves in a given direction (e.g. when a vote for whether lockdowns will increase covid, futarchy has the problem that the information that there are going to be lockdowns is itself baked into the betting of voters). See the comment by linch in this post regarding Covid lockdowns: https://forum.effectivealtruism.org/posts/ijohdoDbPvdeXMpiz/summary-and-takeaways-hanson-s-shall-we-vote-on-values-but
\par
As you mention, one issue AI solves with Futarchy is thin markets - very little liquidity in the prediction market. AI does not have this problem, so AI-based decision making could be used on a micro scale and on an organizational or even personal level. This could aid uptake of the system on small scales.
\par
One issue AI introduces when being used to replace prediction markets with Futarchy is that while markets have natural tendencies to correct manipulation (market automatically adjusts if a policy is manipulated), the development of AI is currently highly centralized. Manipulation in training a model is relatively easy, and specific training data could be introduced to influence the forecast and predictions of policies. There may be ways to get "skin in the game" to correct AI forecasts, such as monetary incentives to detect biases in training or parameters of models - these may be aided by interpretability techniques.},
  file = {/home/dmrivers/Zotero/storage/KV4K5G4P/summary-and-takeaways-hanson-s-shall-we-vote-on-values-but.html}
}

@misc{luAIScientistFully2024,
  title = {The {{AI Scientist}}: {{Towards Fully Automated Open-Ended Scientific Discovery}}},
  shorttitle = {The {{AI Scientist}}},
  author = {Lu, Chris and Lu, Cong and Lange, Robert Tjarko and Foerster, Jakob and Clune, Jeff and Ha, David},
  year = {2024},
  month = sep,
  number = {arXiv:2408.06292},
  eprint = {2408.06292},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.06292},
  urldate = {2025-08-19},
  abstract = {One of the grand challenges of artificial general intelligence is developing agents capable of conducting scientific research and discovering new knowledge. While frontier models have already been used as aides to human scientists, e.g. for brainstorming ideas, writing code, or prediction tasks, they still conduct only a small part of the scientific process. This paper presents the first comprehensive framework for fully automatic scientific discovery, enabling frontier large language models to perform research independently and communicate their findings. We introduce The AI Scientist, which generates novel research ideas, writes code, executes experiments, visualizes results, describes its findings by writing a full scientific paper, and then runs a simulated review process for evaluation. In principle, this process can be repeated to iteratively develop ideas in an open-ended fashion, acting like the human scientific community. We demonstrate its versatility by applying it to three distinct subfields of machine learning: diffusion modeling, transformer-based language modeling, and learning dynamics. Each idea is implemented and developed into a full paper at a cost of less than \$15 per paper. To evaluate the generated papers, we design and validate an automated reviewer, which we show achieves near-human performance in evaluating paper scores. The AI Scientist can produce papers that exceed the acceptance threshold at a top machine learning conference as judged by our automated reviewer. This approach signifies the beginning of a new era in scientific discovery in machine learning: bringing the transformative benefits of AI agents to the entire research process of AI itself, and taking us closer to a world where endless affordable creativity and innovation can be unleashed on the world's most challenging problems. Our code is open-sourced at https://github.com/SakanaAI/AI-Scientist},
  archiveprefix = {arXiv},
  keywords = {future work; outlook},
  file = {/home/dmrivers/Zotero/storage/3GSR8J55/2408.html}
}

@misc{luEvaluatingLLMsRealWorld2025,
  title = {Evaluating {{LLMs}} on {{Real-World Forecasting Against Expert Forecasters}}},
  author = {Lu, Janna},
  year = {2025},
  month = aug,
  number = {arXiv:2507.04562},
  eprint = {2507.04562},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2507.04562},
  urldate = {2025-08-21},
  abstract = {Large language models (LLMs) have demonstrated remarkable capabilities across diverse tasks, but their ability to forecast future events remains understudied. A year ago, large language models struggle to come close to the accuracy of a human crowd. I evaluate state-of-the-art LLMs on 464 forecasting questions from Metaculus, comparing their performance against top forecasters. Frontier models achieve Brier scores that ostensibly surpass the human crowd but still significantly underperform a group of experts.},
  archiveprefix = {arXiv},
  keywords = {AI forecasting attempts,future work; outlook,NEED TO READ},
  note = {Extends Halawi et\,al. by testing LLMs on real forecasting questions and comparing against superforecasters; useful for your ``human crowd vs LLM'' baseline design. \href{https://arxiv.org/abs/2507.04562}{https://arxiv.org/abs/2507.04562}
\par
Has plot showing may 2027 is when we beat human experts at superforecasting},
  file = {/home/dmrivers/Zotero/storage/JF25GSXI/Lu - 2025 - Evaluating LLMs on Real-World Forecasting Against Expert Forecasters.pdf;/home/dmrivers/Zotero/storage/28MSL8HK/2507.html}
}

@article{lyuCalibratingLargeLanguage2025,
  title = {Calibrating {{Large Language Models}} with {{Sample Consistency}}},
  author = {Lyu, Qing and Shridhar, Kumar and Malaviya, Chaitanya and Zhang, Li and Elazar, Yanai and Tandon, Niket and Apidianaki, Marianna and Sachan, Mrinmaya and {Callison-Burch}, Chris},
  year = {2025},
  month = apr,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {39},
  number = {18},
  pages = {19260--19268},
  issn = {2374-3468},
  doi = {10.1609/aaai.v39i18.34120},
  urldate = {2025-08-24},
  abstract = {Accurately gauging the confidence level of Large Language Models' (LLMs) predictions is pivotal for their reliable application. However, LLMs are often uncalibrated inherently and elude conventional calibration techniques due to their proprietary nature and massive scale. In this work, we derive model confidence from the distribution of multiple randomly sampled generations, using three measures of consistency. We extensively evaluate eleven open and closed-source models on nine reasoning datasets. Results show that consistency-based calibration methods outperform existing post-hoc approaches in terms of calibration error. Meanwhile, we find that factors such as intermediate explanations, model scaling, and larger sample sizes enhance calibration, while instruction-tuning makes calibration more difficult. Moreover, confidence scores obtained from consistency can potentially enhance model performance. Finally, we offer guidance on choosing suitable consistency metrics for calibration, tailored to model characteristics such as the exposure to instruction-tuning and RLHF.},
  copyright = {Copyright (c) 2025 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  file = {/home/dmrivers/Zotero/storage/29THZPLX/Lyu et al. - 2025 - Calibrating Large Language Models with Sample Consistency.pdf}
}

@article{mellersIdentifyingCultivatingSuperforecasters2015,
  title = {Identifying and Cultivating Superforecasters as a Method of Improving Probabilistic Predictions},
  author = {Mellers, Barbara and Stone, Eric and Murray, Terry and Minster, Angela and Rohrbaugh, Nick and Bishop, Michael and Chen, Eva and Baker, Joshua and Hou, Yuan and Horowitz, Michael and Ungar, Lyle and Tetlock, Philip},
  year = {2015},
  journal = {Perspectives on Psychological Science},
  volume = {10},
  number = {3},
  pages = {267--281},
  publisher = {Sage Publications},
  address = {US},
  issn = {1745-6924},
  doi = {10.1177/1745691615577794},
  abstract = {Across a wide range of tasks, research has shown that people make poor probabilistic predictions of future events. Recently, the U.S. Intelligence Community sponsored a series of forecasting tournaments designed to explore the best strategies for generating accurate subjective probability estimates of geopolitical events. In this article, we describe the winning strategy: culling off top performers each year and assigning them into elite teams of superforecasters. Defying expectations of regression toward the mean 2 years in a row, superforecasters maintained high accuracy across hundreds of questions and a wide array of topics. We find support for four mutually reinforcing explanations of superforecaster performance: (a) cognitive abilities and styles, (b) task-specific skills, (c) motivation and commitment, and (d) enriched environments. These findings suggest that superforecasters are partly discovered and partly created---and that the high-performance incentives of tournaments highlight aspects of human judgment that would not come to light in laboratory paradigms focused on typical performance. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  keywords = {methods},
  file = {/home/dmrivers/Zotero/storage/V9FRJJUJ/2015-24834-001.html}
}

@article{mellersIdentifyingCultivatingSuperforecasters2015a,
  title = {Identifying and Cultivating Superforecasters as a Method of Improving Probabilistic Predictions},
  author = {Mellers, Barbara and Stone, Eric and Murray, Terry and Minster, Angela and Rohrbaugh, Nick and Bishop, Michael and Chen, Eva and Baker, Joshua and Hou, Yuan and Horowitz, Michael and Ungar, Lyle and Tetlock, Philip},
  year = {2015},
  month = may,
  journal = {Perspectives on Psychological Science: A Journal of the Association for Psychological Science},
  volume = {10},
  number = {3},
  pages = {267--281},
  issn = {1745-6924},
  doi = {10.1177/1745691615577794},
  abstract = {Across a wide range of tasks, research has shown that people make poor probabilistic predictions of future events. Recently, the U.S. Intelligence Community sponsored a series of forecasting tournaments designed to explore the best strategies for generating accurate subjective probability estimates of geopolitical events. In this article, we describe the winning strategy: culling off top performers each year and assigning them into elite teams of superforecasters. Defying expectations of regression toward the mean 2 years in a row, superforecasters maintained high accuracy across hundreds of questions and a wide array of topics. We find support for four mutually reinforcing explanations of superforecaster performance: (a) cognitive abilities and styles, (b) task-specific skills, (c) motivation and commitment, and (d) enriched environments. These findings suggest that superforecasters are partly discovered and partly created-and that the high-performance incentives of tournaments highlight aspects of human judgment that would not come to light in laboratory paradigms focused on typical performance.},
  langid = {english},
  pmid = {25987508},
  keywords = {Area Under Curve,Cognition,Environment,expertise,Forecasting,forecasts,Humans,Judgment,Learning,Models Psychological,Motivation,predictions,Probability,probability training,ROC Curve,teams,Time}
}

@misc{MITEmissionsPrediction,
  title = {The {{MIT Emissions Prediction}} and {{Policy Analysis}} ({{EPPA}}) {{Model}}: {{Version}} 4 {\textbar} {{MIT CS3}}},
  shorttitle = {The {{MIT Emissions Prediction}} and {{Policy Analysis}} ({{EPPA}}) {{Model}}},
  urldate = {2025-08-19},
  howpublished = {https://cs3.mit.edu/publication/14578},
  langid = {english},
  keywords = {AI forecasting attempts,future work; outlook,introduction,NEED TO READ,sustainability},
  note = {``Existing efforts I could identify focus on manually entering assumptions of the effects of policies into models of the world economy, such as the The MIT Emissions Prediction and Policy Analysis (EPPA) Model ``
\par
EPPA is a recursive-dynamic multi-regional general equilibrium model of the world economy, which is built on the GTAP dataset and additional data for the greenhouse gas and urban gas emissions.},
  file = {/home/dmrivers/Zotero/storage/5Y4NLGNJ/14578.html}
}

@inproceedings{nadeemStereoSetMeasuringStereotypical2021,
  title = {{{StereoSet}}: {{Measuring}} Stereotypical Bias in Pretrained Language Models},
  shorttitle = {{{StereoSet}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Nadeem, Moin and Bethke, Anna and Reddy, Siva},
  year = {2021},
  month = aug,
  pages = {5356--5371},
  doi = {10.18653/v1/2021.acl-long.416},
  urldate = {2025-08-31},
  abstract = {Moin Nadeem, Anna Bethke, Siva Reddy. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.},
  langid = {american},
  file = {/home/dmrivers/Zotero/storage/YWDGIQGF/Nadeem et al. - 2021 - StereoSet Measuring stereotypical bias in pretrained language models.pdf}
}

@misc{OpenAlexTopicClassification,
  title = {{{OpenAlex Topic Classification Whitepaper}}.Docx},
  journal = {Google Docs},
  urldate = {2025-08-19},
  abstract = {OpenAlex: End-to-End Process for Topic Classification This article describes the process that OpenAlex uses to assign topics to works in our database. This system was built on top of the recently released classifications from CWTS (An open approach for classifying research publications) which pro...},
  howpublished = {https://docs.google.com/document/d/1bDopkhuGieQ4F8gGNj7sEc8WSE8mvLZS/edit?usp=sharing\&ouid=106329373929967149989\&rtpof=true\&sd=true\&usp=embed\_facebook},
  langid = {english},
  keywords = {methods},
  file = {/home/dmrivers/Zotero/storage/2TVZVTUK/edit.html}
}

@article{OSeMOSYSOpenSource2011,
  title = {{{OSeMOSYS}}: {{The Open Source Energy Modeling System}}: {{An}} Introduction to Its Ethos, Structure and Development},
  shorttitle = {{{OSeMOSYS}}},
  year = {2011},
  month = oct,
  journal = {Energy Policy},
  volume = {39},
  number = {10},
  pages = {5850--5870},
  publisher = {Elsevier},
  issn = {0301-4215},
  doi = {10.1016/j.enpol.2011.06.033},
  urldate = {2025-08-24},
  abstract = {This paper discusses the design and development of the Open Source Energy Modeling System (OSeMOSYS). It describes the model's formulation in terms of{\dots}},
  langid = {american},
  file = {/home/dmrivers/Zotero/storage/LTYBMS77/S0301421511004897.html}
}

@misc{palekaPitfallsEvaluatingLanguage2025,
  title = {Pitfalls in {{Evaluating Language Model Forecasters}}},
  author = {Paleka, Daniel and Goel, Shashwat and Geiping, Jonas and Tram{\`e}r, Florian},
  year = {2025},
  month = may,
  number = {arXiv:2506.00723},
  eprint = {2506.00723},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2506.00723},
  urldate = {2025-08-21},
  abstract = {Large language models (LLMs) have recently been applied to forecasting tasks, with some works claiming these systems match or exceed human performance. In this paper, we argue that, as a community, we should be careful about such conclusions as evaluating LLM forecasters presents unique challenges. We identify two broad categories of issues: (1) difficulty in trusting evaluation results due to many forms of temporal leakage, and (2) difficulty in extrapolating from evaluation performance to real-world forecasting. Through systematic analysis and concrete examples from prior work, we demonstrate how evaluation flaws can raise concerns about current and future performance claims. We argue that more rigorous evaluation methodologies are needed to confidently assess the forecasting abilities of LLMs.},
  archiveprefix = {arXiv},
  keywords = {methods,NEED TO READ},
  note = {Methodological cautions (latent confounders like election outcomes, leakage, horizon leakage) and fixes for building clean, out-of-distribution tests; directly actionable for your evaluation pipeline. \href{https://arxiv.org/abs/2506.00723}{https://arxiv.org/abs/2506.00723}},
  file = {/home/dmrivers/Zotero/storage/829XH4WA/Paleka et al. - 2025 - Pitfalls in Evaluating Language Model Forecasters.pdf;/home/dmrivers/Zotero/storage/JXAE76EB/2506.html}
}

@misc{priemOpenAlexFullyopenIndex2022,
  title = {{{OpenAlex}}: {{A}} Fully-Open Index of Scholarly Works, Authors, Venues, Institutions, and Concepts},
  shorttitle = {{{OpenAlex}}},
  author = {Priem, Jason and Piwowar, Heather and Orr, Richard},
  year = {2022},
  month = jun,
  number = {arXiv:2205.01833},
  eprint = {2205.01833},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2205.01833},
  urldate = {2025-08-19},
  abstract = {OpenAlex is a new, fully-open scientific knowledge graph (SKG), launched to replace the discontinued Microsoft Academic Graph (MAG). It contains metadata for 209M works (journal articles, books, etc); 2013M disambiguated authors; 124k venues (places that host works, such as journals and online repositories); 109k institutions; and 65k Wikidata concepts (linked to works via an automated hierarchical multi-tag classifier). The dataset is fully and freely available via a web-based GUI, a full data dump, and high-volume REST API. The resource is under active development and future work will improve accuracy and coverage of citation information and author/institution parsing and deduplication.},
  archiveprefix = {arXiv},
  keywords = {methods},
  note = {Comment: Submitted to the 26th International Conference on Science, Technology and Innovation Indicators (STI 2022)},
  file = {/home/dmrivers/Zotero/storage/FHKF5S8C/Priem et al. - 2022 - OpenAlex A fully-open index of scholarly works, authors, venues, institutions, and concepts.pdf;/home/dmrivers/Zotero/storage/JL2FRNF9/2205.html}
}

@misc{renCanLLMImprove2025,
  title = {Can {{LLM Improve}} for {{Expert Forecast Combination}}? {{Evidence}} from the {{European Central Bank Survey}}},
  shorttitle = {Can {{LLM Improve}} for {{Expert Forecast Combination}}?},
  author = {Ren, Yinuo and Wang, Jue},
  year = {2025},
  month = jun,
  number = {arXiv:2506.23154},
  eprint = {2506.23154},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2506.23154},
  urldate = {2025-08-22},
  abstract = {This template helps you to create a properly formatted LATEX manuscript. {\textbackslash}beginabstract . . . {\textbackslash}endabstract and {\textbackslash}begin\{keyword\} ... {\textbackslash}end\{keyword\} which contain the abstract and keywords respectively. Each keyword shall be separated by a {\textbackslash}sep command.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/dmrivers/Zotero/storage/2QFITMXV/Ren and Wang - 2025 - Can LLM Improve for Expert Forecast Combination Evidence from the European Central Bank Survey.pdf}
}

@article{schoeneggerAIAugmentedPredictionsLLM2025,
  title = {{{AI-Augmented Predictions}}: {{LLM Assistants Improve Human Forecasting Accuracy}}},
  shorttitle = {{{AI-Augmented Predictions}}},
  author = {Schoenegger, Philipp and Park, Peter S. and Karger, Ezra and Trott, Sean and Tetlock, Philip E.},
  year = {2025},
  month = mar,
  journal = {ACM Transactions on Interactive Intelligent Systems},
  volume = {15},
  number = {1},
  pages = {1--25},
  issn = {2160-6455, 2160-6463},
  doi = {10.1145/3707649},
  urldate = {2025-08-21},
  abstract = {Large language models (LLMs) match and sometimes exceed human performance in many domains. This study explores the potential of LLMs to augment human judgment in a forecasting task. We evaluate the effect on human forecasters of two LLM assistants: one designed to provide high-quality (``superforecasting'') advice, and the other designed to be overconfident and base-rate neglecting, thus providing noisy forecasting advice. We compare participants using these assistants to a control group that received a less advanced model that did not provide numerical predictions or engage in explicit discussion of predictions. Participants (               N                                {\textbackslash}(={\textbackslash})                              991) answered a set of six forecasting questions and had the option to consult their assigned LLM assistant throughout. Our preregistered analyses show that interacting with each of our frontier LLM assistants significantly enhances prediction accuracy by between 24\% and 28\% compared to the control group. Exploratory analyses showed a pronounced outlier effect in one forecasting item, without which we find that the superforecasting assistant increased accuracy by 41\%, compared with 29\% for the noisy assistant. We further examine whether LLM forecasting augmentation disproportionately benefits less skilled forecasters, degrades the wisdom-of-the-crowd by reducing prediction diversity, or varies in effectiveness with question difficulty. Our data do not consistently support these hypotheses. Our results suggest that access to a frontier LLM assistant, even a noisy one, can be a helpful decision aid in cognitively demanding tasks compared to a less powerful model that does not provide specific forecasting advice. However, the effects of outliers suggest that further research into the robustness of this pattern is needed.},
  langid = {english},
  file = {/home/dmrivers/Zotero/storage/KKARDNUQ/Schoenegger et al. - 2025 - AI-Augmented Predictions LLM Assistants Improve Human Forecasting Accuracy.pdf}
}

@misc{schoeneggerLargeLanguageModel2023,
  title = {Large {{Language Model Prediction Capabilities}}: {{Evidence}} from a {{Real-World Forecasting Tournament}}},
  shorttitle = {Large {{Language Model Prediction Capabilities}}},
  author = {Schoenegger, Philipp and Park, Peter S.},
  year = {2023},
  month = oct,
  number = {arXiv:2310.13014},
  eprint = {2310.13014},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.13014},
  urldate = {2025-08-19},
  abstract = {Accurately predicting the future would be an important milestone in the capabilities of artificial intelligence. However, research on the ability of large language models to provide probabilistic predictions about future events remains nascent. To empirically test this ability, we enrolled OpenAI's state-of-the-art large language model, GPT-4, in a three-month forecasting tournament hosted on the Metaculus platform. The tournament, running from July to October 2023, attracted 843 participants and covered diverse topics including Big Tech, U.S. politics, viral outbreaks, and the Ukraine conflict. Focusing on binary forecasts, we show that GPT-4's probabilistic forecasts are significantly less accurate than the median human-crowd forecasts. We find that GPT-4's forecasts did not significantly differ from the no-information forecasting strategy of assigning a 50\% probability to every question. We explore a potential explanation, that GPT-4 might be predisposed to predict probabilities close to the midpoint of the scale, but our data do not support this hypothesis. Overall, we find that GPT-4 significantly underperforms in real-world predictive tasks compared to median human-crowd forecasts. A potential explanation for this underperformance is that in real-world forecasting tournaments, the true answers are genuinely unknown at the time of prediction; unlike in other benchmark tasks like professional exams or time series forecasting, where strong performance may at least partly be due to the answers being memorized from the training data. This makes real-world forecasting tournaments an ideal environment for testing the generalized reasoning and prediction capabilities of artificial intelligence going forward.},
  archiveprefix = {arXiv},
  keywords = {AI forecasting attempts},
  note = {``Schoenegger and Park (2023) and Abolghasemi et al. (2023) evaluated GPT-4 and other LLMs on forecasting tournaments and found that they underperform the human crowd. This observation is in line with ours in Section 3.3. Unlike us, they make little efforts to improve these LMs on forecasting.'' (Halawi et al., 2024, p. 3)

Comment: 13 pages, six visualizations (four figures, two tables)},
  file = {/home/dmrivers/Zotero/storage/WE6XMKI8/Schoenegger and Park - 2023 - Large Language Model Prediction Capabilities Evidence from a Real-World Forecasting Tournament.pdf;/home/dmrivers/Zotero/storage/I4LTDFYS/2310.html}
}

@misc{schoeneggerPromptEngineeringLarge2025,
  title = {Prompt {{Engineering Large Language Models}}' {{Forecasting Capabilities}}},
  author = {Schoenegger, Philipp and Jones, Cameron R. and Tetlock, Philip E. and Mellers, Barbara},
  year = {2025},
  month = jun,
  number = {arXiv:2506.01578},
  eprint = {2506.01578},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2506.01578},
  urldate = {2025-08-21},
  abstract = {Large language model performance can be improved in a large number of ways. Many such techniques, like fine-tuning or advanced tool usage, are time-intensive and expensive. Although prompt engineering is significantly cheaper and often works for simpler tasks, it remains unclear whether prompt engineering suffices for more complex domains like forecasting. Here we show that small prompt modifications rarely boost forecasting accuracy beyond a minimal baseline. In our first study, we tested 38 prompts across Claude 3.5 Sonnet, Claude 3.5 Haiku, GPT-4o, and Llama 3.1 405B. In our second, we introduced compound prompts and prompts from external sources, also including the reasoning models o1 and o1-mini. Our results show that most prompts lead to negligible gains, although references to base rates yield slight benefits. Surprisingly, some strategies showed strong negative effects on accuracy: especially encouraging the model to engage in Bayesian reasoning. These results suggest that, in the context of complex tasks like forecasting, basic prompt refinements alone offer limited gains, implying that more robust or specialized techniques may be required for substantial performance improvements in AI forecasting.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/dmrivers/Zotero/storage/45EAGCKE/Schoenegger et al. - 2025 - Prompt Engineering Large Language Models' Forecasting Capabilities.pdf;/home/dmrivers/Zotero/storage/LIHR42BM/2506.html}
}

@article{silvestroImprovingBiodiversityProtection2022,
  title = {Improving Biodiversity Protection through Artificial Intelligence},
  author = {Silvestro, Daniele and Goria, Stefano and Sterner, Thomas and Antonelli, Alexandre},
  year = {2022},
  month = may,
  journal = {Nature Sustainability},
  volume = {5},
  number = {5},
  pages = {415--424},
  publisher = {Nature Publishing Group},
  issn = {2398-9629},
  doi = {10.1038/s41893-022-00851-6},
  urldate = {2025-08-24},
  abstract = {Over a million species face extinction, highlighting the urgent need for conservation policies that maximize the protection of biodiversity to sustain its manifold contributions to people's lives. Here we present a novel framework for spatial conservation prioritization based on reinforcement learning that consistently outperforms available state-of-the-art software using simulated and empirical data. Our methodology, conservation area prioritization through artificial intelligence (CAPTAIN), quantifies the trade-off between the costs and benefits of area and biodiversity protection, allowing the exploration of multiple biodiversity metrics. Under a limited budget, our model protects significantly more species from extinction than areas selected randomly or naively (such as based on species richness). CAPTAIN achieves substantially better solutions with empirical data than alternative software, meeting conservation targets more reliably and generating more interpretable prioritization maps. Regular biodiversity monitoring, even with a degree of inaccuracy characteristic of citizen science surveys, further improves biodiversity outcomes. Artificial intelligence holds great promise for improving the conservation and sustainable use of biological and ecosystem values in a rapidly changing and resource-limited world.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Biodiversity,Climate-change ecology,Conservation biology,Environmental economics,Sustainability},
  file = {/home/dmrivers/Zotero/storage/69FWGB5M/Silvestro et al. - 2022 - Improving biodiversity protection through artificial intelligence.pdf}
}

@misc{SimulatingPublicOpinion,
  title = {Simulating {{Public Opinion}}: {{Comparing Distributional}} and {{Individual-Level Predictions}} from {{LLMs}} and {{Random Forests}}[v1] {\textbar} {{Preprints}}.Org},
  urldate = {2025-08-19},
  howpublished = {https://www.preprints.org/manuscript/202507.0531/v1},
  file = {/home/dmrivers/Zotero/storage/D46PD6EC/v1.html}
}

@article{stechemesserClimatePoliciesThat2024,
  title = {Climate Policies That Achieved Major Emission Reductions: {{Global}} Evidence from Two Decades},
  shorttitle = {Climate Policies That Achieved Major Emission Reductions},
  author = {Stechemesser, Annika and Koch, Nicolas and Mark, Ebba and Dilger, Elina and Kl{\"o}sel, Patrick and Menicacci, Laura and Nachtigall, Daniel and Pretis, Felix and Ritter, Nolan and Schwarz, Moritz and Vossen, Helena and Wenzel, Anna},
  year = {2024},
  month = aug,
  journal = {Science (New York, N.Y.)},
  volume = {385},
  number = {6711},
  pages = {884--892},
  issn = {1095-9203},
  doi = {10.1126/science.adl6547},
  abstract = {Meeting the Paris Agreement's climate targets necessitates better knowledge about which climate policies work in reducing emissions at the necessary scale. We provide a global, systematic ex post evaluation to identify policy combinations that have led to large emission reductions out of 1500 climate policies implemented between 1998 and 2022 across 41 countries from six continents. Our approach integrates a comprehensive climate policy database with a machine learning-based extension of the common difference-in-differences approach. We identified 63 successful policy interventions with total emission reductions between 0.6 billion and 1.8 billion metric tonnes CO2. Our insights on effective but rarely studied policy combinations highlight the important role of price-based instruments in well-designed policy mixes and the policy efforts necessary for closing the emissions gap.},
  langid = {english},
  pmid = {39172830}
}

@book{tetlockSuperforecastingArtScience2015,
  title = {Superforecasting: {{The}} Art and Science of Prediction},
  shorttitle = {Superforecasting},
  author = {Tetlock, Philip E. and Gardner, Dan},
  year = {2015},
  series = {Superforecasting: {{The}} Art and Science of Prediction},
  pages = {340},
  publisher = {Crown Publishers/Random House},
  address = {New York, NY, US},
  abstract = {Everyone would benefit from seeing further into the future, whether buying stocks, crafting policy, launching a new product, or simply planning the week's meals. Unfortunately, people tend to be terrible forecasters. As Wharton professor Philip Tetlock showed in a landmark 2005 study, even experts' predictions are only slightly better than chance. However, an important and underreported conclusion of that study was that some experts do have real foresight, and Tetlock has spent the past decade trying to figure out why. What makes some people so good? And can this talent be taught? In Superforecasting, Tetlock and coauthor Dan Gardner offer a masterwork on prediction, drawing on decades of research and the results of a massive, government-funded forecasting tournament. The Good Judgment Project involves tens of thousands of ordinary people---including a Brooklyn filmmaker, a retired pipe installer, and a former ballroom dancer---who set out to forecast global events. Some of the volunteers have turned out to be astonishingly good. They've beaten other benchmarks, competitors, and prediction markets. They've even beaten the collective judgment of intelligence analysts with access to classified information. They are "superforecasters." In this groundbreaking and accessible book, Tetlock and Gardner show us how we can learn from this elite group. Weaving together stories of forecasting successes (the raid on Osama bin Laden's compound) and failures (the Bay of Pigs) and interviews with a range of high-level decision makers, from David Petraeus to Robert Rubin, they show that good forecasting doesn't require powerful computers or arcane methods. It involves gathering evidence from a variety of sources, thinking probabilistically, working in teams, keeping score, and being willing to admit error and change course. Superforecasting offers the first demonstrably effective way to improve our ability to predict the future---whether in business, finance, politics, international affairs, or daily life---and is destined to become a modern classic. (PsycInfo Database Record (c) 2025 APA, all rights reserved)},
  isbn = {978-0-8041-3669-3 978-0-8041-3670-9},
  keywords = {introduction},
  file = {/home/dmrivers/Zotero/storage/46MRWBLU/2015-22864-000.html}
}

@article{toetzkeMonitoringGlobalDevelopment2022,
  title = {Monitoring Global Development Aid with Machine Learning},
  author = {Toetzke, Malte and Banholzer, Nicolas and Feuerriegel, Stefan},
  year = {2022},
  month = jun,
  journal = {Nature Sustainability},
  volume = {5},
  number = {6},
  pages = {533--541},
  publisher = {Nature Publishing Group},
  issn = {2398-9629},
  doi = {10.1038/s41893-022-00874-z},
  urldate = {2025-08-23},
  abstract = {Monitoring global development aid provides important evidence for policymakers financing the Sustainable Development Goals (SDGs). To overcome the limitations of existing monitoring, we develop a machine learning framework that enables a comprehensive and granular categorization of development aid activities based on their textual descriptions. Specifically, we cluster the descriptions of {\textasciitilde}3.2 million aid activities conducted between 2000 and 2019 totalling US\$2.8 trillion. As a result, we generated 173 activity clusters representing the topics of underlying aid activities. Among them, 70 activity clusters cover topics that have not yet been analysed empirically (for example, greenhouse gas emissions reduction and maternal health care). On the basis of our activity clusters, global development aid can be monitored for new topics and at new levels of granularity, allowing the identification of unexplored spatio-temporal disparities. Our framework can be adopted by development finance and policy institutions to promote evidence-based decisions targeting the SDGs.},
  copyright = {2022 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Climate change,Economics,Politics},
  file = {/home/dmrivers/Zotero/storage/HAJPBFDK/Toetzke et al. - 2022 - Monitoring global development aid with machine learning.pdf}
}

@misc{touvronLlama2Open2023,
  title = {Llama 2: {{Open Foundation}} and {{Fine-Tuned Chat Models}}},
  shorttitle = {Llama 2},
  author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
  year = {2023},
  month = jul,
  number = {arXiv:2307.09288},
  eprint = {2307.09288},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.09288},
  urldate = {2025-08-19},
  abstract = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.},
  archiveprefix = {arXiv},
  keywords = {methods},
  file = {/home/dmrivers/Zotero/storage/ICX468XJ/2307.html}
}

@inproceedings{turpinLanguageModelsDont2023,
  title = {Language {{Models Don}}'t {{Always Say What They Think}}: {{Unfaithful Explanations}} in {{Chain-of-Thought Prompting}}},
  shorttitle = {Language {{Models Don}}'t {{Always Say What They Think}}},
  booktitle = {Thirty-Seventh {{Conference}} on {{Neural Information Processing Systems}}},
  author = {Turpin, Miles and Michael, Julian and Perez, Ethan and Bowman, Samuel R.},
  year = {2023},
  month = nov,
  urldate = {2025-08-31},
  abstract = {Large Language Models (LLMs) can achieve strong performance on many tasks by producing step-by-step reasoning before giving a final output, often referred to as chain-of-thought reasoning (CoT). It is tempting to interpret these CoT explanations as the LLM's process for solving a task. This level of transparency into LLMs' predictions would yield significant safety benefits. However, we find that CoT explanations can systematically misrepresent the true reason for a model's prediction. We demonstrate that CoT explanations can be heavily influenced by adding biasing features to model inputs---e.g., by reordering the multiple-choice options in a few-shot prompt to make the answer always "(A)"---which models systematically fail to mention in their explanations. When we bias models toward incorrect answers, they frequently generate CoT explanations rationalizing those answers. This causes accuracy to drop by as much as 36\% on a suite of 13 tasks from BIG-Bench Hard, when testing with GPT-3.5 from OpenAI and Claude 1.0 from Anthropic. On a social-bias task, model explanations justify giving answers in line with stereotypes without mentioning the influence of these social biases. Our findings indicate that CoT explanations can be plausible yet misleading, which risks increasing our trust in LLMs without guaranteeing their safety. Building more transparent and explainable systems will require either improving CoT faithfulness through targeted efforts or abandoning CoT in favor of alternative methods.},
  langid = {english},
  file = {/home/dmrivers/Zotero/storage/X4DXLF3T/Turpin et al. - 2023 - Language Models Don't Always Say What They Think Unfaithful Explanations in Chain-of-Thought Prompt.pdf}
}

@misc{turtelLLMsCanTeach2025,
  title = {{{LLMs Can Teach Themselves}} to {{Better Predict}} the {{Future}}},
  author = {Turtel, Benjamin and Franklin, Danny and Schoenegger, Philipp},
  year = {2025},
  month = feb,
  number = {arXiv:2502.05253},
  eprint = {2502.05253},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.05253},
  urldate = {2025-08-29},
  abstract = {We present an outcome-driven fine-tuning framework that enhances the forecasting capabilities of large language models (LLMs) without relying on human-curated reasoning samples. Our method leverages model self-play to generate pairs of diverse reasoning trajectories and probabilistic forecasts for a set of diverse questions that resolve after the models' knowledge cutoff date. We then rank pairs of these reasoning traces by their distance to the actual outcomes before fine-tuning the model via Direct Preference Optimization (DPO). On a separate test set, our approach increases prediction accuracy of Phi-4 14B and DeepSeek-R1 14B by between 7--10{\textbackslash}\% over a base model and a DPO fine-tuned control model with randomized labels, bringing them on par with forecasting capabilities of much larger frontier models like GPT-4o.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/dmrivers/Zotero/storage/WBAXB86Y/Turtel et al. - 2025 - LLMs Can Teach Themselves to Better Predict the Future.pdf;/home/dmrivers/Zotero/storage/RWHNX84K/2502.html}
}

@misc{turtelOutcomebasedReinforcementLearning2025,
  title = {Outcome-Based {{Reinforcement Learning}} to {{Predict}} the {{Future}}},
  author = {Turtel, Benjamin and Franklin, Danny and Skotheim, Kris and Hewitt, Luke and Schoenegger, Philipp},
  year = {2025},
  month = jul,
  number = {arXiv:2505.17989},
  eprint = {2505.17989},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.17989},
  urldate = {2025-08-21},
  abstract = {Reinforcement Learning with Verifiable Rewards (RLVR) has been an effective approach for improving Large Language Models' reasoning in domains such as coding and mathematics. Here, we apply RLVR methods towards forecasting future real-world events - a challenging task for RL due to the very noisy (and delayed) outcomes involved. Using a novel dataset of recent questions from a prediction market, and accompanying relevant news headlines, we show that a compact (14B) reasoning model can be trained to match or surpass the predictive accuracy of frontier models like o1, while greatly improving probabilistic calibration. The model's performance is also practically meaningful: in a Polymarket trading simulation, we estimate that its bets would have yielded a return on investment of over 10\% across all questions in the test set. We detail and compare approaches used in training our model, including augmenting our training-data with synthetic prediction questions, guardrails for learning stability, and median prediction sampling at inference-time.},
  archiveprefix = {arXiv},
  keywords = {AI forecasting attempts,NEED TO READ},
  note = {LOG PROB MAXING

Adapts GRPO/ReMax to forecasting; reports 14B model matching frontier accuracy and better calibration plus hypothetical prediction-market performance. \href{https://arxiv.org/abs/2505.17989}{https://arxiv.org/abs/2505.17989}
\par
In this work, we design and empirically validate a stable RL pipeline for probabilistic forecasting,
\par
On the algorithmic side, we (i) remove per-question standard-deviation scaling from Group Relative
\par
Policy Optimisation (GRPO), (ii) switch to baseline-subtracted advantages in ReMax(Li et al.,
\par
2023), and (iii) add lightweight guard-rails, token-length limits, a gibberish filter, and an early-
\par
stop criterion, to keep gradients proportional to Brier loss and prevent collapse over more than
\par
100,000 sequential events. On the evaluation side, we collect a novel dataset of 1,265 questions
\par
on Polymarket, with accompanying news headlines up to a given prediction date, taking several
\par
measures to avoid temporal leakage (a common issue when backtesting the accuracy of forecasting
\par
models, see Section 2.1). We assess accuracy with the soft-Brier score and calibration with expected
\par
calibration error (ECE). Finally, we quantify economic value by converting each forecast into a set
\par
of hypothetical trades and comparing realised profits with those of the frontier reasoning model o1
\par
as a benchmark.},
  file = {/home/dmrivers/Zotero/storage/FWQVTKMB/Turtel et al. - 2025 - Outcome-based Reinforcement Learning to Predict the Future.pdf;/home/dmrivers/Zotero/storage/KGRWDSPH/2505.html}
}

@techreport{watsonGlobalAssessmentReport2019,
  title = {The Global Assessment Report on {{BIODIVERSITY AND ECOSYSTEM SERVICES}}},
  author = {Watson, Robert T and Baste, Ivar A and Larigauderie, Anne and Leadley, Paul and Pascual, Unai and Baptiste, Brigitte and Demissew, Sebsebe and Dziba, Luthando and Erpul, Gunay and Fazel, Asghar M and Fischer, Markus and Hern{\'a}ndez, Ana Maria and Karki, Madhav and Mathur, Vinod and Pataridze, Tamar and Pinto, Isabel Sousa and Stenseke, Marie and T{\"o}r{\"o}k, Katalin and Vil{\'a}, Bibiana},
  year = {2019},
  institution = {{Intergovernmental Science-Policy Platform on Biodiversity and Ecosystem Services (IPBES)}},
  abstract = {FOR POLICYMAKERS AUTHORS:1 Sandra D{\'i}az (Co-Chair, Argentina), Josef Settele (Co-Chair, Germany), Eduardo Brond{\'i}zio (Co-Chair, Brazil/United States of America), Hien T. Ngo (IPBES), Maximilien Gu{\`e}ze (IPBES); John Agard (Trinidad and Tobago), Almut Arneth (Germany), Patricia Balvanera (Mexico), Kate Brauman (United States of America), Stuart Butchart (United Kingdom of Great Britain and Northern Ireland/BirdLife International), Kai Chan (Canada), Lucas A. Garibaldi (Argentina), Kazuhito Ichii (Japan), Jianguo Liu (United States of America), Suneetha Mazhenchery Subramanian (India/United Nations University), Guy F. Midgley (South Africa), Patricia Miloslavich (Bolivarian Republic of Venezuela/Australia), Zsolt Moln{\'a}r (Hungary), David Obura (Kenya), Alexander Pfaff (United States of America), Stephen Polasky (United States of America), Andy Purvis (United Kingdom of Great Britain and Northern Ireland), Jona Razzaque (Bangladesh/United Kingdom of Great Britain and Northern Ireland), Belinda Reyers (South Africa), Rinku Roy Chowdhury (United States of America), Yunne-Jai Shin (France), Ingrid Visseren-Hamakers (Netherlands/United States of America), Katherine Willis (United Kingdom of Great Britain and Northern Ireland), Cynthia Zayas (Philippines).},
  langid = {english},
  file = {/home/dmrivers/Zotero/storage/IWZDR972/Watson et al. - MEMBERS OF THE MANAGEMENT COMMITTEE WHO PROVIDED GUIDANCE FOR THE PRODUCTION OF THIS ASSESSMENT.pdf}
}

@misc{wenPredictingEmpiricalAI2025,
  title = {Predicting {{Empirical AI Research Outcomes}} with {{Language Models}}},
  author = {Wen, Jiaxin and Si, Chenglei and Chen, Yueh-han and He, He and Feng, Shi},
  year = {2025},
  month = jun,
  number = {arXiv:2506.00794},
  eprint = {2506.00794},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2506.00794},
  urldate = {2025-08-18},
  abstract = {Many promising-looking ideas in AI research fail to deliver, but their validation takes substantial human labor and compute. Predicting an idea's chance of success is thus crucial for accelerating empirical AI research, a skill that even expert researchers can only acquire through substantial experience. We build the first benchmark for this task and compare LMs with human experts. Concretely, given two research ideas (e.g., two jailbreaking methods), we aim to predict which will perform better on a set of benchmarks. We scrape ideas and experimental results from conference papers, yielding 1,585 human-verified idea pairs published after our base model's cut-off date for testing, and 6,000 pairs for training. We then develop a system that combines a fine-tuned GPT-4.1 with a paper retrieval agent, and we recruit 25 human experts to compare with. In the NLP domain, our system beats human experts by a large margin (64.4\% v.s. 48.9\%). On the full test set, our system achieves 77\% accuracy, while off-the-shelf frontier LMs like o3 perform no better than random guessing, even with the same retrieval augmentation. We verify that our system does not exploit superficial features like idea complexity through extensive human-written and LM-designed robustness tests. Finally, we evaluate our system on unpublished novel ideas, including ideas generated by an AI ideation agent. Our system achieves 63.6\% accuracy, demonstrating its potential as a reward model for improving idea generation models. Altogether, our results outline a promising new direction for LMs to accelerate empirical AI research.},
  archiveprefix = {arXiv},
  keywords = {AI forecasting attempts,NEED TO READ},
  note = {from devon, shows empirical results can be predicted},
  file = {/home/dmrivers/Zotero/storage/MVVJ3Z2P/Wen et al. - 2025 - Predicting Empirical AI Research Outcomes with Language Models.pdf;/home/dmrivers/Zotero/storage/MPSADKI7/2506.html}
}

@misc{WisdomSiliconCrowd,
  title = {Wisdom of the Silicon Crowd: {{LLM}} Ensemble Prediction Capabilities Rival Human Crowd Accuracy {\textbar} {{Science Advances}}},
  urldate = {2025-08-21},
  howpublished = {https://www.science.org/doi/10.1126/sciadv.adp1528},
  keywords = {AI forecasting attempts,introduction,NEED TO READ},
  file = {/home/dmrivers/Zotero/storage/RIBV966L/sciadv.html}
}

@article{xLargeLanguageModels2025,
  title = {Large Language Models Surpass Human Experts in Predicting Neuroscience Results},
  author = {X, Luo and A, Rechardt and G, Sun and Kk, Nejad and F, Y{\'a}{\~n}ez and B, Yilmaz and K, Lee and Ao, Cohen and V, Borghesani and A, Pashkov and D, Marinazzo and J, Nicholas and A, Salatiello and I, Sucholutsky and P, Minervini and S, Razavi and R, Rocca and E, Yusifov and T, Okalova and N, Gu and M, Ferianc and M, Khona and Kr, Patil and Ps, Lee and R, Mata and Ne, Myers and Jk, Bizley and S, Musslick and Ip, Bilgin and G, Niso and Jm, Ales and M, Gaebler and Na, Ratan Murty and L, Loued-Khenissi and A, Behler and Cm, Hall and J, Dafflon and Sd, Bao and Bc, Love},
  year = {2025},
  month = feb,
  journal = {Nature human behaviour},
  volume = {9},
  number = {2},
  publisher = {Nat Hum Behav},
  issn = {2397-3374},
  doi = {10.1038/s41562-024-02046-9},
  urldate = {2025-08-18},
  abstract = {Scientific discoveries often hinge on synthesizing decades of research, a task that potentially outstrips human information processing capacities. Large language models (LLMs) offer a solution. LLMs trained on the vast scientific literature could potentially integrate noisy yet interrelated findings {\dots}},
  langid = {english},
  pmid = {39604572},
  keywords = {AI forecasting attempts},
  note = {braingpt
\par
``LLM training data memorization analysis. One concern regarding LLMs outperforming human experts on BrainBench is the possibility that LLMs were exposed to the original abstracts during their pre-training. If LLMs have simply memorized the training data, they would naturally assign lower perplexity scores to the correct abstracts. To address this concern, we employed a common method from the literature to determine whether a given text is part of LLM's training data22,33. This method involves calculating the zlib entropy and the perplexity ratio (equation (3)) of a text sequence to infer its membership status:'' (X et al., 2025, p. 312)},
  file = {/home/dmrivers/Zotero/storage/JMU69DNS/X et al. - 2025 - Large language models surpass human experts in predicting neuroscience results.pdf;/home/dmrivers/Zotero/storage/TQGHJYJS/39604572.html}
}

@misc{yamadaAIScientistv2WorkshopLevel2025,
  title = {The {{AI Scientist-v2}}: {{Workshop-Level Automated Scientific Discovery}} via {{Agentic Tree Search}}},
  shorttitle = {The {{AI Scientist-v2}}},
  author = {Yamada, Yutaro and Lange, Robert Tjarko and Lu, Cong and Hu, Shengran and Lu, Chris and Foerster, Jakob and Clune, Jeff and Ha, David},
  year = {2025},
  month = apr,
  number = {arXiv:2504.08066},
  eprint = {2504.08066},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2504.08066},
  urldate = {2025-08-21},
  abstract = {AI is increasingly playing a pivotal role in transforming how scientific discoveries are made. We introduce The AI Scientist-v2, an end-to-end agentic system capable of producing the first entirely AI generated peer-review-accepted workshop paper. This system iteratively formulates scientific hypotheses, designs and executes experiments, analyzes and visualizes data, and autonomously authors scientific manuscripts. Compared to its predecessor (v1, Lu et al., 2024 arXiv:2408.06292), The AI Scientist-v2 eliminates the reliance on human-authored code templates, generalizes effectively across diverse machine learning domains, and leverages a novel progressive agentic tree-search methodology managed by a dedicated experiment manager agent. Additionally, we enhance the AI reviewer component by integrating a Vision-Language Model (VLM) feedback loop for iterative refinement of content and aesthetics of the figures. We evaluated The AI Scientist-v2 by submitting three fully autonomous manuscripts to a peer-reviewed ICLR workshop. Notably, one manuscript achieved high enough scores to exceed the average human acceptance threshold, marking the first instance of a fully AI-generated paper successfully navigating a peer review. This accomplishment highlights the growing capability of AI in conducting all aspects of scientific research. We anticipate that further advancements in autonomous scientific discovery technologies will profoundly impact human knowledge generation, enabling unprecedented scalability in research productivity and significantly accelerating scientific breakthroughs, greatly benefiting society at large. We have open-sourced the code at https://github.com/SakanaAI/AI-Scientist-v2 to foster the future development of this transformative technology. We also discuss the role of AI in science, including AI safety.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/dmrivers/Zotero/storage/7VBCTTGF/2504.html}
}

@article{yangEstimatingDeepReplicability2020,
  title = {Estimating the Deep Replicability of Scientific Findings Using Human and Artificial Intelligence},
  author = {Yang, Yang and Youyou, Wu and Uzzi, Brian},
  year = {2020},
  month = may,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {117},
  number = {20},
  pages = {10762--10768},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1909046117},
  urldate = {2025-08-24},
  abstract = {Replicability tests of scientific papers show that the majority of papers fail replication. Moreover, failed papers circulate through the literatur...},
  langid = {english},
  file = {/home/dmrivers/Zotero/storage/3Y96NF3E/Yang et al. - 2020 - Estimating the deep replicability of scientific findings using human and artificial intelligence.pdf}
}

@article{yanq.serajr.hej.mengl.andsylvaint.AutoCastEnhancingWorld2024,
  title = {{{AutoCast}}++: {{Enhancing World Event Prediction}} with {{Zero-shot Ranking-based Context Retrieval}}},
  author = {{Yan, Q., Seraj, R., He, J., Meng, L., and Sylvain, T.}},
  year = {2024},
  journal = {International Conference on Learning  Representations (ICLR)},
  urldate = {2025-08-19},
  abstract = {Leveraging a pre-trained language model, we conduct both the relevance evaluation and article summarization without needing domain-specific training.},
  langid = {american},
  keywords = {AI forecasting attempts},
  note = {``In a competition with a large prize pool, no machine learning system was able to approach the performance of human forecasters on Autocast (Zou et al., 2022). The knowledge cut-offs of the latest LMs have moved past 2022, necessitating more recent data. In this work, we source questions in 2023--2024, enabling us to apply recent LMs. Yan et al. (2024) built a retrieval system that led to improved accuracy on Autocast. They trained a Fusion-in-Decoder model to directly predict the final (binary) resolution'' (Halawi et al., 2024, p. 2)},
  file = {/home/dmrivers/Zotero/storage/WW5M5NYI/AutoCast++ Enhancing World Event Prediction with Zero-shot Ranking-based Context Retrieval.pdf;/home/dmrivers/Zotero/storage/TBFF5SWJ/autocast-enhancing-world-event-prediction-with-zero-shot-ranking-based-context-retrieval.html}
}
